{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the github repository and navigate to the project directory.\n",
    "!git clone https://github.com/AI4Bharat/IndicTrans2\n",
    "%cd IndicTrans2/huggingface_interface\n",
    "\n",
    "# Install all the dependencies and requirements associated with the project for running HF compatible models.\n",
    "!yes|source install.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the file\n",
    "file_path = '/home2/shivani.thakur/IndicTrans2/huggingface_interface/train_lora.sh'\n",
    "\n",
    "# Define the new content to write to the file\n",
    "new_content = \"\"\"\n",
    "export CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "data_dir=${1:-\"en-indic-exp\"}\n",
    "model_name=${2:-\"ai4bharat/indictrans2-indic-indic-dist-320M\"}\n",
    "output_dir=${3:-\"output\"}\n",
    "src_lang_list=${4:-\"hin_Deva\"}\n",
    "tgt_lang_list=${5:-\"sat_Olck\"}\n",
    "\n",
    "python3 train_lora.py \\\\\n",
    "    --data_dir $data_dir \\\\\n",
    "    --model_name $model_name \\\\\n",
    "    --output_dir $output_dir \\\\\n",
    "    --src_lang_list $src_lang_list \\\\\n",
    "    --tgt_lang_list $tgt_lang_list \\\\\n",
    "    --save_steps 1000 \\\\\n",
    "    --max_steps 1000000 \\\\\n",
    "    --batch_size 32 \\\\\n",
    "    --grad_accum_steps 4 \\\\\n",
    "    --warmup_steps 4000 \\\\\n",
    "    --max_grad_norm 1.0 \\\\\n",
    "    --learning_rate 2e-4 \\\\\n",
    "    --adam_beta1 0.9 \\\\\n",
    "    --adam_beta2 0.98 \\\\\n",
    "    --optimizer adamw_torch \\\\\n",
    "    --lr_scheduler inverse_sqrt \\\\\n",
    "    --num_workers 16 \\\\\n",
    "    --metric_for_best_model eval_BLEU \\\\\n",
    "    --greater_is_better \\\\\n",
    "    --patience 10 \\\\\n",
    "    --weight_decay 0.01 \\\\\n",
    "    --lora_target_modules \"q_proj,k_proj\" \\\\\n",
    "    --lora_dropout 0.1 \\\\\n",
    "    --lora_r 16 \\\\\n",
    "    --lora_alpha 32 \\\\\n",
    "    --print_samples\n",
    "\"\"\"\n",
    "\n",
    "# Open the file in write mode and replace the existing content\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(new_content)\n",
    "\n",
    "print(f\"File '{file_path}' has been updated with new content.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the file\n",
    "file_path = '/home2/shivani.thakur/IndicTrans2/huggingface_interface/train_lora.sh'\n",
    "\n",
    "# Open the file and read its contents\n",
    "with open(file_path, 'r') as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "# Print the contents of the file\n",
    "print(file_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the file\n",
    "file_path = '/home2/shivani.thakur/IndicTrans2/huggingface_interface/example.py'\n",
    "\n",
    "# Read the existing content of the file\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.readlines()\n",
    "\n",
    "# Open the file in write mode to update the content\n",
    "with open(file_path, 'w') as file:\n",
    "    for line in content:\n",
    "        # Replace the specific mapping in the line\n",
    "        line = line.replace('\"sat_Olck\": \"or\"', '\"sat_Olck\": \"sat\"')\n",
    "        file.write(line)\n",
    "\n",
    "print(f\"Updated the mapping for 'sat_Olck' in '{file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the file\n",
    "file_path = '/home2/shivani.thakur/IndicTrans2/huggingface_interface/example.py'\n",
    "\n",
    "# Open the file and read its contents\n",
    "with open(file_path, 'r') as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "# Print the contents of the file\n",
    "print(file_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the file\n",
    "file_path = '/home2/shivani.thakur/IndicTrans2/huggingface_interface/train_lora.py'\n",
    "\n",
    "# Define the new content to write to the file\n",
    "new_content = \"\"\"\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from IndicTransToolkit import IndicProcessor, IndicDataCollator\n",
    "\n",
    "from transformers import (\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "bleu_metric = BLEU()\n",
    "chrf_metric = CHRF()\n",
    "\n",
    "\n",
    "def get_arg_parse():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--model_name\",\n",
    "        type=str,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--src_lang_list\",\n",
    "        type=str,\n",
    "        help=\"comma separated list of source languages\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tgt_lang_list\",\n",
    "        type=str,\n",
    "        help=\"comma separated list of target languages\",\n",
    "    )\n",
    "    parser.add_argument(\"--data_dir\", type=str)\n",
    "    parser.add_argument(\"--output_dir\", type=str)\n",
    "    parser.add_argument(\"--save_steps\", type=int, default=1000)\n",
    "    parser.add_argument(\"--eval_steps\", type=int, default=1000)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--num_train_epochs\", type=int, default=100)\n",
    "    parser.add_argument(\"--max_steps\", type=int, default=1000000)\n",
    "    parser.add_argument(\"--grad_accum_steps\", type=int, default=4)\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=4000)\n",
    "    parser.add_argument(\"--warmup_ratio\", type=int, default=0.0)\n",
    "    parser.add_argument(\"--max_grad_norm\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=5e-4)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
    "    parser.add_argument(\"--adam_beta1\", type=float, default=0.9)\n",
    "    parser.add_argument(\"--adam_beta2\", type=float, default=0.98)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.0)\n",
    "    parser.add_argument(\"--print_samples\", action=\"store_true\")\n",
    "    parser.add_argument(\n",
    "        \"--optimizer\",\n",
    "        type=str,\n",
    "        default=\"adamw_torch\",\n",
    "        choices=[\n",
    "            \"adam_hf\",\n",
    "            \"adamw_torch\",\n",
    "            \"adamw_torch_fused\",\n",
    "            \"adamw_apex_fused\",\n",
    "            \"adafactor\",\n",
    "        ],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_scheduler\",\n",
    "        type=str,\n",
    "        default=\"inverse_sqrt\",\n",
    "        choices=[\n",
    "            \"inverse_sqrt\",\n",
    "            \"linear\",\n",
    "            \"polynomial\",\n",
    "            \"cosine\",\n",
    "            \"constant\",\n",
    "            \"constant_with_warmup\",\n",
    "        ],\n",
    "    )\n",
    "    parser.add_argument(\"--label_smoothing\", type=float, default=0.0)\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=8)\n",
    "    parser.add_argument(\"--metric_for_best_model\", type=str, default=\"eval_loss\")\n",
    "    parser.add_argument(\"--greater_is_better\", action=\"store_true\")\n",
    "    parser.add_argument(\"--lora_target_modules\", type=str, default=\"q_proj,k_proj\")\n",
    "    parser.add_argument(\"--lora_dropout\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--lora_r\", type=int, default=16)\n",
    "    parser.add_argument(\"--lora_alpha\", type=int, default=32)\n",
    "    parser.add_argument(\n",
    "        \"--report_to\",\n",
    "        type=str,\n",
    "        default=\"none\",\n",
    "        choices=[\"wandb\", \"tensorboard\", \"azure_ml\", \"none\"],\n",
    "    )\n",
    "    parser.add_argument(\"--patience\", type=int, default=5),\n",
    "    parser.add_argument(\"--threshold\", type=float, default=1e-3)\n",
    "    return parser\n",
    "\n",
    "\n",
    "def load_and_process_translation_dataset(\n",
    "    data_dir,\n",
    "    split=\"train\",\n",
    "    tokenizer=None,\n",
    "    processor=None,\n",
    "    src_lang_list=None,\n",
    "    tgt_lang_list=None,\n",
    "    num_proc=8,\n",
    "    seed=42\n",
    "):\n",
    "    complete_dataset = {\n",
    "        \"sentence_SRC\": [],\n",
    "        \"sentence_TGT\": [],\n",
    "    }\n",
    "\n",
    "    for src_lang in src_lang_list:\n",
    "        for tgt_lang in tgt_lang_list:\n",
    "            if src_lang == tgt_lang:\n",
    "                continue\n",
    "            src_path = os.path.join(\n",
    "                data_dir, split, f\"{src_lang}-{tgt_lang}\", f\"{split}.{src_lang}\"\n",
    "            )\n",
    "            tgt_path = os.path.join(\n",
    "                data_dir, split, f\"{src_lang}-{tgt_lang}\", f\"{split}.{tgt_lang}\"\n",
    "            )\n",
    "            if not os.path.exists(src_path) or not os.path.exists(tgt_path):\n",
    "                raise FileNotFoundError(\n",
    "                    f\"Source ({split}.{src_lang}) or Target ({split}.{tgt_lang}) file not found in {data_dir}\"\n",
    "                )\n",
    "            with open(src_path, encoding=\"utf-8\") as src_file, open(\n",
    "                tgt_path, encoding=\"utf-8\"\n",
    "            ) as tgt_file:\n",
    "                src_lines = src_file.readlines()\n",
    "                tgt_lines = tgt_file.readlines()\n",
    "\n",
    "            # Ensure both files have the same number of lines\n",
    "            assert len(src_lines) == len(\n",
    "                tgt_lines\n",
    "            ), f\"Source and Target files have different number of lines for {split}.{src_lang} and {split}.{tgt_lang}\"\n",
    "\n",
    "            complete_dataset[\"sentence_SRC\"] += processor.preprocess_batch(\n",
    "                src_lines, src_lang=src_lang, tgt_lang=tgt_lang, is_target=False\n",
    "            )\n",
    "\n",
    "            complete_dataset[\"sentence_TGT\"] += processor.preprocess_batch(\n",
    "                tgt_lines, src_lang=tgt_lang, tgt_lang=src_lang, is_target=True\n",
    "            )\n",
    "\n",
    "    complete_dataset = Dataset.from_dict(complete_dataset).shuffle(seed=seed)\n",
    "\n",
    "    return complete_dataset.map(\n",
    "        lambda example: preprocess_fn(\n",
    "            example,\n",
    "            tokenizer=tokenizer\n",
    "        ),\n",
    "        batched=True,\n",
    "        num_proc=num_proc,\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_metrics_factory(\n",
    "    tokenizer, metric_dict=None, print_samples=False, n_samples=10\n",
    "):\n",
    "    def compute_metrics(eval_preds):\n",
    "        preds, labels = eval_preds\n",
    "\n",
    "        labels[labels == -100] = tokenizer.pad_token_id\n",
    "        preds[preds == -100] = tokenizer.pad_token_id\n",
    "\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            preds = [\n",
    "                x.strip()\n",
    "                for x in tokenizer.batch_decode(\n",
    "                    preds, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "                )\n",
    "            ]\n",
    "            labels = [\n",
    "                x.strip()\n",
    "                for x in tokenizer.batch_decode(\n",
    "                    labels, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        assert len(preds) == len(\n",
    "            labels\n",
    "        ), \"Predictions and Labels have different lengths\"\n",
    "\n",
    "        df = pd.DataFrame({\"Predictions\": preds, \"References\": labels}).sample(\n",
    "            n=n_samples\n",
    "        )\n",
    "\n",
    "        if print_samples:\n",
    "            for pred, label in zip(df[\"Predictions\"].values, df[\"References\"].values):\n",
    "                print(f\" | > Prediction: {pred}\")\n",
    "                print(f\" | > Reference: {label}\\\\n\")\n",
    "\n",
    "        return {\n",
    "            metric_name: metric.corpus_score(preds, [labels]).score\n",
    "            for (metric_name, metric) in metric_dict.items()\n",
    "        }\n",
    "\n",
    "    return compute_metrics\n",
    "\n",
    "\n",
    "def preprocess_fn(example, tokenizer, **kwargs):\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"sentence_SRC\"], truncation=True, padding=False, max_length=256\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            example[\"sentence_TGT\"], truncation=True, padding=False, max_length=256\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    print(f\" | > Loading {args.model_name} and tokenizer ...\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        args.model_name,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"eager\",\n",
    "        dropout=args.dropout\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name, trust_remote_code=True)\n",
    "    processor = IndicProcessor(inference=False) # pre-process before tokenization\n",
    "    \n",
    "    data_collator = IndicDataCollator(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        padding=\"longest\", # saves padding tokens\n",
    "        pad_to_multiple_of=8, # better to have it as 8 when using fp16\n",
    "        label_pad_token_id=-100\n",
    "    )\n",
    "\n",
    "    if args.data_dir is not None:\n",
    "        train_dataset = load_and_process_translation_dataset(\n",
    "            args.data_dir,\n",
    "            split=\"train\",\n",
    "            tokenizer=tokenizer,\n",
    "            processor=processor,\n",
    "            src_lang_list=args.src_lang_list.split(\",\"),\n",
    "            tgt_lang_list=args.tgt_lang_list.split(\",\"),\n",
    "        )\n",
    "        print(f\" | > Loaded train dataset from {args.data_dir}. Size: {len(train_dataset)} ...\")\n",
    "\n",
    "        eval_dataset = load_and_process_translation_dataset(\n",
    "            args.data_dir,\n",
    "            split=\"dev\",\n",
    "            tokenizer=tokenizer,\n",
    "            processor=processor,\n",
    "            src_lang_list=args.src_lang_list.split(\",\"),\n",
    "            tgt_lang_list=args.tgt_lang_list.split(\",\"),\n",
    "        )\n",
    "        print(f\" | > Loaded eval dataset from {args.data_dir}. Size: {len(eval_dataset)} ...\")\n",
    "    else:\n",
    "        raise ValueError(\" | > Data directory not provided\")\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=args.lora_r,\n",
    "        bias=\"none\",\n",
    "        inference_mode=False,\n",
    "        task_type=\"SEQ_2_SEQ_LM\",\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        target_modules=args.lora_target_modules.split(\",\"),\n",
    "    )\n",
    "\n",
    "    model.set_label_smoothing(args.label_smoothing)\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    print(f\" | > Loading metrics factory with BLEU and chrF ...\")\n",
    "    seq2seq_compute_metrics = compute_metrics_factory(\n",
    "        tokenizer=tokenizer,\n",
    "        print_samples=args.print_samples,\n",
    "        metric_dict={\"BLEU\": bleu_metric, \"chrF\": chrf_metric},\n",
    "    )\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        fp16=True, # use fp16 for faster training\n",
    "        logging_strategy=\"steps\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        save_total_limit=1,\n",
    "        predict_with_generate=True,\n",
    "        load_best_model_at_end=True,\n",
    "        max_steps=args.max_steps, # max_steps overrides num_train_epochs\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        gradient_accumulation_steps=args.grad_accum_steps,\n",
    "        eval_accumulation_steps=args.grad_accum_steps,\n",
    "        weight_decay=args.weight_decay,\n",
    "        adam_beta1=args.adam_beta1,\n",
    "        adam_beta2=args.adam_beta2,\n",
    "        max_grad_norm=args.max_grad_norm,\n",
    "        optim=args.optimizer,\n",
    "        lr_scheduler_type=args.lr_scheduler,\n",
    "        warmup_ratio=args.warmup_ratio,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        learning_rate=args.learning_rate,\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        save_steps=args.save_steps,\n",
    "        eval_steps=args.eval_steps,\n",
    "        dataloader_num_workers=args.num_workers,\n",
    "        metric_for_best_model=args.metric_for_best_model,\n",
    "        greater_is_better=args.greater_is_better,\n",
    "        report_to=args.report_to,\n",
    "        generation_max_length=256,\n",
    "        generation_num_beams=5,\n",
    "        sortish_sampler=True,\n",
    "        group_by_length=True,\n",
    "        include_tokens_per_second=True,\n",
    "        include_num_input_tokens_seen=True,\n",
    "        dataloader_prefetch_factor=2,\n",
    "    )\n",
    "\n",
    "    # Create Trainer instance\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=seq2seq_compute_metrics,\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(\n",
    "                early_stopping_patience=args.patience,\n",
    "                early_stopping_threshold=args.threshold,\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    print(f\" | > Starting training ...\")\n",
    "\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\" | > Training interrupted ...\")\n",
    "\n",
    "    # this will only save the LoRA adapter weights\n",
    "    model.save_pretrained(args.output_dir)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = get_arg_parse()\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args)\n",
    "\"\"\"\n",
    "\n",
    "# Open the file in write mode and replace the existing content\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(new_content)\n",
    "\n",
    "print(f\"File '{file_path}' has been updated with new content.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash train_lora.sh /home2/shivani.thakur/hi-santali-exp/hi-santali-exp ai4bharat/indictrans2-indic-indic-dist-320M /home2/shivani.thakur/output hin_Deva sat_Olck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
