{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8930607,"sourceType":"datasetVersion","datasetId":5372399}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-31T08:59:30.744420Z","iopub.execute_input":"2024-07-31T08:59:30.745192Z","iopub.status.idle":"2024-07-31T08:59:31.183070Z","shell.execute_reply.started":"2024-07-31T08:59:30.745155Z","shell.execute_reply":"2024-07-31T08:59:31.181999Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/sat-eng-testset/dev.sat_Olck\n/kaggle/input/sat-eng-testset/dev.eng_Latn\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/VarunGumma/IndicTransTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:33.693575Z","iopub.execute_input":"2024-07-31T08:59:33.694071Z","iopub.status.idle":"2024-07-31T08:59:35.940999Z","shell.execute_reply.started":"2024-07-31T08:59:33.694039Z","shell.execute_reply":"2024-07-31T08:59:35.940021Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'IndicTransTokenizer'...\nremote: Enumerating objects: 166, done.\u001b[K\nremote: Counting objects: 100% (21/21), done.\u001b[K\nremote: Compressing objects: 100% (9/9), done.\u001b[K\nremote: Total 166 (delta 14), reused 12 (delta 12), pack-reused 145\u001b[K\nReceiving objects: 100% (166/166), 3.87 MiB | 14.56 MiB/s, done.\nResolving deltas: 100% (76/76), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"cd IndicTransTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:38.384283Z","iopub.execute_input":"2024-07-31T08:59:38.385024Z","iopub.status.idle":"2024-07-31T08:59:38.391790Z","shell.execute_reply.started":"2024-07-31T08:59:38.384987Z","shell.execute_reply":"2024-07-31T08:59:38.390941Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/working/IndicTransTokenizer\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --editable ./","metadata":{"execution":{"iopub.status.busy":"2024-07-31T08:59:41.813931Z","iopub.execute_input":"2024-07-31T08:59:41.814797Z","iopub.status.idle":"2024-07-31T09:00:19.078568Z","shell.execute_reply.started":"2024-07-31T08:59:41.814763Z","shell.execute_reply":"2024-07-31T09:00:19.077619Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Obtaining file:///kaggle/working/IndicTransTokenizer\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library (from IndicTransTokenizer==1.0.1)\n  Cloning https://github.com/VarunGumma/indic_nlp_library to /tmp/pip-install-zq6oz4mk/indic-nlp-library-it2_85d37097d016483fa7a513064a91a72d\n  Running command git clone --filter=blob:none --quiet https://github.com/VarunGumma/indic_nlp_library /tmp/pip-install-zq6oz4mk/indic-nlp-library-it2_85d37097d016483fa7a513064a91a72d\n  Resolved https://github.com/VarunGumma/indic_nlp_library to commit 601521e05ed0ed8f2165ac317a47d186e25b6f0d\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting setuptools==68.2.2 (from IndicTransTokenizer==1.0.1)\n  Downloading setuptools-68.2.2-py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from IndicTransTokenizer==1.0.1) (2.1.2)\nCollecting sacremoses (from IndicTransTokenizer==1.0.1)\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from IndicTransTokenizer==1.0.1) (0.2.0)\nRequirement already satisfied: transformers>=4.39.3 in /opt/conda/lib/python3.10/site-packages (from IndicTransTokenizer==1.0.1) (4.42.3)\nCollecting sacrebleu>=2.3.1 (from IndicTransTokenizer==1.0.1)\n  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu>=2.3.1->IndicTransTokenizer==1.0.1)\n  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=2.3.1->IndicTransTokenizer==1.0.1) (2023.12.25)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=2.3.1->IndicTransTokenizer==1.0.1) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=2.3.1->IndicTransTokenizer==1.0.1) (1.26.4)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=2.3.1->IndicTransTokenizer==1.0.1) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=2.3.1->IndicTransTokenizer==1.0.1) (5.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.3->IndicTransTokenizer==1.0.1) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.3->IndicTransTokenizer==1.0.1) (0.23.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.3->IndicTransTokenizer==1.0.1) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.3->IndicTransTokenizer==1.0.1) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.3->IndicTransTokenizer==1.0.1) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.3->IndicTransTokenizer==1.0.1) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.3->IndicTransTokenizer==1.0.1) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.3->IndicTransTokenizer==1.0.1) (4.66.4)\nCollecting sphinx-argparse (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1)\n  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: sphinx_rtd_theme in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1) (0.2.4)\nCollecting morfessor (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1)\n  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1) (2.2.2)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from sacremoses->IndicTransTokenizer==1.0.1) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from sacremoses->IndicTransTokenizer==1.0.1) (1.4.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->IndicTransTokenizer==1.0.1) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->IndicTransTokenizer==1.0.1) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->IndicTransTokenizer==1.0.1) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->IndicTransTokenizer==1.0.1) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->IndicTransTokenizer==1.0.1) (2024.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.39.3->IndicTransTokenizer==1.0.1) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->IndicTransTokenizer==1.0.1) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.39.3->IndicTransTokenizer==1.0.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.39.3->IndicTransTokenizer==1.0.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.39.3->IndicTransTokenizer==1.0.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.39.3->IndicTransTokenizer==1.0.1) (2024.7.4)\nCollecting sphinx>=5.1.0 (from sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1)\n  Downloading sphinx-8.0.2-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: docutils>=0.19 in /opt/conda/lib/python3.10/site-packages (from sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1) (0.21.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->IndicTransTokenizer==1.0.1) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1) (1.16.0)\nCollecting sphinxcontrib-applehelp (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1)\n  Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting sphinxcontrib-devhelp (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1)\n  Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting sphinxcontrib-jsmath (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1)\n  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting sphinxcontrib-htmlhelp>=2.0.0 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1)\n  Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1)\n  Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl.metadata (2.4 kB)\nCollecting sphinxcontrib-qthelp (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1)\n  Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: Pygments>=2.17 in /opt/conda/lib/python3.10/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1) (2.17.2)\nRequirement already satisfied: snowballstemmer>=2.2 in /opt/conda/lib/python3.10/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1) (2.2.0)\nRequirement already satisfied: babel>=2.13 in /opt/conda/lib/python3.10/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1) (2.14.0)\nCollecting alabaster>=0.7.14 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1)\n  Downloading alabaster-1.0.0-py3-none-any.whl.metadata (2.8 kB)\nCollecting imagesize>=1.3 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1)\n  Downloading imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting packaging>=20.0 (from transformers>=4.39.3->IndicTransTokenizer==1.0.1)\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: tomli>=2 in /opt/conda/lib/python3.10/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==1.0.1) (2.0.1)\nDownloading setuptools-68.2.2-py3-none-any.whl (807 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\nDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\nDownloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\nDownloading sphinx-8.0.2-py3-none-any.whl (3.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading alabaster-1.0.0-py3-none-any.whl (13 kB)\nDownloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\nDownloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl (98 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl (92 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl (119 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.3/119.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl (82 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\nDownloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl (88 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.7/88.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: indic-nlp-library-IT2\n  Building wheel for indic-nlp-library-IT2 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for indic-nlp-library-IT2: filename=indic_nlp_library_IT2-0.0.2-py3-none-any.whl size=49558 sha256=901d69730b49a1b0d1d7cd366389aee0f2a5209b729bae397542362789609e16\n  Stored in directory: /tmp/pip-ephem-wheel-cache-el7nke5l/wheels/e9/72/fa/bd9f19a3f2bacb50efcaf28b7ab89fe7ca539e35b75334befc\nSuccessfully built indic-nlp-library-IT2\nInstalling collected packages: morfessor, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, setuptools, sacremoses, portalocker, packaging, imagesize, alabaster, sphinx, sacrebleu, sphinx-argparse, indic-nlp-library-IT2, IndicTransTokenizer\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 69.0.3\n    Uninstalling setuptools-69.0.3:\n      Successfully uninstalled setuptools-69.0.3\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Running setup.py develop for IndicTransTokenizer\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.6.1 requires cubinlinker, which is not installed.\ncudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.6.1 requires ptxcompiler, which is not installed.\ncuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ncudf 24.6.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.5.1 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.6.0a0 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed IndicTransTokenizer-1.0.1 alabaster-1.0.0 imagesize-1.4.1 indic-nlp-library-IT2-0.0.2 morfessor-2.0.6 packaging-24.1 portalocker-2.10.1 sacrebleu-2.4.2 sacremoses-0.1.1 setuptools-68.2.2 sphinx-8.0.2 sphinx-argparse-0.5.2 sphinxcontrib-applehelp-2.0.0 sphinxcontrib-devhelp-2.0.0 sphinxcontrib-htmlhelp-2.1.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-2.0.0 sphinxcontrib-serializinghtml-2.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\n\nimport torch\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n)\nfrom IndicTransTokenizer import IndicProcessor","metadata":{"execution":{"iopub.status.busy":"2024-07-31T09:00:29.092883Z","iopub.execute_input":"2024-07-31T09:00:29.093632Z","iopub.status.idle":"2024-07-31T09:00:45.468481Z","shell.execute_reply.started":"2024-07-31T09:00:29.093594Z","shell.execute_reply":"2024-07-31T09:00:45.467567Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-07-31 09:00:36.048474: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-31 09:00:36.048583: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-31 09:00:36.181982: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# DEVICE = \"cpu\"\n\nmodel_name = \"ai4bharat/indictrans2-en-indic-dist-200M\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name, trust_remote_code=True).to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T09:00:48.303076Z","iopub.execute_input":"2024-07-31T09:00:48.304145Z","iopub.status.idle":"2024-07-31T09:00:58.267077Z","shell.execute_reply.started":"2024-07-31T09:00:48.304113Z","shell.execute_reply":"2024-07-31T09:00:58.266200Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad714be6e10c45b9828a4bd601f58fd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenization_indictrans.py:   0%|          | 0.00/8.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78729e7d45f74850a982e06b5baaef68"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-en-indic-dist-200M:\n- tokenization_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"dict.SRC.json:   0%|          | 0.00/645k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06c5eae3405a400997a5ad2409249ec3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dict.TGT.json:   0%|          | 0.00/3.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"deb249ab08984f739990ce8d8739adcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.SRC:   0%|          | 0.00/759k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e585597d6e7241c2b65509da0e13c052"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.TGT:   0%|          | 0.00/3.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b24570412a345a9859a5799b9d5ae10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"890cadffedca425d9eebf9ad24222200"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"412c4b3883f34ae3be58e2a063c2a215"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_indictrans.py:   0%|          | 0.00/14.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fdf31e10aab4c59b55d5d883b3fa5dd"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-en-indic-dist-200M:\n- configuration_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_indictrans.py:   0%|          | 0.00/79.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8ab6819780747979e350d03ceeeb189"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-en-indic-dist-200M:\n- modeling_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.10G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88868aa2bb4b41e19063c57b84f89d78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff0abaa2fe574b3e9227c9531dc966f6"}},"metadata":{}}]},{"cell_type":"code","source":"ip = IndicProcessor(inference=True)\n\nsrc_lang, tgt_lang = \"eng_Latn\", \"sat_Olck\"\nwith open(\"/kaggle/input/sat-eng-testset/dev.eng_Latn\", \"r\") as f:\n    src = f.readlines()","metadata":{"execution":{"iopub.status.busy":"2024-07-31T09:01:03.642992Z","iopub.execute_input":"2024-07-31T09:01:03.643370Z","iopub.status.idle":"2024-07-31T09:01:03.659874Z","shell.execute_reply.started":"2024-07-31T09:01:03.643340Z","shell.execute_reply":"2024-07-31T09:01:03.658858Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"batch_size = 16\noutput = []\n\nfor i in range(0, len(src), batch_size):\n    batch = ip.preprocess_batch(src[i:i+batch_size], src_lang=src_lang, tgt_lang=tgt_lang)\n    inputs = tokenizer(batch, truncation=True, padding=\"longest\", return_tensors=\"pt\",\n                        return_attention_mask=True).to(DEVICE)\n\n    with torch.no_grad():\n        generated_tokens = model.generate(\n            **inputs,\n            use_cache=True,\n            min_length=0,\n            max_length=256,\n            num_beams=5,\n            num_return_sequences=1,\n        )\n        \n    with tokenizer.as_target_tokenizer():\n        generated_tokens = tokenizer.batch_decode(\n            generated_tokens.detach().cpu().tolist(),\n            skip_special_tokens=True,\n            clean_up_tokenization_spaces=True,\n        )\n        \n    translations = ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n    output.append(translations)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T09:01:05.933279Z","iopub.execute_input":"2024-07-31T09:01:05.934355Z","iopub.status.idle":"2024-07-31T09:04:15.651351Z","shell.execute_reply.started":"2024-07-31T09:01:05.934305Z","shell.execute_reply":"2024-07-31T09:04:15.650482Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4016: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"with open(\"sat_dev.txt\", \"w\") as f:\n    for translations in output:\n        for translation in translations:\n            f.write(translation + \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-07-31T09:05:49.145015Z","iopub.execute_input":"2024-07-31T09:05:49.145861Z","iopub.status.idle":"2024-07-31T09:05:49.152834Z","shell.execute_reply.started":"2024-07-31T09:05:49.145829Z","shell.execute_reply":"2024-07-31T09:05:49.152016Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/vmujadia/tokenizer.git --upgrade","metadata":{"execution":{"iopub.status.busy":"2024-07-31T09:05:50.947899Z","iopub.execute_input":"2024-07-31T09:05:50.948344Z","iopub.status.idle":"2024-07-31T09:06:06.522800Z","shell.execute_reply.started":"2024-07-31T09:05:50.948313Z","shell.execute_reply":"2024-07-31T09:06:06.521490Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting git+https://github.com/vmujadia/tokenizer.git\n  Cloning https://github.com/vmujadia/tokenizer.git to /tmp/pip-req-build-n6nigdtu\n  Running command git clone --filter=blob:none --quiet https://github.com/vmujadia/tokenizer.git /tmp/pip-req-build-n6nigdtu\n  Resolved https://github.com/vmujadia/tokenizer.git to commit 93cd09b81702108a51c08c9796fd1cc941a1b98b\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from IL-Tokenizer==0.0.2) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from IL-Tokenizer==0.0.2) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->IL-Tokenizer==0.0.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->IL-Tokenizer==0.0.2) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->IL-Tokenizer==0.0.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->IL-Tokenizer==0.0.2) (2024.7.4)\nBuilding wheels for collected packages: IL-Tokenizer\n  Building wheel for IL-Tokenizer (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for IL-Tokenizer: filename=IL_Tokenizer-0.0.2-py3-none-any.whl size=7225 sha256=b69a2e384844d0211eb10684a400eac93c9346e1f07829b98e365f5636c814e3\n  Stored in directory: /tmp/pip-ephem-wheel-cache-apk6_ctw/wheels/9a/fb/5b/3d75bfde8561726121c09f0f0a83389c05312df8a513808c41\nSuccessfully built IL-Tokenizer\nInstalling collected packages: IL-Tokenizer\nSuccessfully installed IL-Tokenizer-0.0.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from ilstokenizer import tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-07-31T09:06:09.002888Z","iopub.execute_input":"2024-07-31T09:06:09.003820Z","iopub.status.idle":"2024-07-31T09:06:09.082181Z","shell.execute_reply.started":"2024-07-31T09:06:09.003783Z","shell.execute_reply":"2024-07-31T09:06:09.081326Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from sacrebleu import corpus_bleu\n\n# Step 1: Read the translated output\ntranslated_output_file = \"sat_dev.txt\" \ntranslated_output = []\nwith open(translated_output_file, \"r\") as f:\n    for line in f:\n        translated_output.append(line.strip())\n\n# Step 2: Read the reference translations\nreference_file = \"/kaggle/input/sat-eng-testset/dev.sat_Olck\"  \nreference_translations = []\nwith open(reference_file, \"r\") as f:\n    for line in f:\n        reference_translations.append(line.strip())\n\n# Step 3: Tokenize the translated output and reference translations\ntokenized_output = [tokenizer.tokenize(sentence).lower() for sentence in translated_output]\ntokenized_references = [[tokenizer.tokenize(sentence).lower()] for sentence in reference_translations]\n\n# Step 4: Compute the BLEU score\nbleu_score = corpus_bleu(tokenized_output, tokenized_references)\nprint(\"BLEU score:\", bleu_score.score)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T09:06:49.336659Z","iopub.execute_input":"2024-07-31T09:06:49.337490Z","iopub.status.idle":"2024-07-31T09:06:51.177551Z","shell.execute_reply.started":"2024-07-31T09:06:49.337455Z","shell.execute_reply":"2024-07-31T09:06:51.176522Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"BLEU score: 5.179174010947287\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}