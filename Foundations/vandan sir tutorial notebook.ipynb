{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axRPPFFocszE"
      },
      "source": [
        "# **Neural Machine Translation Hands-on for HimangiY**\n",
        "#### Vandan Mujadia, Dipti Misra Sharma\n",
        "#### LTRC, IIIT-Hyderabad, Hyderabad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX4IVQ5qf52I"
      },
      "source": [
        "This demonstrates how to train a sequence-to-sequence (seq2seq) model for English-to-Hindi translation **roughly** based on [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1706.03762) (Vaswani, Ashish et al).\n",
        "\n",
        "## An Example to Understand sequence to Sequence processing using Transformar Network.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/apply_the_transformer_to_machine_translation.gif\" alt=\"Applying the Transformer to machine translation\">\n",
        "\n",
        "Source: [Google AI Blog](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J36LSLbtf_Co"
      },
      "source": [
        "## Applying the Transformer to machine translation.\n",
        "\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://miro.medium.com/max/720/1*57LYNxwBGcCFFhkOCSnJ3g.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th colspan=1>This tutorial: An encoder/decoder connected by self attention neural network.</th>\n",
        "<tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7BlcJtET3DH"
      },
      "source": [
        "# Tools that we are using here\n",
        "\n",
        "*   Library : pytorch\n",
        "*   Library : fairseq for neural network implemtation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGhkwuMOo4pw",
        "outputId": "1ee19739-3d3e-4265-887a-6e890409e22a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gknS9QN5aU7P",
        "outputId": "11c44b65-a079-4056-aca2-e83f5ebd414e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.2.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-23.2.1\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 34777, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 34777 (delta 0), reused 2 (delta 0), pack-reused 34769\u001b[K\n",
            "Receiving objects: 100% (34777/34777), 25.03 MiB | 26.53 MiB/s, done.\n",
            "Resolving deltas: 100% (25248/25248), done.\n",
            "/content/fairseq\n",
            "Obtaining file:///content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.15.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (0.29.36)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq==0.12.2)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq==0.12.2)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.23.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2023.6.3)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq==0.12.2)\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (4.66.1)\n",
            "Collecting bitarray (from fairseq==0.12.2)\n",
            "  Obtaining dependency information for bitarray from https://files.pythonhosted.org/packages/5b/81/938e947b54e6e1eee683c069fb19d35ee0c292fc91d7b10eaab84611439b/bitarray-2.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading bitarray-2.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.0.2+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (23.1)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.7.1)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq==0.12.2)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq==0.12.2)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13->fairseq==0.12.2) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13->fairseq==0.12.2) (16.0.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.12.2) (2.21)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->fairseq==0.12.2) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->fairseq==0.12.2) (1.3.0)\n",
            "Downloading bitarray-2.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (286 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.2/286.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building editable for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-0.editable-cp310-cp310-linux_x86_64.whl size=9379 sha256=4b068876a0ec1c6dbb6c20f88d98f2a41ee6cfe28478443e47f42dc477ec3109\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-51nkk3rz/wheels/c6/d7/db/bc419b1daa8266aa8de2a7c4d29f62dbfa814e8701fe4695a2\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=f370f0ed3bed65744ae111e9ec4e4572949cb8d339567b95f460e09cd81c1e51\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.8.1 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.7.0 sacrebleu-2.3.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m/content\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pip\n",
        "!git clone https://github.com/pytorch/fairseq.git\n",
        "%cd fairseq\n",
        "#!git checkout v0.10.2\n",
        "##!python -m pip install --user ./\n",
        "!pip install --editable .\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0jUxSkJTpCY"
      },
      "source": [
        "# Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6W53TaterZ4",
        "outputId": "02747113-31be-448b-90cf-6668e76dd185"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Aug 23 04:54:21 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCO9U93FVy74"
      },
      "source": [
        "# Tokenizer Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RXkfYjYL35Q",
        "outputId": "8b813595-f0c0-48b6-9159-5a02fc4dca45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/vmujadia/tokenizer.git\n",
            "  Cloning https://github.com/vmujadia/tokenizer.git to /tmp/pip-req-build-vk09hsa8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/vmujadia/tokenizer.git /tmp/pip-req-build-vk09hsa8\n",
            "  Resolved https://github.com/vmujadia/tokenizer.git to commit 93cd09b81702108a51c08c9796fd1cc941a1b98b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from IL-Tokenizer==0.0.2) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from IL-Tokenizer==0.0.2) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->IL-Tokenizer==0.0.2) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->IL-Tokenizer==0.0.2) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->IL-Tokenizer==0.0.2) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->IL-Tokenizer==0.0.2) (2023.7.22)\n",
            "Building wheels for collected packages: IL-Tokenizer\n",
            "  Building wheel for IL-Tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for IL-Tokenizer: filename=IL_Tokenizer-0.0.2-py3-none-any.whl size=7224 sha256=5abfdd3c98345f910040423ad4b63b017180d2e182abde3099560e883dd750f1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-a3vgylm3/wheels/9a/fb/5b/3d75bfde8561726121c09f0f0a83389c05312df8a513808c41\n",
            "Successfully built IL-Tokenizer\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: IL-Tokenizer\n",
            "Successfully installed IL-Tokenizer-0.0.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/vmujadia/tokenizer.git --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDJjJCnEV9Cx"
      },
      "source": [
        "# To Clean and Filter Parallel Corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtF7MxJdMeky",
        "outputId": "26e07074-9a65-40ee-c4ce-d63273ddae32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'mosesdecoder'...\n",
            "remote: Enumerating objects: 148097, done.\u001b[K\n",
            "remote: Counting objects: 100% (525/525), done.\u001b[K\n",
            "remote: Compressing objects: 100% (229/229), done.\u001b[K\n",
            "remote: Total 148097 (delta 323), reused 441 (delta 292), pack-reused 147572\u001b[K\n",
            "Receiving objects: 100% (148097/148097), 129.88 MiB | 13.65 MiB/s, done.\n",
            "Resolving deltas: 100% (114349/114349), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/moses-smt/mosesdecoder.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XksbkTQ9VUaQ"
      },
      "source": [
        "# To tackle vocabulary issue : Subword algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGWYp4kFLUYL",
        "outputId": "403cd575-1c8f-44c4-a4f4-bef320b8c0db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'subword-nmt'...\n",
            "remote: Enumerating objects: 597, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 597 (delta 8), reused 12 (delta 4), pack-reused 576\u001b[K\n",
            "Receiving objects: 100% (597/597), 252.23 KiB | 6.31 MiB/s, done.\n",
            "Resolving deltas: 100% (357/357), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/rsennrich/subword-nmt.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC4xAujyMiYH",
        "outputId": "3d7cf8ad-50b4-4fcc-8598-6ea0a569f6d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mosesdecoder/scripts/training/clean-corpus-n.perl\n"
          ]
        }
      ],
      "source": [
        "!ls mosesdecoder/scripts/training/clean-corpus-n.perl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAMhRFrfZBGx"
      },
      "source": [
        "# For this; Training Corpora\n",
        "\n",
        "## English - Hindi\n",
        "## (small PMI courpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roXS-U5oM_A4",
        "outputId": "328977d8-b16a-4eb4-87e6-658beecf1aa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-08-23 04:54:43--  https://swayam.iiit.ac.in/upload/uploadfiles/ssmt/nmt-IIITH/pmindia.en\n",
            "Resolving swayam.iiit.ac.in (swayam.iiit.ac.in)... 196.12.53.52\n",
            "Connecting to swayam.iiit.ac.in (swayam.iiit.ac.in)|196.12.53.52|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6695890 (6.4M) [text/plain]\n",
            "Saving to: ‘train.src’\n",
            "\n",
            "train.src           100%[===================>]   6.38M  43.9KB/s    in 70s     \n",
            "\n",
            "2023-08-23 04:55:54 (93.6 KB/s) - ‘train.src’ saved [6695890/6695890]\n",
            "\n",
            "--2023-08-23 04:55:54--  https://swayam.iiit.ac.in/upload/uploadfiles/ssmt/nmt-IIITH/pmindia.hi\n",
            "Resolving swayam.iiit.ac.in (swayam.iiit.ac.in)... 196.12.53.52\n",
            "Connecting to swayam.iiit.ac.in (swayam.iiit.ac.in)|196.12.53.52|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16554142 (16M)\n",
            "Saving to: ‘train.tgt’\n",
            "\n",
            "train.tgt           100%[===================>]  15.79M  1.06MB/s    in 58s     \n",
            "\n",
            "2023-08-23 04:56:54 (279 KB/s) - ‘train.tgt’ saved [16554142/16554142]\n",
            "\n",
            "--2023-08-23 04:56:54--  https://swayam.iiit.ac.in/upload/uploadfiles/ssmt/nmt-IIITH/dev.hi-en.en\n",
            "Resolving swayam.iiit.ac.in (swayam.iiit.ac.in)... 196.12.53.52\n",
            "Connecting to swayam.iiit.ac.in (swayam.iiit.ac.in)|196.12.53.52|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 218272 (213K) [text/plain]\n",
            "Saving to: ‘valid.src’\n",
            "\n",
            "valid.src           100%[===================>] 213.16K   402KB/s    in 0.5s    \n",
            "\n",
            "2023-08-23 04:56:56 (402 KB/s) - ‘valid.src’ saved [218272/218272]\n",
            "\n",
            "--2023-08-23 04:56:56--  https://swayam.iiit.ac.in/upload/uploadfiles/ssmt/nmt-IIITH/dev.hi-en.hi\n",
            "Resolving swayam.iiit.ac.in (swayam.iiit.ac.in)... 196.12.53.52\n",
            "Connecting to swayam.iiit.ac.in (swayam.iiit.ac.in)|196.12.53.52|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 490096 (479K)\n",
            "Saving to: ‘valid.tgt’\n",
            "\n",
            "valid.tgt           100%[===================>] 478.61K   267KB/s    in 1.8s    \n",
            "\n",
            "2023-08-23 04:56:59 (267 KB/s) - ‘valid.tgt’ saved [490096/490096]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget -O train.src https://swayam.iiit.ac.in/upload/uploadfiles/ssmt/nmt-IIITH/pmindia.en\n",
        "! wget -O train.tgt https://swayam.iiit.ac.in/upload/uploadfiles/ssmt/nmt-IIITH/pmindia.hi\n",
        "! wget -O valid.src https://swayam.iiit.ac.in/upload/uploadfiles/ssmt/nmt-IIITH/dev.hi-en.en\n",
        "! wget -O valid.tgt https://swayam.iiit.ac.in/upload/uploadfiles/ssmt/nmt-IIITH/dev.hi-en.hi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plXXn4vYh7dL"
      },
      "source": [
        "# Data Numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnRRGSCnQ8DI",
        "outputId": "5c3c9e75-908a-4e7c-f6b5-5eaee496a675"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Stats\n",
            "   56832 train.src\n",
            "   56832 train.tgt\n",
            "  113664 total\n",
            "  2000 valid.src\n",
            "  2000 valid.tgt\n",
            "  4000 total\n"
          ]
        }
      ],
      "source": [
        "print ('Data Stats')\n",
        "! wc -l train.*\n",
        "! wc -l valid.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXbHzLYYiCvZ"
      },
      "source": [
        "# Tokenize the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbWZ1WpJPR1R"
      },
      "outputs": [],
      "source": [
        "from ilstokenizer import tokenizer\n",
        "import codecs\n",
        "\n",
        "def to_tokenize_and_lower(input_path, output_path):\n",
        "  outfile = open(output_path, 'w')\n",
        "  for line in codecs.open(input_path):\n",
        "    line = line.strip()\n",
        "    line = tokenizer.tokenize(line).lower()\n",
        "    #print (line)\n",
        "    outfile.write(line+'\\n')\n",
        "  outfile.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjtXoUVZQXk4"
      },
      "outputs": [],
      "source": [
        "to_tokenize_and_lower('train.src','train.src.tkn')\n",
        "to_tokenize_and_lower('train.tgt','train.tgt.tkn')\n",
        "\n",
        "to_tokenize_and_lower('valid.src','valid.src.tkn')\n",
        "to_tokenize_and_lower('valid.tgt','valid.tgt.tkn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lzgo8VWmNYAG"
      },
      "outputs": [],
      "source": [
        "! cat train.src.tkn > train.all.tkn\n",
        "! cat train.tgt.tkn >> train.all.tkn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWpmi90piKUC"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBSq7A5gRuux",
        "outputId": "8a8d22c6-3df2-4704-eb02-1580d783f9c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "clean-corpus.perl: processing train.src.tkn & .tgt.tkn to train_filtered, cutoff 1-250, ratio 1.5\n",
            ".....\n",
            "Input sentences: 56832  Output sentences:  53833\n"
          ]
        }
      ],
      "source": [
        "! perl mosesdecoder/scripts/training/clean-corpus-n.perl -ratio 1.5 train src.tkn tgt.tkn train_filtered 1 250"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2ibyHwUSkrL",
        "outputId": "04290195-d0ea-40f2-9297-94dd9ffb0d17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Stats\n",
            "  113664 train.all.tkn\n",
            "   53833 train_filtered.src.tkn\n",
            "   53833 train_filtered.tgt.tkn\n",
            "   56832 train.src\n",
            "   56832 train.src.tkn\n",
            "   56832 train.tgt\n",
            "   56832 train.tgt.tkn\n",
            "  448658 total\n",
            "   2000 valid.src\n",
            "   2000 valid.src.tkn\n",
            "   2000 valid.tgt\n",
            "   2000 valid.tgt.tkn\n",
            "   8000 total\n"
          ]
        }
      ],
      "source": [
        "print ('Data Stats')\n",
        "! wc -l train*\n",
        "! wc -l valid*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oukoeGHniPEC"
      },
      "source": [
        "# Train subword model,\n",
        "## Experiment with no of subword merge operation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COV2XMvATTIw",
        "outputId": "c8c7161e-655f-4dbb-9626-f1150ea2d82b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100% 7500/7500 [00:12<00:00, 586.03it/s]\n"
          ]
        }
      ],
      "source": [
        "!python subword-nmt/subword_nmt/learn_bpe.py -s 7500 < train.all.tkn > train.codes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AZodBXIir_D"
      },
      "source": [
        "# How do subword codes look"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kduoQxrOwc7",
        "outputId": "c4564e20-adde-498b-c55d-b2900a9b362c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#version: 0.2\n",
            "t h\n",
            "i n\n",
            "् र\n",
            "th e</w>\n",
            "a n\n",
            "क े</w>\n",
            "e n\n",
            "t i\n",
            "e r\n"
          ]
        }
      ],
      "source": [
        "! head -n 10 train.codes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSbwhzpqjbH8"
      },
      "source": [
        "# Apply Subword to the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13QghGSZT6tA"
      },
      "outputs": [],
      "source": [
        "!python subword-nmt/subword_nmt/apply_bpe.py -c train.codes < train.src.tkn > train.en\n",
        "!python subword-nmt/subword_nmt/apply_bpe.py -c train.codes < train.tgt.tkn > train.hi\n",
        "\n",
        "!python subword-nmt/subword_nmt/apply_bpe.py -c train.codes < valid.src.tkn > valid.en\n",
        "!python subword-nmt/subword_nmt/apply_bpe.py -c train.codes < valid.tgt.tkn > valid.hi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOo9h4ULjXuU"
      },
      "source": [
        "# Training Corpus now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CRKB3qOURoT",
        "outputId": "338d94de-b8e5-4607-a434-a33808ba50ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "an advance is plac@@ ed with the medical su@@ per@@ int@@ end@@ ents of such hospit@@ als who then provide assistance on a case to case basis .\n",
            "since the do@@ h@@ f@@ w provides funds to the hospit@@ als , the gr@@ ants can be given from the department to the hospital directly .\n",
            "r@@ an func@@ tions can , therefore , be vest@@ ed in do@@ h@@ f@@ w .\n",
            "man@@ aging committee of r@@ an society will meet to dis@@ sol@@ ve the aut@@ onom@@ ous body ( a@@ b ) as per provisions of societies regist@@ ration act , 18@@ 60 ( s@@ ra ) .\n",
            "in addition to this , health minister ’ s canc@@ er pati@@ ent fund ( h@@ m@@ cp@@ f ) shall also be trans@@ ferred to the department .\n",
            "the tim@@ eline required for this is one year .\n",
            "j@@ s@@ k organiz@@ es various activities with target popul@@ ations as a part of its man@@ date .\n",
            "there has been no continu@@ ous funding to j@@ s@@ k from the ministry .\n",
            "population st@@ abil@@ ization strate@@ g@@ ies requ@@ ir@@ e private and corporate funding , which can be acc@@ essed through j@@ s@@ k .\n",
            "al@@ thou@@ gh , j@@ s@@ k will continue to play a significant role in population st@@ abil@@ ization strate@@ g@@ ies , its exist@@ ence as an aut@@ onom@@ ous body is not necessary .\n"
          ]
        }
      ],
      "source": [
        "! head -n 10 train.en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39V10nlIUXT9",
        "outputId": "43926ba2-8475-4ed3-a2f3-dd1a050114c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "अग@@ ्रि@@ म धन राशि इन अस्प@@ ता@@ लों को चिकित्सा नि@@ री@@ क्ष@@ कों को दी जाएगी , जो हर मामले को देखते हुए सहायता प्रदान करेंगे ।\n",
            "च@@ ू@@ ंकि स्वास्थ्य एवं परिवार कल्याण विभाग अस्प@@ ता@@ लों को धन@@ राशि प्रदान करता है इसलिए विभाग द्वारा अस्प@@ ता@@ लों को सीधे अनु@@ दान दिया जा सकता है ।\n",
            "इस तरह आर@@ ए@@ एन का काम@@ का@@ ज स्वास्थ्य एवं परिवार कल्याण विभाग के अध@@ ीन लाया जाएगा ।\n",
            "आर@@ ए@@ एन , सो@@ साय@@ टी की प्र@@ बंध समिति सो@@ साय@@ टी पंजी@@ करण अधिनियम , 18@@ 60 के प्रावधानों के तहत स्वा@@ य@@ त्@@ त@@ शा@@ सी निका@@ यों को र@@ द्@@ द करने के लिए बैठक करेगा ।\n",
            "इसके अलावा स्वास्थ्य मंत्री के कैं@@ सर रो@@ गी निधि को भी विभाग को स्थ@@ ान@@ ा@@ ंत@@ रित कर दिया जाएगा ।\n",
            "इसके लिए एक वर्ष का समय रखा गया है ।\n",
            "जे@@ एस@@ के लक्ष@@ ित आबादी के म@@ द्@@ दे@@ नजर विभिन्न गतिविधियों का आयोजन करता है ।\n",
            "मंत्रालय द्वारा जे@@ एस@@ के का कोई लगातार वित्@@ त@@ पोषण नहीं किया जाता ।\n",
            "जन@@ संख्या स्थि@@ री@@ करण रण@@ नीतियों के निजी और कार्@@ पोरे@@ ट वित्@@ त@@ पोषण की जरूरत होती है , जो जे@@ एस@@ के के जरिए संभव है ।\n",
            "य@@ द्@@ य@@ प@@ ि जे@@ एस@@ के जन@@ संख्या स्थि@@ री@@ करण रण@@ नीतियों में अहम भूमिका निभा@@ ता रहेगा , लेकिन एक स्वा@@ य@@ त@@ शा@@ सी निका@@ य के रूप में उसका अ@@ स्ति@@ त्व आवश्यक नहीं होगा ।\n"
          ]
        }
      ],
      "source": [
        "! head -n 10 train.hi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC60k_11u_gb",
        "outputId": "4f3f3e96-5315-4968-faff-9dda9b6cf24f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/env/python:/content/fairseq/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "\n",
        "! echo $PYTHONPATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuZl-vwLjqy8"
      },
      "source": [
        "# Starting  NMT Training\n",
        "## Preprocessing stage ; create dictionaries, make corpora ready for parallel processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaamLXAwuYXT",
        "outputId": "eb3e7e15-e97e-4f13-d875-d705ea003e60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-08-23 04:58:05.598942: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-23 04:58:06.962491: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
            "INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', simul_type=None, scoring='bleu', task='translation', source_lang='en', target_lang='hi', trainpref='train', validpref='valid', testpref='valid', align_suffix=None, destdir='data-bin/trial', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=True, only_source=False, padding_factor=8, workers=20, dict_only=False)\n",
            "INFO:fairseq_cli.preprocess:[en] Dictionary: 7776 types\n",
            "INFO:fairseq_cli.preprocess:[en] train.en: 56832 sents, 1590553 tokens, 0.0% replaced (by <unk>)\n",
            "INFO:fairseq_cli.preprocess:[en] Dictionary: 7776 types\n",
            "INFO:fairseq_cli.preprocess:[en] valid.en: 2000 sents, 57218 tokens, 0.014% replaced (by <unk>)\n",
            "INFO:fairseq_cli.preprocess:[en] Dictionary: 7776 types\n",
            "INFO:fairseq_cli.preprocess:[en] valid.en: 2000 sents, 57218 tokens, 0.014% replaced (by <unk>)\n",
            "INFO:fairseq_cli.preprocess:[hi] Dictionary: 7776 types\n",
            "INFO:fairseq_cli.preprocess:[hi] train.hi: 56832 sents, 1683922 tokens, 0.0% replaced (by <unk>)\n",
            "INFO:fairseq_cli.preprocess:[hi] Dictionary: 7776 types\n",
            "INFO:fairseq_cli.preprocess:[hi] valid.hi: 2000 sents, 60982 tokens, 0.00492% replaced (by <unk>)\n",
            "INFO:fairseq_cli.preprocess:[hi] Dictionary: 7776 types\n",
            "INFO:fairseq_cli.preprocess:[hi] valid.hi: 2000 sents, 60982 tokens, 0.00492% replaced (by <unk>)\n",
            "INFO:fairseq_cli.preprocess:Wrote preprocessed data to data-bin/trial\n"
          ]
        }
      ],
      "source": [
        "! python fairseq/fairseq_cli/preprocess.py \\\n",
        "    --joined-dictionary \\\n",
        "    --source-lang en --target-lang hi \\\n",
        "    --trainpref train --validpref valid --testpref valid \\\n",
        "    --destdir data-bin/trial --thresholdtgt 0 --thresholdsrc 0 \\\n",
        "    --workers 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_F8k7167up78",
        "outputId": "78695fff-e36a-4bf8-ce0a-80ad4421f54f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict.en.txt        test.en-hi.en.idx   train.en-hi.en.idx  valid.en-hi.en.idx\n",
            "dict.hi.txt        test.en-hi.hi.bin   train.en-hi.hi.bin  valid.en-hi.hi.bin\n",
            "preprocess.log     test.en-hi.hi.idx   train.en-hi.hi.idx  valid.en-hi.hi.idx\n",
            "test.en-hi.en.bin  train.en-hi.en.bin  valid.en-hi.en.bin\n"
          ]
        }
      ],
      "source": [
        "ls data-bin/trial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwZtWm12DXqF"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThmR6CUikOCr"
      },
      "source": [
        "# Training\n",
        "## Parameters to fix for your corpora and language pair\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "    --encoder-embed-dim\t128 --encoder-ffn-embed-dim\t128 \\\n",
        "    --encoder-layers\t2 --encoder-attention-heads\t2 \\\n",
        "    --decoder-embed-dim\t128 --decoder-ffn-embed-dim\t128 \\\n",
        "    --decoder-layers\t2 --decoder-attention-heads\t2 \\\n",
        "    --dropout 0.3 --weight-decay 0.0 \\\n",
        "    --max-update 4000 \\\n",
        "    --keep-last-epochs\t10 \\\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RonQ8-I1yaPB",
        "outputId": "5f90d9fc-9951-4500-acff-9d7235579ec0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-08-23 04:58:57.447848: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-23 04:58:59.738396: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-08-23 04:59:01 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2023-08-23 04:59:01 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2023-08-23 04:59:03 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 4000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [0.01], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'trained_models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 10, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', simul_type=None, scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_iwslt_de_en', max_epoch=0, max_update=4000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[16], lr=[0.01], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='trained_models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=10, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin/trial', source_lang='en', target_lang='hi', load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=10, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_all_embeddings=True, encoder_embed_dim=128, encoder_ffn_embed_dim=128, encoder_layers=2, encoder_attention_heads=2, decoder_embed_dim=128, decoder_ffn_embed_dim=128, decoder_layers=2, decoder_attention_heads=2, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=128, decoder_input_dim=128, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_iwslt_de_en'), 'task': {'_name': 'translation', 'data': 'data-bin/trial', 'source_lang': 'en', 'target_lang': 'hi', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.01]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 10, 'warmup_init_lr': -1.0, 'lr': [0.01]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2023-08-23 04:59:03 | INFO | fairseq.tasks.translation | [en] dictionary: 7776 types\n",
            "2023-08-23 04:59:03 | INFO | fairseq.tasks.translation | [hi] dictionary: 7776 types\n",
            "2023-08-23 04:59:04 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(7776, 128, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(7776, 128, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=128, out_features=7776, bias=False)\n",
            "  )\n",
            ")\n",
            "2023-08-23 04:59:04 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2023-08-23 04:59:04 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2023-08-23 04:59:04 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2023-08-23 04:59:04 | INFO | fairseq_cli.train | num. shared model params: 1,526,272 (num. trained: 1,526,272)\n",
            "2023-08-23 04:59:04 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-08-23 04:59:04 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data-bin/trial/valid.en-hi.en\n",
            "2023-08-23 04:59:04 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data-bin/trial/valid.en-hi.hi\n",
            "2023-08-23 04:59:04 | INFO | fairseq.tasks.translation | data-bin/trial valid en-hi 2000 examples\n",
            "2023-08-23 04:59:12 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n",
            "2023-08-23 04:59:12 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2023-08-23 04:59:12 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-08-23 04:59:12 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-08-23 04:59:12 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-08-23 04:59:12 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-08-23 04:59:12 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
            "2023-08-23 04:59:12 | INFO | fairseq.trainer | Preparing to load checkpoint trained_models/checkpoint_last.pt\n",
            "2023-08-23 04:59:12 | INFO | fairseq.trainer | No existing checkpoint found trained_models/checkpoint_last.pt\n",
            "2023-08-23 04:59:12 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-08-23 04:59:12 | INFO | fairseq.data.data_utils | loaded 56,832 examples from: data-bin/trial/train.en-hi.en\n",
            "2023-08-23 04:59:12 | INFO | fairseq.data.data_utils | loaded 56,832 examples from: data-bin/trial/train.en-hi.hi\n",
            "2023-08-23 04:59:12 | INFO | fairseq.tasks.translation | data-bin/trial train en-hi 56832 examples\n",
            "2023-08-23 04:59:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 04:59:12 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
            "2023-08-23 04:59:12 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
            "2023-08-23 04:59:12 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
            "2023-08-23 04:59:12 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
            "2023-08-23 04:59:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 04:59:12 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
            "2023-08-23 04:59:12 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
            "2023-08-23 04:59:12 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
            "2023-08-23 04:59:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 001:   0% 0/32 [00:00<?, ?it/s]2023-08-23 04:59:12 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-08-23 04:59:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "2023-08-23 04:59:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
            "epoch 001:   3% 1/32 [00:03<02:02,  3.96s/it]2023-08-23 04:59:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
            "epoch 001:   6% 2/32 [00:04<01:02,  2.07s/it]2023-08-23 04:59:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
            "epoch 001:  47% 15/32 [00:11<00:10,  1.63it/s]2023-08-23 04:59:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
            "epoch 001:  97% 31/32 [00:20<00:00,  2.19it/s]2023-08-23 04:59:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 04:59:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 33.06it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 38.74it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 41.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  93% 25/27 [00:00<00:00, 64.60it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 04:59:33 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.7 | nll_loss 10.187 | ppl 1165.78 | wps 138065 | wpb 2258.6 | bsz 74.1 | num_updates 28\n",
            "2023-08-23 04:59:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 28 updates\n",
            "2023-08-23 04:59:33 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint1.pt\n",
            "2023-08-23 04:59:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint1.pt\n",
            "2023-08-23 04:59:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint1.pt (epoch 1 @ 28 updates, score 10.7) (writing took 0.12042452799994408 seconds)\n",
            "2023-08-23 04:59:33 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-08-23 04:59:34 | INFO | train | epoch 001 | loss 11.113 | nll_loss 10.689 | ppl 1650.86 | wps 94332.7 | ups 1.79 | wpb 52588.9 | bsz 1776.9 | num_updates 28 | lr 0.00597614 | gnorm 0.567 | loss_scale 8 | train_wall 19 | gb_free 14.3 | wall 22\n",
            "2023-08-23 04:59:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 04:59:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 002:   0% 0/32 [00:00<?, ?it/s]2023-08-23 04:59:34 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-08-23 04:59:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  97% 31/32 [00:16<00:00,  2.21it/s]2023-08-23 04:59:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 04:59:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 36.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 39.48it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 43.76it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  89% 24/27 [00:00<00:00, 64.75it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 04:59:50 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.676 | nll_loss 10.175 | ppl 1156.06 | wps 140176 | wpb 2258.6 | bsz 74.1 | num_updates 60 | best_loss 10.676\n",
            "2023-08-23 04:59:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 60 updates\n",
            "2023-08-23 04:59:50 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint2.pt\n",
            "2023-08-23 04:59:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint2.pt\n",
            "2023-08-23 04:59:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint2.pt (epoch 2 @ 60 updates, score 10.676) (writing took 0.1358469919999834 seconds)\n",
            "2023-08-23 04:59:51 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-08-23 04:59:51 | INFO | train | epoch 002 | loss 10.445 | nll_loss 9.957 | ppl 993.91 | wps 98968 | ups 1.88 | wpb 52622.6 | bsz 1776 | num_updates 60 | lr 0.00408248 | gnorm 0.12 | loss_scale 8 | train_wall 15 | gb_free 14.3 | wall 39\n",
            "2023-08-23 04:59:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 04:59:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 003:   0% 0/32 [00:00<?, ?it/s]2023-08-23 04:59:51 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2023-08-23 04:59:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  97% 31/32 [00:16<00:00,  2.19it/s]2023-08-23 05:00:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:00:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 36.31it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 34.25it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  44% 12/27 [00:00<00:00, 31.82it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  67% 18/27 [00:00<00:00, 38.99it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  89% 24/27 [00:00<00:00, 45.50it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:00:07 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.629 | nll_loss 10.126 | ppl 1117.75 | wps 100442 | wpb 2258.6 | bsz 74.1 | num_updates 92 | best_loss 10.629\n",
            "2023-08-23 05:00:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 92 updates\n",
            "2023-08-23 05:00:07 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint3.pt\n",
            "2023-08-23 05:00:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint3.pt\n",
            "2023-08-23 05:00:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint3.pt (epoch 3 @ 92 updates, score 10.629) (writing took 0.18067500799998015 seconds)\n",
            "2023-08-23 05:00:08 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2023-08-23 05:00:08 | INFO | train | epoch 003 | loss 10.387 | nll_loss 9.904 | ppl 957.91 | wps 98488.5 | ups 1.87 | wpb 52622.6 | bsz 1776 | num_updates 92 | lr 0.0032969 | gnorm 0.08 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 56\n",
            "2023-08-23 05:00:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:00:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 004:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:00:08 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2023-08-23 05:00:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:  97% 31/32 [00:17<00:00,  1.60it/s, loss=10.61, nll_loss=10.142, ppl=1129.59, wps=95994.7, ups=1.83, wpb=52570.8, bsz=1763.6, num_updates=100, lr=0.00316228, gnorm=0.229, loss_scale=8, train_wall=52, gb_free=14.3, wall=61]2023-08-23 05:00:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:00:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 24.87it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  26% 7/27 [00:00<00:00, 28.71it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 27.58it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  48% 13/27 [00:00<00:00, 27.82it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 28.36it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  85% 23/27 [00:00<00:00, 41.27it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:00:26 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.642 | nll_loss 10.149 | ppl 1135.6 | wps 86763.2 | wpb 2258.6 | bsz 74.1 | num_updates 124 | best_loss 10.629\n",
            "2023-08-23 05:00:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 124 updates\n",
            "2023-08-23 05:00:26 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint4.pt\n",
            "2023-08-23 05:00:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint4.pt\n",
            "2023-08-23 05:00:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint4.pt (epoch 4 @ 124 updates, score 10.642) (writing took 0.12844100699999217 seconds)\n",
            "2023-08-23 05:00:26 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2023-08-23 05:00:26 | INFO | train | epoch 004 | loss 10.369 | nll_loss 9.887 | ppl 946.84 | wps 92635.8 | ups 1.76 | wpb 52622.6 | bsz 1776 | num_updates 124 | lr 0.00283981 | gnorm 0.074 | loss_scale 8 | train_wall 15 | gb_free 14.3 | wall 74\n",
            "2023-08-23 05:00:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:00:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 005:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:00:26 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2023-08-23 05:00:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  97% 31/32 [00:16<00:00,  1.90it/s]2023-08-23 05:00:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:00:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 36.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 35.55it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  48% 13/27 [00:00<00:00, 40.12it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  78% 21/27 [00:00<00:00, 53.97it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:00:43 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.632 | nll_loss 10.141 | ppl 1128.92 | wps 130960 | wpb 2258.6 | bsz 74.1 | num_updates 156 | best_loss 10.629\n",
            "2023-08-23 05:00:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 156 updates\n",
            "2023-08-23 05:00:43 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint5.pt\n",
            "2023-08-23 05:00:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint5.pt\n",
            "2023-08-23 05:00:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint5.pt (epoch 5 @ 156 updates, score 10.632) (writing took 0.09274524200009182 seconds)\n",
            "2023-08-23 05:00:43 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2023-08-23 05:00:43 | INFO | train | epoch 005 | loss 10.358 | nll_loss 9.877 | ppl 940.15 | wps 97935.6 | ups 1.86 | wpb 52622.6 | bsz 1776 | num_updates 156 | lr 0.00253185 | gnorm 0.069 | loss_scale 8 | train_wall 15 | gb_free 14.3 | wall 91\n",
            "2023-08-23 05:00:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:00:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 006:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:00:43 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2023-08-23 05:00:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006:  97% 31/32 [00:18<00:00,  2.09it/s]2023-08-23 05:01:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:01:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 34.34it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 41.09it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 40.54it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  74% 20/27 [00:00<00:00, 47.34it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:01:02 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 10.638 | nll_loss 10.145 | ppl 1132.53 | wps 123944 | wpb 2258.6 | bsz 74.1 | num_updates 188 | best_loss 10.629\n",
            "2023-08-23 05:01:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 188 updates\n",
            "2023-08-23 05:01:02 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint6.pt\n",
            "2023-08-23 05:01:02 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint6.pt\n",
            "2023-08-23 05:01:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint6.pt (epoch 6 @ 188 updates, score 10.638) (writing took 0.09029692399997202 seconds)\n",
            "2023-08-23 05:01:02 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2023-08-23 05:01:02 | INFO | train | epoch 006 | loss 10.353 | nll_loss 9.872 | ppl 936.77 | wps 88944.5 | ups 1.69 | wpb 52622.6 | bsz 1776 | num_updates 188 | lr 0.00230633 | gnorm 0.073 | loss_scale 8 | train_wall 16 | gb_free 14.3 | wall 110\n",
            "2023-08-23 05:01:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:01:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 007:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:01:02 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2023-08-23 05:01:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007:  97% 31/32 [00:16<00:00,  2.12it/s, loss=10.356, nll_loss=9.874, ppl=938.52, wps=94131.8, ups=1.78, wpb=52795.7, bsz=1792.3, num_updates=200, lr=0.00223607, gnorm=0.071, loss_scale=8, train_wall=48, gb_free=14.3, wall=117]2023-08-23 05:01:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:01:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 28.42it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 40.61it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 44.75it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  81% 22/27 [00:00<00:00, 57.20it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:01:19 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 10.616 | nll_loss 10.137 | ppl 1126.12 | wps 133630 | wpb 2258.6 | bsz 74.1 | num_updates 220 | best_loss 10.616\n",
            "2023-08-23 05:01:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 220 updates\n",
            "2023-08-23 05:01:19 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint7.pt\n",
            "2023-08-23 05:01:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint7.pt\n",
            "2023-08-23 05:01:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint7.pt (epoch 7 @ 220 updates, score 10.616) (writing took 0.1465111300000217 seconds)\n",
            "2023-08-23 05:01:19 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2023-08-23 05:01:19 | INFO | train | epoch 007 | loss 10.349 | nll_loss 9.867 | ppl 933.59 | wps 97199.1 | ups 1.85 | wpb 52622.6 | bsz 1776 | num_updates 220 | lr 0.00213201 | gnorm 0.07 | loss_scale 8 | train_wall 15 | gb_free 14.3 | wall 127\n",
            "2023-08-23 05:01:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:01:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 008:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:01:19 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2023-08-23 05:01:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008:  97% 31/32 [00:16<00:00,  1.89it/s]2023-08-23 05:01:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:01:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 25.26it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  26% 7/27 [00:00<00:00, 28.44it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 28.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 36.30it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  74% 20/27 [00:00<00:00, 33.82it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:01:37 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 10.625 | nll_loss 10.147 | ppl 1133.68 | wps 89545.6 | wpb 2258.6 | bsz 74.1 | num_updates 252 | best_loss 10.616\n",
            "2023-08-23 05:01:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 252 updates\n",
            "2023-08-23 05:01:37 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint8.pt\n",
            "2023-08-23 05:01:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint8.pt\n",
            "2023-08-23 05:01:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint8.pt (epoch 8 @ 252 updates, score 10.625) (writing took 0.13942720399995778 seconds)\n",
            "2023-08-23 05:01:37 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2023-08-23 05:01:37 | INFO | train | epoch 008 | loss 10.346 | nll_loss 9.864 | ppl 931.87 | wps 94493 | ups 1.8 | wpb 52622.6 | bsz 1776 | num_updates 252 | lr 0.00199205 | gnorm 0.069 | loss_scale 8 | train_wall 15 | gb_free 14.3 | wall 145\n",
            "2023-08-23 05:01:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:01:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 009:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:01:37 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2023-08-23 05:01:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009:  97% 31/32 [00:17<00:00,  1.62it/s]2023-08-23 05:01:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:01:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 36.64it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 45.43it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 43.36it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  93% 25/27 [00:00<00:00, 62.45it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:01:55 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 10.624 | nll_loss 10.136 | ppl 1125.44 | wps 136756 | wpb 2258.6 | bsz 74.1 | num_updates 284 | best_loss 10.616\n",
            "2023-08-23 05:01:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 284 updates\n",
            "2023-08-23 05:01:55 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint9.pt\n",
            "2023-08-23 05:01:55 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint9.pt\n",
            "2023-08-23 05:01:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint9.pt (epoch 9 @ 284 updates, score 10.624) (writing took 0.08602204999999685 seconds)\n",
            "2023-08-23 05:01:55 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2023-08-23 05:01:55 | INFO | train | epoch 009 | loss 10.344 | nll_loss 9.862 | ppl 930.85 | wps 91915.3 | ups 1.75 | wpb 52622.6 | bsz 1776 | num_updates 284 | lr 0.00187647 | gnorm 0.067 | loss_scale 8 | train_wall 16 | gb_free 14.3 | wall 164\n",
            "2023-08-23 05:01:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:01:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 010:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:01:55 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2023-08-23 05:01:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010:  97% 31/32 [00:16<00:00,  2.03it/s, loss=10.344, nll_loss=9.863, ppl=931.18, wps=95986.7, ups=1.83, wpb=52498, bsz=1770, num_updates=300, lr=0.00182574, gnorm=0.068, loss_scale=8, train_wall=47, gb_free=14.3, wall=172]2023-08-23 05:02:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:02:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 39.71it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 40.79it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 43.45it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  78% 21/27 [00:00<00:00, 52.71it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:02:13 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 10.631 | nll_loss 10.149 | ppl 1135.62 | wps 131768 | wpb 2258.6 | bsz 74.1 | num_updates 316 | best_loss 10.616\n",
            "2023-08-23 05:02:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 316 updates\n",
            "2023-08-23 05:02:13 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint10.pt\n",
            "2023-08-23 05:02:13 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint10.pt\n",
            "2023-08-23 05:02:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint10.pt (epoch 10 @ 316 updates, score 10.631) (writing took 0.0978036809999594 seconds)\n",
            "2023-08-23 05:02:13 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2023-08-23 05:02:13 | INFO | train | epoch 010 | loss 10.341 | nll_loss 9.859 | ppl 928.78 | wps 95682.9 | ups 1.82 | wpb 52622.6 | bsz 1776 | num_updates 316 | lr 0.00177892 | gnorm 0.061 | loss_scale 8 | train_wall 15 | gb_free 14.3 | wall 181\n",
            "2023-08-23 05:02:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:02:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 011:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:02:13 | INFO | fairseq.trainer | begin training epoch 11\n",
            "2023-08-23 05:02:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 011:  66% 21/32 [00:11<00:06,  1.75it/s]2023-08-23 05:02:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0\n",
            "epoch 011:  97% 31/32 [00:16<00:00,  2.14it/s]2023-08-23 05:02:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:02:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 43.03it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 44.17it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 44.97it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  93% 25/27 [00:00<00:00, 65.13it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:02:30 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 11.633 | nll_loss 11.17 | ppl 2303.97 | wps 141242 | wpb 2258.6 | bsz 74.1 | num_updates 347 | best_loss 10.616\n",
            "2023-08-23 05:02:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 347 updates\n",
            "2023-08-23 05:02:30 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint11.pt\n",
            "2023-08-23 05:02:30 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint11.pt\n",
            "2023-08-23 05:02:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint11.pt (epoch 11 @ 347 updates, score 11.633) (writing took 0.0924728589999404 seconds)\n",
            "2023-08-23 05:02:30 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2023-08-23 05:02:30 | INFO | train | epoch 011 | loss 10.179 | nll_loss 9.672 | ppl 815.69 | wps 94695 | ups 1.8 | wpb 52521.1 | bsz 1774.5 | num_updates 347 | lr 0.0016976 | gnorm 0.267 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 198\n",
            "2023-08-23 05:02:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:02:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 012:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:02:30 | INFO | fairseq.trainer | begin training epoch 12\n",
            "2023-08-23 05:02:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 012:  97% 31/32 [00:16<00:00,  2.22it/s]2023-08-23 05:02:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:02:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 39.63it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 41.91it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 43.79it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  81% 22/27 [00:00<00:00, 56.22it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:02:47 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 10.909 | nll_loss 10.379 | ppl 1332.07 | wps 133489 | wpb 2258.6 | bsz 74.1 | num_updates 379 | best_loss 10.616\n",
            "2023-08-23 05:02:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 379 updates\n",
            "2023-08-23 05:02:47 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint12.pt\n",
            "2023-08-23 05:02:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint12.pt\n",
            "2023-08-23 05:02:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint12.pt (epoch 12 @ 379 updates, score 10.909) (writing took 0.0842133000001013 seconds)\n",
            "2023-08-23 05:02:47 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2023-08-23 05:02:48 | INFO | train | epoch 012 | loss 10 | nll_loss 9.47 | ppl 709.37 | wps 97219.1 | ups 1.85 | wpb 52622.6 | bsz 1776 | num_updates 379 | lr 0.00162435 | gnorm 0.138 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 216\n",
            "2023-08-23 05:02:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:02:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 013:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:02:48 | INFO | fairseq.trainer | begin training epoch 13\n",
            "2023-08-23 05:02:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 013:  97% 31/32 [00:16<00:00,  1.86it/s, loss=10.083, nll_loss=9.564, ppl=756.71, wps=95140.5, ups=1.8, wpb=52888.8, bsz=1791.9, num_updates=400, lr=0.00158114, gnorm=0.168, loss_scale=4, train_wall=48, gb_free=14.3, wall=227]2023-08-23 05:03:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:03:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 013 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 29.74it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  26% 7/27 [00:00<00:00, 32.35it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  41% 11/27 [00:00<00:00, 32.96it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 31.95it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  70% 19/27 [00:00<00:00, 34.17it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset: 100% 27/27 [00:00<00:00, 47.98it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:03:05 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 10.531 | nll_loss 9.981 | ppl 1010.74 | wps 95323.8 | wpb 2258.6 | bsz 74.1 | num_updates 411 | best_loss 10.531\n",
            "2023-08-23 05:03:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 411 updates\n",
            "2023-08-23 05:03:05 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint13.pt\n",
            "2023-08-23 05:03:05 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint13.pt\n",
            "2023-08-23 05:03:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint13.pt (epoch 13 @ 411 updates, score 10.531) (writing took 0.16978855600007137 seconds)\n",
            "2023-08-23 05:03:05 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
            "2023-08-23 05:03:05 | INFO | train | epoch 013 | loss 9.859 | nll_loss 9.308 | ppl 633.83 | wps 96078.7 | ups 1.83 | wpb 52622.6 | bsz 1776 | num_updates 411 | lr 0.00155984 | gnorm 0.139 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 233\n",
            "2023-08-23 05:03:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:03:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 014:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:03:05 | INFO | fairseq.trainer | begin training epoch 14\n",
            "2023-08-23 05:03:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 014:  97% 31/32 [00:17<00:00,  1.67it/s]2023-08-23 05:03:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:03:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 014 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 35.10it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 46.62it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 42.50it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  89% 24/27 [00:00<00:00, 59.22it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:03:23 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 10.218 | nll_loss 9.635 | ppl 795 | wps 134160 | wpb 2258.6 | bsz 74.1 | num_updates 443 | best_loss 10.218\n",
            "2023-08-23 05:03:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 443 updates\n",
            "2023-08-23 05:03:23 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint14.pt\n",
            "2023-08-23 05:03:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint14.pt\n",
            "2023-08-23 05:03:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint14.pt (epoch 14 @ 443 updates, score 10.218) (writing took 0.12328151499991691 seconds)\n",
            "2023-08-23 05:03:23 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
            "2023-08-23 05:03:23 | INFO | train | epoch 014 | loss 9.662 | nll_loss 9.079 | ppl 540.73 | wps 94084 | ups 1.79 | wpb 52622.6 | bsz 1776 | num_updates 443 | lr 0.00150244 | gnorm 0.139 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 251\n",
            "2023-08-23 05:03:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:03:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 015:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:03:23 | INFO | fairseq.trainer | begin training epoch 15\n",
            "2023-08-23 05:03:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 015:  97% 31/32 [00:16<00:00,  2.16it/s]2023-08-23 05:03:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:03:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 015 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 38.36it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 42.76it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 44.49it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  74% 20/27 [00:00<00:00, 50.28it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:03:40 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 10.072 | nll_loss 9.451 | ppl 699.77 | wps 131168 | wpb 2258.6 | bsz 74.1 | num_updates 475 | best_loss 10.072\n",
            "2023-08-23 05:03:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 475 updates\n",
            "2023-08-23 05:03:40 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint15.pt\n",
            "2023-08-23 05:03:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint15.pt\n",
            "2023-08-23 05:03:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint15.pt (epoch 15 @ 475 updates, score 10.072) (writing took 0.12135380699999132 seconds)\n",
            "2023-08-23 05:03:40 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
            "2023-08-23 05:03:40 | INFO | train | epoch 015 | loss 9.488 | nll_loss 8.875 | ppl 469.39 | wps 97502.4 | ups 1.85 | wpb 52622.6 | bsz 1776 | num_updates 475 | lr 0.00145095 | gnorm 0.12 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 268\n",
            "2023-08-23 05:03:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:03:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 016:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:03:40 | INFO | fairseq.trainer | begin training epoch 16\n",
            "2023-08-23 05:03:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 016:  97% 31/32 [00:16<00:00,  2.22it/s, loss=9.544, nll_loss=8.94, ppl=491.12, wps=96445.2, ups=1.83, wpb=52626.4, bsz=1771.4, num_updates=500, lr=0.00141421, gnorm=0.134, loss_scale=4, train_wall=47, gb_free=14.3, wall=282]2023-08-23 05:03:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:03:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 016 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 39.23it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 44.38it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 46.61it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  93% 25/27 [00:00<00:00, 65.99it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:03:57 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 9.911 | nll_loss 9.261 | ppl 613.63 | wps 141954 | wpb 2258.6 | bsz 74.1 | num_updates 507 | best_loss 9.911\n",
            "2023-08-23 05:03:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 507 updates\n",
            "2023-08-23 05:03:57 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint16.pt\n",
            "2023-08-23 05:03:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint16.pt\n",
            "2023-08-23 05:03:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint16.pt (epoch 16 @ 507 updates, score 9.911) (writing took 0.15679565499999626 seconds)\n",
            "2023-08-23 05:03:57 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
            "2023-08-23 05:03:57 | INFO | train | epoch 016 | loss 9.332 | nll_loss 8.692 | ppl 413.52 | wps 99562.7 | ups 1.89 | wpb 52622.6 | bsz 1776 | num_updates 507 | lr 0.00140442 | gnorm 0.162 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 285\n",
            "2023-08-23 05:03:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:03:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 017:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:03:57 | INFO | fairseq.trainer | begin training epoch 17\n",
            "2023-08-23 05:03:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 017:  97% 31/32 [00:17<00:00,  1.67it/s]2023-08-23 05:04:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:04:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 017 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 36.01it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 32.49it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 42.79it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  78% 21/27 [00:00<00:00, 42.10it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:04:15 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 9.742 | nll_loss 9.064 | ppl 535.25 | wps 105810 | wpb 2258.6 | bsz 74.1 | num_updates 539 | best_loss 9.742\n",
            "2023-08-23 05:04:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 539 updates\n",
            "2023-08-23 05:04:15 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint17.pt\n",
            "2023-08-23 05:04:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint17.pt\n",
            "2023-08-23 05:04:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint17.pt (epoch 17 @ 539 updates, score 9.742) (writing took 0.18110398499993607 seconds)\n",
            "2023-08-23 05:04:16 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
            "2023-08-23 05:04:16 | INFO | train | epoch 017 | loss 9.146 | nll_loss 8.476 | ppl 356.13 | wps 90971.7 | ups 1.73 | wpb 52622.6 | bsz 1776 | num_updates 539 | lr 0.00136209 | gnorm 0.172 | loss_scale 4 | train_wall 16 | gb_free 14.3 | wall 304\n",
            "2023-08-23 05:04:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:04:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 018:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:04:16 | INFO | fairseq.trainer | begin training epoch 18\n",
            "2023-08-23 05:04:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 018:  97% 31/32 [00:16<00:00,  1.71it/s]2023-08-23 05:04:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:04:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 018 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  11% 3/27 [00:00<00:01, 23.66it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  26% 7/27 [00:00<00:00, 28.21it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 28.33it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  48% 13/27 [00:00<00:00, 27.64it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  63% 17/27 [00:00<00:00, 31.59it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  93% 25/27 [00:00<00:00, 45.44it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:04:33 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 9.599 | nll_loss 8.897 | ppl 476.69 | wps 91051.7 | wpb 2258.6 | bsz 74.1 | num_updates 571 | best_loss 9.599\n",
            "2023-08-23 05:04:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 571 updates\n",
            "2023-08-23 05:04:33 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint18.pt\n",
            "2023-08-23 05:04:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint18.pt\n",
            "2023-08-23 05:04:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint18.pt (epoch 18 @ 571 updates, score 9.599) (writing took 0.21147984300000644 seconds)\n",
            "2023-08-23 05:04:34 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
            "2023-08-23 05:04:34 | INFO | train | epoch 018 | loss 8.974 | nll_loss 8.274 | ppl 309.58 | wps 93299.7 | ups 1.77 | wpb 52622.6 | bsz 1776 | num_updates 571 | lr 0.00132337 | gnorm 0.2 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 322\n",
            "2023-08-23 05:04:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:04:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 019:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:04:34 | INFO | fairseq.trainer | begin training epoch 19\n",
            "2023-08-23 05:04:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 019:  97% 31/32 [00:16<00:00,  1.92it/s, loss=9.008, nll_loss=8.315, ppl=318.39, wps=94817.8, ups=1.8, wpb=52629, bsz=1780.6, num_updates=600, lr=0.00129099, gnorm=0.208, loss_scale=4, train_wall=47, gb_free=14.3, wall=337]2023-08-23 05:04:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:04:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 019 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 39.25it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 44.91it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 44.63it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  81% 22/27 [00:00<00:00, 57.54it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:04:51 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 9.453 | nll_loss 8.735 | ppl 426.19 | wps 137990 | wpb 2258.6 | bsz 74.1 | num_updates 603 | best_loss 9.453\n",
            "2023-08-23 05:04:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 603 updates\n",
            "2023-08-23 05:04:51 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint19.pt\n",
            "2023-08-23 05:04:51 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint19.pt\n",
            "2023-08-23 05:04:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint19.pt (epoch 19 @ 603 updates, score 9.453) (writing took 0.12007140599996546 seconds)\n",
            "2023-08-23 05:04:51 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
            "2023-08-23 05:04:51 | INFO | train | epoch 019 | loss 8.842 | nll_loss 8.119 | ppl 277.93 | wps 98924.5 | ups 1.88 | wpb 52622.6 | bsz 1776 | num_updates 603 | lr 0.00128778 | gnorm 0.258 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 339\n",
            "2023-08-23 05:04:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:04:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 020:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:04:51 | INFO | fairseq.trainer | begin training epoch 20\n",
            "2023-08-23 05:04:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 020:  97% 31/32 [00:16<00:00,  2.13it/s]2023-08-23 05:05:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:05:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 020 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 36.53it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 47.25it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 47.27it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  89% 24/27 [00:00<00:00, 62.26it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:05:07 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 9.299 | nll_loss 8.566 | ppl 378.93 | wps 140215 | wpb 2258.6 | bsz 74.1 | num_updates 635 | best_loss 9.299\n",
            "2023-08-23 05:05:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 635 updates\n",
            "2023-08-23 05:05:07 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint20.pt\n",
            "2023-08-23 05:05:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint20.pt\n",
            "2023-08-23 05:05:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint20.pt (epoch 20 @ 635 updates, score 9.299) (writing took 0.12767358099995363 seconds)\n",
            "2023-08-23 05:05:07 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
            "2023-08-23 05:05:07 | INFO | train | epoch 020 | loss 8.722 | nll_loss 7.978 | ppl 252.14 | wps 100257 | ups 1.91 | wpb 52622.6 | bsz 1776 | num_updates 635 | lr 0.00125491 | gnorm 0.182 | loss_scale 4 | train_wall 14 | gb_free 14.3 | wall 356\n",
            "2023-08-23 05:05:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:05:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 021:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:05:08 | INFO | fairseq.trainer | begin training epoch 21\n",
            "2023-08-23 05:05:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 021:  97% 31/32 [00:16<00:00,  2.17it/s]2023-08-23 05:05:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:05:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 021 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 35.93it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 45.22it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 44.50it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  93% 25/27 [00:00<00:00, 63.15it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:05:24 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 9.198 | nll_loss 8.448 | ppl 349.13 | wps 136360 | wpb 2258.6 | bsz 74.1 | num_updates 667 | best_loss 9.198\n",
            "2023-08-23 05:05:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 667 updates\n",
            "2023-08-23 05:05:24 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint21.pt\n",
            "2023-08-23 05:05:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint21.pt\n",
            "2023-08-23 05:05:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint21.pt (epoch 21 @ 667 updates, score 9.198) (writing took 0.11619590800000879 seconds)\n",
            "2023-08-23 05:05:24 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
            "2023-08-23 05:05:24 | INFO | train | epoch 021 | loss 8.616 | nll_loss 7.853 | ppl 231.24 | wps 99563.5 | ups 1.89 | wpb 52622.6 | bsz 1776 | num_updates 667 | lr 0.00122444 | gnorm 0.209 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 373\n",
            "2023-08-23 05:05:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:05:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 022:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:05:24 | INFO | fairseq.trainer | begin training epoch 22\n",
            "2023-08-23 05:05:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 022:  97% 31/32 [00:16<00:00,  2.17it/s]2023-08-23 05:05:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:05:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 022 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 39.42it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 44.74it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 46.76it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  89% 24/27 [00:00<00:00, 62.16it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:05:41 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 9.09 | nll_loss 8.322 | ppl 319.98 | wps 140647 | wpb 2258.6 | bsz 74.1 | num_updates 699 | best_loss 9.09\n",
            "2023-08-23 05:05:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 699 updates\n",
            "2023-08-23 05:05:41 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint22.pt\n",
            "2023-08-23 05:05:41 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint22.pt\n",
            "2023-08-23 05:05:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint22.pt (epoch 22 @ 699 updates, score 9.09) (writing took 0.13949002899994412 seconds)\n",
            "2023-08-23 05:05:41 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
            "2023-08-23 05:05:41 | INFO | train | epoch 022 | loss 8.513 | nll_loss 7.733 | ppl 212.8 | wps 98989.7 | ups 1.88 | wpb 52622.6 | bsz 1776 | num_updates 699 | lr 0.00119608 | gnorm 0.217 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 390\n",
            "2023-08-23 05:05:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:05:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 023:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:05:41 | INFO | fairseq.trainer | begin training epoch 23\n",
            "2023-08-23 05:05:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 023:  97% 31/32 [00:16<00:00,  1.98it/s, loss=8.618, nll_loss=7.857, ppl=231.77, wps=98683.8, ups=1.89, wpb=52331.7, bsz=1769.1, num_updates=700, lr=0.00119523, gnorm=0.205, loss_scale=4, train_wall=45, gb_free=14.3, wall=390]2023-08-23 05:05:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:05:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 023 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:   7% 2/27 [00:00<00:01, 18.79it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 24.62it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 28.52it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  48% 13/27 [00:00<00:00, 31.51it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  74% 20/27 [00:00<00:00, 43.85it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:05:59 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 9.012 | nll_loss 8.222 | ppl 298.62 | wps 101686 | wpb 2258.6 | bsz 74.1 | num_updates 731 | best_loss 9.012\n",
            "2023-08-23 05:05:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 731 updates\n",
            "2023-08-23 05:05:59 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint23.pt\n",
            "2023-08-23 05:05:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint23.pt\n",
            "2023-08-23 05:05:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint23.pt (epoch 23 @ 731 updates, score 9.012) (writing took 0.17851064899991798 seconds)\n",
            "2023-08-23 05:05:59 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
            "2023-08-23 05:05:59 | INFO | train | epoch 023 | loss 8.398 | nll_loss 7.599 | ppl 193.88 | wps 96944.9 | ups 1.84 | wpb 52622.6 | bsz 1776 | num_updates 731 | lr 0.00116961 | gnorm 0.197 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 407\n",
            "2023-08-23 05:05:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:05:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 024:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:05:59 | INFO | fairseq.trainer | begin training epoch 24\n",
            "2023-08-23 05:05:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 024:  97% 31/32 [00:16<00:00,  1.70it/s]2023-08-23 05:06:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:06:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 024 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 33.75it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 41.09it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 43.12it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  78% 21/27 [00:00<00:00, 52.03it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:06:16 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 8.935 | nll_loss 8.131 | ppl 280.33 | wps 127556 | wpb 2258.6 | bsz 74.1 | num_updates 763 | best_loss 8.935\n",
            "2023-08-23 05:06:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 763 updates\n",
            "2023-08-23 05:06:16 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint24.pt\n",
            "2023-08-23 05:06:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint24.pt\n",
            "2023-08-23 05:06:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint24.pt (epoch 24 @ 763 updates, score 8.935) (writing took 0.12257207199991171 seconds)\n",
            "2023-08-23 05:06:17 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
            "2023-08-23 05:06:17 | INFO | train | epoch 024 | loss 8.277 | nll_loss 7.458 | ppl 175.87 | wps 94977.9 | ups 1.8 | wpb 52622.6 | bsz 1776 | num_updates 763 | lr 0.00114482 | gnorm 0.264 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 425\n",
            "2023-08-23 05:06:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:06:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 025:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:06:17 | INFO | fairseq.trainer | begin training epoch 25\n",
            "2023-08-23 05:06:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 025:  97% 31/32 [00:15<00:00,  2.18it/s]2023-08-23 05:06:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:06:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 025 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 30.91it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 39.81it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 42.29it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  89% 24/27 [00:00<00:00, 61.72it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:06:33 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 8.824 | nll_loss 8 | ppl 255.92 | wps 134039 | wpb 2258.6 | bsz 74.1 | num_updates 795 | best_loss 8.824\n",
            "2023-08-23 05:06:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 795 updates\n",
            "2023-08-23 05:06:33 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint25.pt\n",
            "2023-08-23 05:06:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint25.pt\n",
            "2023-08-23 05:06:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint25.pt (epoch 25 @ 795 updates, score 8.824) (writing took 0.12795803499989233 seconds)\n",
            "2023-08-23 05:06:33 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
            "2023-08-23 05:06:33 | INFO | train | epoch 025 | loss 8.154 | nll_loss 7.316 | ppl 159.38 | wps 100729 | ups 1.91 | wpb 52622.6 | bsz 1776 | num_updates 795 | lr 0.00112154 | gnorm 0.252 | loss_scale 4 | train_wall 14 | gb_free 14.3 | wall 441\n",
            "2023-08-23 05:06:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:06:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 026:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:06:33 | INFO | fairseq.trainer | begin training epoch 26\n",
            "2023-08-23 05:06:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 026:  97% 31/32 [00:16<00:00,  2.16it/s, loss=8.267, nll_loss=7.447, ppl=174.48, wps=98171.8, ups=1.86, wpb=52674.2, bsz=1770.7, num_updates=800, lr=0.00111803, gnorm=0.237, loss_scale=4, train_wall=46, gb_free=14.3, wall=444]2023-08-23 05:06:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:06:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 026 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 33.82it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  41% 11/27 [00:00<00:00, 41.90it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 44.15it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset: 100% 27/27 [00:00<00:00, 67.09it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:06:50 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 8.756 | nll_loss 7.916 | ppl 241.58 | wps 134798 | wpb 2258.6 | bsz 74.1 | num_updates 827 | best_loss 8.756\n",
            "2023-08-23 05:06:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 827 updates\n",
            "2023-08-23 05:06:50 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint26.pt\n",
            "2023-08-23 05:06:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint26.pt\n",
            "2023-08-23 05:06:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint26.pt (epoch 26 @ 827 updates, score 8.756) (writing took 0.11910941699989053 seconds)\n",
            "2023-08-23 05:06:50 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
            "2023-08-23 05:06:50 | INFO | train | epoch 026 | loss 8.039 | nll_loss 7.183 | ppl 145.33 | wps 99297.9 | ups 1.89 | wpb 52622.6 | bsz 1776 | num_updates 827 | lr 0.00109963 | gnorm 0.233 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 458\n",
            "2023-08-23 05:06:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:06:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 027:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:06:50 | INFO | fairseq.trainer | begin training epoch 27\n",
            "2023-08-23 05:06:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 027:  97% 31/32 [00:16<00:00,  2.23it/s]2023-08-23 05:07:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:07:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 027 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 37.06it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 43.32it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 46.52it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  85% 23/27 [00:00<00:00, 57.88it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:07:07 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 8.654 | nll_loss 7.8 | ppl 222.9 | wps 135860 | wpb 2258.6 | bsz 74.1 | num_updates 859 | best_loss 8.654\n",
            "2023-08-23 05:07:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 859 updates\n",
            "2023-08-23 05:07:07 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint27.pt\n",
            "2023-08-23 05:07:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint27.pt\n",
            "2023-08-23 05:07:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint27.pt (epoch 27 @ 859 updates, score 8.654) (writing took 0.11554117200012115 seconds)\n",
            "2023-08-23 05:07:07 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
            "2023-08-23 05:07:07 | INFO | train | epoch 027 | loss 7.934 | nll_loss 7.061 | ppl 133.57 | wps 99445.8 | ups 1.89 | wpb 52622.6 | bsz 1776 | num_updates 859 | lr 0.00107896 | gnorm 0.267 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 475\n",
            "2023-08-23 05:07:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:07:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 028:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:07:07 | INFO | fairseq.trainer | begin training epoch 28\n",
            "2023-08-23 05:07:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 028:  97% 31/32 [00:16<00:00,  2.20it/s]2023-08-23 05:07:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:07:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 028 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 26.58it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  26% 7/27 [00:00<00:00, 31.00it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  41% 11/27 [00:00<00:00, 32.91it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 30.65it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  74% 20/27 [00:00<00:00, 35.47it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:07:24 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.538 | nll_loss 7.673 | ppl 204.01 | wps 91909.1 | wpb 2258.6 | bsz 74.1 | num_updates 891 | best_loss 8.538\n",
            "2023-08-23 05:07:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 891 updates\n",
            "2023-08-23 05:07:24 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint28.pt\n",
            "2023-08-23 05:07:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint28.pt\n",
            "2023-08-23 05:07:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint28.pt (epoch 28 @ 891 updates, score 8.538) (writing took 0.18774212500011345 seconds)\n",
            "2023-08-23 05:07:24 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
            "2023-08-23 05:07:24 | INFO | train | epoch 028 | loss 7.838 | nll_loss 6.95 | ppl 123.61 | wps 97899.2 | ups 1.86 | wpb 52622.6 | bsz 1776 | num_updates 891 | lr 0.0010594 | gnorm 0.25 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 493\n",
            "2023-08-23 05:07:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:07:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 029:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:07:24 | INFO | fairseq.trainer | begin training epoch 29\n",
            "2023-08-23 05:07:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 029:  97% 31/32 [00:18<00:00,  1.64it/s, loss=7.914, nll_loss=7.038, ppl=131.38, wps=97395, ups=1.85, wpb=52690, bsz=1782.4, num_updates=900, lr=0.00105409, gnorm=0.247, loss_scale=4, train_wall=46, gb_free=14.3, wall=498]2023-08-23 05:07:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:07:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 029 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 39.46it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 39.22it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 45.78it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  74% 20/27 [00:00<00:00, 50.60it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:07:43 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.484 | nll_loss 7.601 | ppl 194.15 | wps 132234 | wpb 2258.6 | bsz 74.1 | num_updates 923 | best_loss 8.484\n",
            "2023-08-23 05:07:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 923 updates\n",
            "2023-08-23 05:07:43 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint29.pt\n",
            "2023-08-23 05:07:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint29.pt\n",
            "2023-08-23 05:07:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint29.pt (epoch 29 @ 923 updates, score 8.484) (writing took 0.1319578359998559 seconds)\n",
            "2023-08-23 05:07:43 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
            "2023-08-23 05:07:43 | INFO | train | epoch 029 | loss 7.755 | nll_loss 6.853 | ppl 115.57 | wps 87886 | ups 1.67 | wpb 52622.6 | bsz 1776 | num_updates 923 | lr 0.00104088 | gnorm 0.256 | loss_scale 4 | train_wall 17 | gb_free 14.3 | wall 512\n",
            "2023-08-23 05:07:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:07:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 030:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:07:44 | INFO | fairseq.trainer | begin training epoch 30\n",
            "2023-08-23 05:07:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 030:  97% 31/32 [00:16<00:00,  2.07it/s]2023-08-23 05:08:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:08:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 030 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 29.19it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 39.90it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  48% 13/27 [00:00<00:00, 44.13it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  70% 19/27 [00:00<00:00, 49.69it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:08:00 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.462 | nll_loss 7.579 | ppl 191.17 | wps 137701 | wpb 2258.6 | bsz 74.1 | num_updates 955 | best_loss 8.462\n",
            "2023-08-23 05:08:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 955 updates\n",
            "2023-08-23 05:08:00 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint30.pt\n",
            "2023-08-23 05:08:00 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint30.pt\n",
            "2023-08-23 05:08:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint30.pt (epoch 30 @ 955 updates, score 8.462) (writing took 0.11643903499998487 seconds)\n",
            "2023-08-23 05:08:00 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
            "2023-08-23 05:08:00 | INFO | train | epoch 030 | loss 7.686 | nll_loss 6.773 | ppl 109.35 | wps 100252 | ups 1.91 | wpb 52622.6 | bsz 1776 | num_updates 955 | lr 0.00102329 | gnorm 0.309 | loss_scale 4 | train_wall 14 | gb_free 14.3 | wall 528\n",
            "2023-08-23 05:08:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:08:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 031:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:08:00 | INFO | fairseq.trainer | begin training epoch 31\n",
            "2023-08-23 05:08:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 031:  97% 31/32 [00:16<00:00,  2.13it/s]2023-08-23 05:08:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:08:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 031 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 38.40it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 38.47it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  48% 13/27 [00:00<00:00, 43.15it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  70% 19/27 [00:00<00:00, 48.85it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:08:17 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.398 | nll_loss 7.5 | ppl 181.08 | wps 132224 | wpb 2258.6 | bsz 74.1 | num_updates 987 | best_loss 8.398\n",
            "2023-08-23 05:08:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 987 updates\n",
            "2023-08-23 05:08:17 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint31.pt\n",
            "2023-08-23 05:08:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint31.pt\n",
            "2023-08-23 05:08:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint31.pt (epoch 31 @ 987 updates, score 8.398) (writing took 0.13855949300000248 seconds)\n",
            "2023-08-23 05:08:17 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
            "2023-08-23 05:08:17 | INFO | train | epoch 031 | loss 7.62 | nll_loss 6.697 | ppl 103.72 | wps 99155.4 | ups 1.88 | wpb 52622.6 | bsz 1776 | num_updates 987 | lr 0.00100656 | gnorm 0.238 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 545\n",
            "2023-08-23 05:08:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:08:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 032:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:08:17 | INFO | fairseq.trainer | begin training epoch 32\n",
            "2023-08-23 05:08:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 032:  97% 31/32 [00:16<00:00,  2.14it/s, loss=7.67, nll_loss=6.754, ppl=107.95, wps=96863.5, ups=1.84, wpb=52548.6, bsz=1749.6, num_updates=1000, lr=0.001, gnorm=0.272, loss_scale=4, train_wall=47, gb_free=14.3, wall=552]2023-08-23 05:08:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:08:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 032 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 40.72it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  41% 11/27 [00:00<00:00, 48.98it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 48.48it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset: 100% 27/27 [00:00<00:00, 71.30it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:08:34 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.39 | nll_loss 7.487 | ppl 179.45 | wps 146932 | wpb 2258.6 | bsz 74.1 | num_updates 1019 | best_loss 8.39\n",
            "2023-08-23 05:08:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 1019 updates\n",
            "2023-08-23 05:08:34 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint32.pt\n",
            "2023-08-23 05:08:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint32.pt\n",
            "2023-08-23 05:08:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint32.pt (epoch 32 @ 1019 updates, score 8.39) (writing took 0.1181978969998454 seconds)\n",
            "2023-08-23 05:08:34 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
            "2023-08-23 05:08:34 | INFO | train | epoch 032 | loss 7.566 | nll_loss 6.633 | ppl 99.27 | wps 98895 | ups 1.88 | wpb 52622.6 | bsz 1776 | num_updates 1019 | lr 0.000990633 | gnorm 0.244 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 562\n",
            "2023-08-23 05:08:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:08:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 033:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:08:34 | INFO | fairseq.trainer | begin training epoch 33\n",
            "2023-08-23 05:08:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 033:  97% 31/32 [00:16<00:00,  2.15it/s]2023-08-23 05:08:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:08:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 033 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 24.59it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  22% 6/27 [00:00<00:00, 27.05it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 30.10it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 28.65it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  85% 23/27 [00:00<00:00, 46.31it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:08:51 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.314 | nll_loss 7.407 | ppl 169.69 | wps 98991 | wpb 2258.6 | bsz 74.1 | num_updates 1051 | best_loss 8.314\n",
            "2023-08-23 05:08:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1051 updates\n",
            "2023-08-23 05:08:51 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint33.pt\n",
            "2023-08-23 05:08:51 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint33.pt\n",
            "2023-08-23 05:08:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint33.pt (epoch 33 @ 1051 updates, score 8.314) (writing took 0.18472191000000748 seconds)\n",
            "2023-08-23 05:08:51 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
            "2023-08-23 05:08:51 | INFO | train | epoch 033 | loss 7.516 | nll_loss 6.576 | ppl 95.38 | wps 97947.8 | ups 1.86 | wpb 52622.6 | bsz 1776 | num_updates 1051 | lr 0.000975436 | gnorm 0.272 | loss_scale 4 | train_wall 14 | gb_free 14.3 | wall 580\n",
            "2023-08-23 05:08:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:08:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 034:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:08:52 | INFO | fairseq.trainer | begin training epoch 34\n",
            "2023-08-23 05:08:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 034:  97% 31/32 [00:16<00:00,  1.67it/s]2023-08-23 05:09:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:09:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 034 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 26.65it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  26% 7/27 [00:00<00:00, 28.13it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  41% 11/27 [00:00<00:00, 31.14it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 31.33it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  70% 19/27 [00:00<00:00, 33.92it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset: 100% 27/27 [00:00<00:00, 47.43it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:09:09 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.289 | nll_loss 7.378 | ppl 166.29 | wps 91063.5 | wpb 2258.6 | bsz 74.1 | num_updates 1083 | best_loss 8.289\n",
            "2023-08-23 05:09:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 1083 updates\n",
            "2023-08-23 05:09:09 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint34.pt\n",
            "2023-08-23 05:09:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint34.pt\n",
            "2023-08-23 05:09:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint34.pt (epoch 34 @ 1083 updates, score 8.289) (writing took 0.1449726980001742 seconds)\n",
            "2023-08-23 05:09:09 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
            "2023-08-23 05:09:09 | INFO | train | epoch 034 | loss 7.47 | nll_loss 6.523 | ppl 91.97 | wps 94178.7 | ups 1.79 | wpb 52622.6 | bsz 1776 | num_updates 1083 | lr 0.000960917 | gnorm 0.237 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 598\n",
            "2023-08-23 05:09:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:09:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 035:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:09:09 | INFO | fairseq.trainer | begin training epoch 35\n",
            "2023-08-23 05:09:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 035:  97% 31/32 [00:16<00:00,  1.99it/s, loss=7.487, nll_loss=6.543, ppl=93.22, wps=98745.6, ups=1.87, wpb=52817.3, bsz=1808.1, num_updates=1100, lr=0.000953463, gnorm=0.25, loss_scale=4, train_wall=45, gb_free=14.3, wall=606]2023-08-23 05:09:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:09:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 035 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 46.25it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 46.97it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 47.44it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  96% 26/27 [00:00<00:00, 69.77it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:09:26 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.265 | nll_loss 7.355 | ppl 163.73 | wps 148847 | wpb 2258.6 | bsz 74.1 | num_updates 1115 | best_loss 8.265\n",
            "2023-08-23 05:09:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 1115 updates\n",
            "2023-08-23 05:09:26 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint35.pt\n",
            "2023-08-23 05:09:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint35.pt\n",
            "2023-08-23 05:09:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint35.pt (epoch 35 @ 1115 updates, score 8.265) (writing took 0.13035169199997654 seconds)\n",
            "2023-08-23 05:09:26 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
            "2023-08-23 05:09:26 | INFO | train | epoch 035 | loss 7.431 | nll_loss 6.478 | ppl 89.12 | wps 100412 | ups 1.91 | wpb 52622.6 | bsz 1776 | num_updates 1115 | lr 0.000947027 | gnorm 0.271 | loss_scale 4 | train_wall 14 | gb_free 14.3 | wall 614\n",
            "2023-08-23 05:09:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:09:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 036:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:09:26 | INFO | fairseq.trainer | begin training epoch 36\n",
            "2023-08-23 05:09:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 036:  97% 31/32 [00:16<00:00,  2.13it/s]2023-08-23 05:09:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:09:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 036 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 48.33it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  41% 11/27 [00:00<00:00, 50.41it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  63% 17/27 [00:00<00:00, 48.19it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:09:43 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.226 | nll_loss 7.31 | ppl 158.73 | wps 144550 | wpb 2258.6 | bsz 74.1 | num_updates 1147 | best_loss 8.226\n",
            "2023-08-23 05:09:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 1147 updates\n",
            "2023-08-23 05:09:43 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint36.pt\n",
            "2023-08-23 05:09:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint36.pt\n",
            "2023-08-23 05:09:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint36.pt (epoch 36 @ 1147 updates, score 8.226) (writing took 0.1213310470000124 seconds)\n",
            "2023-08-23 05:09:43 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
            "2023-08-23 05:09:43 | INFO | train | epoch 036 | loss 7.393 | nll_loss 6.433 | ppl 86.43 | wps 99934.2 | ups 1.9 | wpb 52622.6 | bsz 1776 | num_updates 1147 | lr 0.000933724 | gnorm 0.242 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 631\n",
            "2023-08-23 05:09:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:09:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 037:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:09:43 | INFO | fairseq.trainer | begin training epoch 37\n",
            "2023-08-23 05:09:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 037:  97% 31/32 [00:16<00:00,  2.17it/s]2023-08-23 05:09:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:09:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 037 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 35.34it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 46.12it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 47.64it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset: 100% 27/27 [00:00<00:00, 69.19it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:10:00 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.181 | nll_loss 7.262 | ppl 153.46 | wps 142775 | wpb 2258.6 | bsz 74.1 | num_updates 1179 | best_loss 8.181\n",
            "2023-08-23 05:10:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 1179 updates\n",
            "2023-08-23 05:10:00 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint37.pt\n",
            "2023-08-23 05:10:00 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint37.pt\n",
            "2023-08-23 05:10:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint37.pt (epoch 37 @ 1179 updates, score 8.181) (writing took 0.12839261299995997 seconds)\n",
            "2023-08-23 05:10:00 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
            "2023-08-23 05:10:00 | INFO | train | epoch 037 | loss 7.359 | nll_loss 6.395 | ppl 84.17 | wps 99496.2 | ups 1.89 | wpb 52622.6 | bsz 1776 | num_updates 1179 | lr 0.000920965 | gnorm 0.248 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 648\n",
            "2023-08-23 05:10:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:10:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 038:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:10:00 | INFO | fairseq.trainer | begin training epoch 38\n",
            "2023-08-23 05:10:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 038:  97% 31/32 [00:16<00:00,  2.20it/s, loss=7.375, nll_loss=6.413, ppl=85.23, wps=97840.9, ups=1.86, wpb=52695, bsz=1773.5, num_updates=1200, lr=0.000912871, gnorm=0.249, loss_scale=4, train_wall=47, gb_free=14.3, wall=660]2023-08-23 05:10:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:10:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 038 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 37.06it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 47.41it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 45.59it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  85% 23/27 [00:00<00:00, 57.49it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:10:17 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.176 | nll_loss 7.25 | ppl 152.22 | wps 134664 | wpb 2258.6 | bsz 74.1 | num_updates 1211 | best_loss 8.176\n",
            "2023-08-23 05:10:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 1211 updates\n",
            "2023-08-23 05:10:17 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint38.pt\n",
            "2023-08-23 05:10:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint38.pt\n",
            "2023-08-23 05:10:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint38.pt (epoch 38 @ 1211 updates, score 8.176) (writing took 0.11940253700004178 seconds)\n",
            "2023-08-23 05:10:17 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n",
            "2023-08-23 05:10:17 | INFO | train | epoch 038 | loss 7.328 | nll_loss 6.358 | ppl 82.02 | wps 99084.8 | ups 1.88 | wpb 52622.6 | bsz 1776 | num_updates 1211 | lr 0.000908715 | gnorm 0.235 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 665\n",
            "2023-08-23 05:10:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:10:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 039:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:10:17 | INFO | fairseq.trainer | begin training epoch 39\n",
            "2023-08-23 05:10:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 039:  97% 31/32 [00:16<00:00,  2.00it/s]2023-08-23 05:10:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:10:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 039 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  11% 3/27 [00:00<00:01, 21.31it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  26% 7/27 [00:00<00:00, 28.08it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  41% 11/27 [00:00<00:00, 32.16it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 30.70it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  85% 23/27 [00:00<00:00, 45.43it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:10:34 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.138 | nll_loss 7.209 | ppl 147.99 | wps 99437.1 | wpb 2258.6 | bsz 74.1 | num_updates 1243 | best_loss 8.138\n",
            "2023-08-23 05:10:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 1243 updates\n",
            "2023-08-23 05:10:34 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint39.pt\n",
            "2023-08-23 05:10:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint39.pt\n",
            "2023-08-23 05:10:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint39.pt (epoch 39 @ 1243 updates, score 8.138) (writing took 0.17077291199984757 seconds)\n",
            "2023-08-23 05:10:34 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n",
            "2023-08-23 05:10:34 | INFO | train | epoch 039 | loss 7.298 | nll_loss 6.324 | ppl 80.12 | wps 97686.3 | ups 1.86 | wpb 52622.6 | bsz 1776 | num_updates 1243 | lr 0.000896942 | gnorm 0.235 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 682\n",
            "2023-08-23 05:10:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:10:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 040:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:10:34 | INFO | fairseq.trainer | begin training epoch 40\n",
            "2023-08-23 05:10:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 040:  97% 31/32 [00:16<00:00,  1.64it/s]2023-08-23 05:10:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:10:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 040 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:   7% 2/27 [00:00<00:01, 18.74it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 38.70it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  48% 13/27 [00:00<00:00, 43.35it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  70% 19/27 [00:00<00:00, 48.31it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:10:52 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.152 | nll_loss 7.217 | ppl 148.82 | wps 133059 | wpb 2258.6 | bsz 74.1 | num_updates 1275 | best_loss 8.138\n",
            "2023-08-23 05:10:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 1275 updates\n",
            "2023-08-23 05:10:52 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint40.pt\n",
            "2023-08-23 05:10:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint40.pt\n",
            "2023-08-23 05:10:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint40.pt (epoch 40 @ 1275 updates, score 8.152) (writing took 0.08997335600020051 seconds)\n",
            "2023-08-23 05:10:52 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n",
            "2023-08-23 05:10:52 | INFO | train | epoch 040 | loss 7.274 | nll_loss 6.296 | ppl 78.57 | wps 95326.6 | ups 1.81 | wpb 52622.6 | bsz 1776 | num_updates 1275 | lr 0.000885615 | gnorm 0.284 | loss_scale 4 | train_wall 15 | gb_free 14.3 | wall 700\n",
            "2023-08-23 05:10:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:10:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 041:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:10:52 | INFO | fairseq.trainer | begin training epoch 41\n",
            "2023-08-23 05:10:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 041:  97% 31/32 [00:17<00:00,  2.06it/s, loss=7.281, nll_loss=6.304, ppl=79.03, wps=95530.9, ups=1.82, wpb=52446.8, bsz=1774.1, num_updates=1300, lr=0.000877058, gnorm=0.257, loss_scale=4, train_wall=47, gb_free=14.3, wall=715]2023-08-23 05:11:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:11:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 041 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 37.90it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  41% 11/27 [00:00<00:00, 47.25it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 47.96it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  96% 26/27 [00:00<00:00, 64.79it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:11:10 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.151 | nll_loss 7.214 | ppl 148.5 | wps 138832 | wpb 2258.6 | bsz 74.1 | num_updates 1307 | best_loss 8.138\n",
            "2023-08-23 05:11:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 1307 updates\n",
            "2023-08-23 05:11:10 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint41.pt\n",
            "2023-08-23 05:11:10 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint41.pt\n",
            "2023-08-23 05:11:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint41.pt (epoch 41 @ 1307 updates, score 8.151) (writing took 0.08672561200000928 seconds)\n",
            "2023-08-23 05:11:10 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n",
            "2023-08-23 05:11:10 | INFO | train | epoch 041 | loss 7.246 | nll_loss 6.264 | ppl 76.85 | wps 92967.8 | ups 1.77 | wpb 52622.6 | bsz 1776 | num_updates 1307 | lr 0.000874706 | gnorm 0.25 | loss_scale 4 | train_wall 16 | gb_free 14.3 | wall 718\n",
            "2023-08-23 05:11:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:11:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 042:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:11:10 | INFO | fairseq.trainer | begin training epoch 42\n",
            "2023-08-23 05:11:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 042:  97% 31/32 [00:16<00:00,  2.20it/s]2023-08-23 05:11:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:11:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 042 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 26.33it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 42.06it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 43.91it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  81% 22/27 [00:00<00:00, 57.21it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:11:27 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.103 | nll_loss 7.167 | ppl 143.75 | wps 132195 | wpb 2258.6 | bsz 74.1 | num_updates 1339 | best_loss 8.103\n",
            "2023-08-23 05:11:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 1339 updates\n",
            "2023-08-23 05:11:27 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint42.pt\n",
            "2023-08-23 05:11:27 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint42.pt\n",
            "2023-08-23 05:11:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint42.pt (epoch 42 @ 1339 updates, score 8.103) (writing took 0.12667656999997234 seconds)\n",
            "2023-08-23 05:11:27 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n",
            "2023-08-23 05:11:27 | INFO | train | epoch 042 | loss 7.224 | nll_loss 6.237 | ppl 75.45 | wps 99973.3 | ups 1.9 | wpb 52622.6 | bsz 1776 | num_updates 1339 | lr 0.000864191 | gnorm 0.241 | loss_scale 4 | train_wall 14 | gb_free 14.3 | wall 735\n",
            "2023-08-23 05:11:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:11:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 043:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:11:27 | INFO | fairseq.trainer | begin training epoch 43\n",
            "2023-08-23 05:11:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 043:  97% 31/32 [00:15<00:00,  2.16it/s]2023-08-23 05:11:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:11:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 043 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 39.34it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 41.76it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 45.33it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  81% 22/27 [00:00<00:00, 58.01it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:11:43 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.102 | nll_loss 7.163 | ppl 143.33 | wps 138278 | wpb 2258.6 | bsz 74.1 | num_updates 1371 | best_loss 8.102\n",
            "2023-08-23 05:11:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 1371 updates\n",
            "2023-08-23 05:11:43 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint43.pt\n",
            "2023-08-23 05:11:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint43.pt\n",
            "2023-08-23 05:11:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint43.pt (epoch 43 @ 1371 updates, score 8.102) (writing took 0.12501374799990117 seconds)\n",
            "2023-08-23 05:11:43 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n",
            "2023-08-23 05:11:43 | INFO | train | epoch 043 | loss 7.2 | nll_loss 6.21 | ppl 74.03 | wps 100859 | ups 1.92 | wpb 52622.6 | bsz 1776 | num_updates 1371 | lr 0.000854046 | gnorm 0.249 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 752\n",
            "2023-08-23 05:11:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:11:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 044:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:11:43 | INFO | fairseq.trainer | begin training epoch 44\n",
            "2023-08-23 05:11:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 044:  97% 31/32 [00:15<00:00,  2.18it/s, loss=7.204, nll_loss=6.215, ppl=74.29, wps=101406, ups=1.92, wpb=52726.6, bsz=1773.2, num_updates=1400, lr=0.000845154, gnorm=0.257, loss_scale=8, train_wall=45, gb_free=14.3, wall=767]2023-08-23 05:11:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:11:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 044 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 25.03it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  26% 7/27 [00:00<00:00, 31.36it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  41% 11/27 [00:00<00:00, 31.84it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 31.53it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  78% 21/27 [00:00<00:00, 40.08it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:12:00 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.097 | nll_loss 7.154 | ppl 142.41 | wps 98527.5 | wpb 2258.6 | bsz 74.1 | num_updates 1403 | best_loss 8.097\n",
            "2023-08-23 05:12:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 1403 updates\n",
            "2023-08-23 05:12:00 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint44.pt\n",
            "2023-08-23 05:12:00 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint44.pt\n",
            "2023-08-23 05:12:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint44.pt (epoch 44 @ 1403 updates, score 8.097) (writing took 0.19048978799992256 seconds)\n",
            "2023-08-23 05:12:00 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n",
            "2023-08-23 05:12:00 | INFO | train | epoch 044 | loss 7.179 | nll_loss 6.186 | ppl 72.79 | wps 99946.7 | ups 1.9 | wpb 52622.6 | bsz 1776 | num_updates 1403 | lr 0.00084425 | gnorm 0.286 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 769\n",
            "2023-08-23 05:12:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:12:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 045:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:12:00 | INFO | fairseq.trainer | begin training epoch 45\n",
            "2023-08-23 05:12:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 045:  97% 31/32 [00:16<00:00,  1.69it/s]2023-08-23 05:12:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:12:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 045 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 28.53it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  26% 7/27 [00:00<00:00, 30.79it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  41% 11/27 [00:00<00:00, 31.05it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 29.54it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  81% 22/27 [00:00<00:00, 40.63it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:12:18 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.065 | nll_loss 7.117 | ppl 138.82 | wps 93691.8 | wpb 2258.6 | bsz 74.1 | num_updates 1435 | best_loss 8.065\n",
            "2023-08-23 05:12:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 1435 updates\n",
            "2023-08-23 05:12:18 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint45.pt\n",
            "2023-08-23 05:12:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint45.pt\n",
            "2023-08-23 05:12:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint45.pt (epoch 45 @ 1435 updates, score 8.065) (writing took 0.1672466180000356 seconds)\n",
            "2023-08-23 05:12:18 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n",
            "2023-08-23 05:12:18 | INFO | train | epoch 045 | loss 7.158 | nll_loss 6.162 | ppl 71.58 | wps 96762.9 | ups 1.84 | wpb 52622.6 | bsz 1776 | num_updates 1435 | lr 0.000834784 | gnorm 0.253 | loss_scale 8 | train_wall 15 | gb_free 14.3 | wall 786\n",
            "2023-08-23 05:12:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:12:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 046:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:12:18 | INFO | fairseq.trainer | begin training epoch 46\n",
            "2023-08-23 05:12:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 046:  97% 31/32 [00:15<00:00,  1.99it/s]2023-08-23 05:12:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:12:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 046 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 35.75it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 38.07it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  48% 13/27 [00:00<00:00, 41.27it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  81% 22/27 [00:00<00:00, 58.71it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:12:34 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.031 | nll_loss 7.082 | ppl 135.53 | wps 135757 | wpb 2258.6 | bsz 74.1 | num_updates 1467 | best_loss 8.031\n",
            "2023-08-23 05:12:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 1467 updates\n",
            "2023-08-23 05:12:34 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint46.pt\n",
            "2023-08-23 05:12:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint46.pt\n",
            "2023-08-23 05:12:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint46.pt (epoch 46 @ 1467 updates, score 8.031) (writing took 0.12723033699990083 seconds)\n",
            "2023-08-23 05:12:34 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n",
            "2023-08-23 05:12:34 | INFO | train | epoch 046 | loss 7.137 | nll_loss 6.137 | ppl 70.38 | wps 101194 | ups 1.92 | wpb 52622.6 | bsz 1776 | num_updates 1467 | lr 0.000825629 | gnorm 0.281 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 803\n",
            "2023-08-23 05:12:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:12:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 047:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:12:34 | INFO | fairseq.trainer | begin training epoch 47\n",
            "2023-08-23 05:12:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 047:  97% 31/32 [00:16<00:00,  2.15it/s]2023-08-23 05:12:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:12:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 047 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 39.19it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 38.76it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  48% 13/27 [00:00<00:00, 43.07it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  70% 19/27 [00:00<00:00, 49.11it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:12:51 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 8.019 | nll_loss 7.069 | ppl 134.25 | wps 132340 | wpb 2258.6 | bsz 74.1 | num_updates 1499 | best_loss 8.019\n",
            "2023-08-23 05:12:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 1499 updates\n",
            "2023-08-23 05:12:51 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint47.pt\n",
            "2023-08-23 05:12:51 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint47.pt\n",
            "2023-08-23 05:12:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint47.pt (epoch 47 @ 1499 updates, score 8.019) (writing took 0.1246958500000801 seconds)\n",
            "2023-08-23 05:12:51 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n",
            "2023-08-23 05:12:51 | INFO | train | epoch 047 | loss 7.116 | nll_loss 6.113 | ppl 69.19 | wps 99339.5 | ups 1.89 | wpb 52622.6 | bsz 1776 | num_updates 1499 | lr 0.000816769 | gnorm 0.225 | loss_scale 8 | train_wall 15 | gb_free 14.3 | wall 820\n",
            "2023-08-23 05:12:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:12:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 048:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:12:51 | INFO | fairseq.trainer | begin training epoch 48\n",
            "2023-08-23 05:12:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 048:  97% 31/32 [00:15<00:00,  2.28it/s, loss=7.137, nll_loss=6.137, ppl=70.37, wps=97744.3, ups=1.87, wpb=52405, bsz=1770.2, num_updates=1500, lr=0.000816497, gnorm=0.253, loss_scale=8, train_wall=45, gb_free=14.3, wall=820]2023-08-23 05:13:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:13:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 048 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 43.94it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 42.28it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 45.86it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:13:08 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.994 | nll_loss 7.042 | ppl 131.8 | wps 141750 | wpb 2258.6 | bsz 74.1 | num_updates 1531 | best_loss 7.994\n",
            "2023-08-23 05:13:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 1531 updates\n",
            "2023-08-23 05:13:08 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint48.pt\n",
            "2023-08-23 05:13:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint48.pt\n",
            "2023-08-23 05:13:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint48.pt (epoch 48 @ 1531 updates, score 7.994) (writing took 0.12745584599997528 seconds)\n",
            "2023-08-23 05:13:08 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n",
            "2023-08-23 05:13:08 | INFO | train | epoch 048 | loss 7.1 | nll_loss 6.093 | ppl 68.28 | wps 101498 | ups 1.93 | wpb 52622.6 | bsz 1776 | num_updates 1531 | lr 0.000808188 | gnorm 0.278 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 836\n",
            "2023-08-23 05:13:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:13:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 049:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:13:08 | INFO | fairseq.trainer | begin training epoch 49\n",
            "2023-08-23 05:13:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 049:  97% 31/32 [00:15<00:00,  2.19it/s]2023-08-23 05:13:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:13:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 049 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 38.11it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 44.04it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 42.89it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  85% 23/27 [00:00<00:00, 58.12it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:13:24 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.974 | nll_loss 7.018 | ppl 129.63 | wps 135457 | wpb 2258.6 | bsz 74.1 | num_updates 1563 | best_loss 7.974\n",
            "2023-08-23 05:13:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 1563 updates\n",
            "2023-08-23 05:13:24 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint49.pt\n",
            "2023-08-23 05:13:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint49.pt\n",
            "2023-08-23 05:13:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint49.pt (epoch 49 @ 1563 updates, score 7.974) (writing took 0.127121064999983 seconds)\n",
            "2023-08-23 05:13:24 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n",
            "2023-08-23 05:13:24 | INFO | train | epoch 049 | loss 7.077 | nll_loss 6.068 | ppl 67.07 | wps 101646 | ups 1.93 | wpb 52622.6 | bsz 1776 | num_updates 1563 | lr 0.000799872 | gnorm 0.268 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 853\n",
            "2023-08-23 05:13:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:13:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 050:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:13:24 | INFO | fairseq.trainer | begin training epoch 50\n",
            "2023-08-23 05:13:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 050:  97% 31/32 [00:15<00:00,  2.29it/s]2023-08-23 05:13:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:13:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 050 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 33.38it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 33.32it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  44% 12/27 [00:00<00:00, 33.37it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 33.77it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  81% 22/27 [00:00<00:00, 41.54it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:13:41 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.966 | nll_loss 7.007 | ppl 128.64 | wps 97892 | wpb 2258.6 | bsz 74.1 | num_updates 1595 | best_loss 7.966\n",
            "2023-08-23 05:13:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 1595 updates\n",
            "2023-08-23 05:13:41 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint50.pt\n",
            "2023-08-23 05:13:41 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint50.pt\n",
            "2023-08-23 05:13:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint50.pt (epoch 50 @ 1595 updates, score 7.966) (writing took 0.18651214900000923 seconds)\n",
            "2023-08-23 05:13:41 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n",
            "2023-08-23 05:13:41 | INFO | train | epoch 050 | loss 7.058 | nll_loss 6.045 | ppl 66.03 | wps 100511 | ups 1.91 | wpb 52622.6 | bsz 1776 | num_updates 1595 | lr 0.000791808 | gnorm 0.233 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 869\n",
            "2023-08-23 05:13:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:13:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 051:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:13:41 | INFO | fairseq.trainer | begin training epoch 51\n",
            "2023-08-23 05:13:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 051:  97% 31/32 [00:16<00:00,  1.68it/s, loss=7.077, nll_loss=6.067, ppl=67.04, wps=100074, ups=1.9, wpb=52723.5, bsz=1777.1, num_updates=1600, lr=0.000790569, gnorm=0.261, loss_scale=8, train_wall=45, gb_free=14.3, wall=873]2023-08-23 05:13:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:13:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 051 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:   7% 2/27 [00:00<00:01, 19.15it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  22% 6/27 [00:00<00:00, 31.09it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 31.91it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 34.25it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  67% 18/27 [00:00<00:00, 33.57it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  85% 23/27 [00:00<00:00, 38.55it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:13:59 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.947 | nll_loss 6.985 | ppl 126.72 | wps 90629 | wpb 2258.6 | bsz 74.1 | num_updates 1627 | best_loss 7.947\n",
            "2023-08-23 05:13:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 1627 updates\n",
            "2023-08-23 05:13:59 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint51.pt\n",
            "2023-08-23 05:13:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint51.pt\n",
            "2023-08-23 05:13:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint51.pt (epoch 51 @ 1627 updates, score 7.947) (writing took 0.17508686299993315 seconds)\n",
            "2023-08-23 05:13:59 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)\n",
            "2023-08-23 05:13:59 | INFO | train | epoch 051 | loss 7.04 | nll_loss 6.023 | ppl 65.04 | wps 94261.4 | ups 1.79 | wpb 52622.6 | bsz 1776 | num_updates 1627 | lr 0.000783982 | gnorm 0.242 | loss_scale 8 | train_wall 15 | gb_free 14.3 | wall 887\n",
            "2023-08-23 05:13:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:13:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 052:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:13:59 | INFO | fairseq.trainer | begin training epoch 52\n",
            "2023-08-23 05:13:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 052:  97% 31/32 [00:16<00:00,  1.86it/s]2023-08-23 05:14:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:14:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 052 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 30.99it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 42.83it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 45.33it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  85% 23/27 [00:00<00:00, 56.24it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:14:16 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 7.925 | nll_loss 6.96 | ppl 124.52 | wps 129526 | wpb 2258.6 | bsz 74.1 | num_updates 1659 | best_loss 7.925\n",
            "2023-08-23 05:14:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 1659 updates\n",
            "2023-08-23 05:14:16 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint52.pt\n",
            "2023-08-23 05:14:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint52.pt\n",
            "2023-08-23 05:14:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint52.pt (epoch 52 @ 1659 updates, score 7.925) (writing took 0.1286376770001425 seconds)\n",
            "2023-08-23 05:14:16 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)\n",
            "2023-08-23 05:14:16 | INFO | train | epoch 052 | loss 7.02 | nll_loss 6.001 | ppl 64.02 | wps 97143.7 | ups 1.85 | wpb 52622.6 | bsz 1776 | num_updates 1659 | lr 0.000776384 | gnorm 0.226 | loss_scale 8 | train_wall 15 | gb_free 14.3 | wall 905\n",
            "2023-08-23 05:14:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:14:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 053:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:14:16 | INFO | fairseq.trainer | begin training epoch 53\n",
            "2023-08-23 05:14:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 053:  97% 31/32 [00:17<00:00,  2.21it/s]2023-08-23 05:14:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:14:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 053 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 33.63it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 43.70it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 46.55it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset: 100% 27/27 [00:00<00:00, 67.71it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:14:35 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 7.926 | nll_loss 6.958 | ppl 124.35 | wps 138069 | wpb 2258.6 | bsz 74.1 | num_updates 1691 | best_loss 7.925\n",
            "2023-08-23 05:14:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 1691 updates\n",
            "2023-08-23 05:14:35 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint53.pt\n",
            "2023-08-23 05:14:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint53.pt\n",
            "2023-08-23 05:14:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint53.pt (epoch 53 @ 1691 updates, score 7.926) (writing took 0.0819433100000424 seconds)\n",
            "2023-08-23 05:14:35 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)\n",
            "2023-08-23 05:14:35 | INFO | train | epoch 053 | loss 7.001 | nll_loss 5.977 | ppl 63.01 | wps 91272.6 | ups 1.73 | wpb 52622.6 | bsz 1776 | num_updates 1691 | lr 0.000769003 | gnorm 0.206 | loss_scale 8 | train_wall 16 | gb_free 14.3 | wall 923\n",
            "2023-08-23 05:14:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:14:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 054:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:14:35 | INFO | fairseq.trainer | begin training epoch 54\n",
            "2023-08-23 05:14:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 054:  97% 31/32 [00:16<00:00,  2.20it/s, loss=7.012, nll_loss=5.991, ppl=63.62, wps=95893.7, ups=1.82, wpb=52715.3, bsz=1787.9, num_updates=1700, lr=0.000766965, gnorm=0.223, loss_scale=8, train_wall=47, gb_free=14.3, wall=928]2023-08-23 05:14:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:14:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 054 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 054 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 36.98it/s]\u001b[A\n",
            "epoch 054 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 40.57it/s]\u001b[A\n",
            "epoch 054 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 47.42it/s]\u001b[A\n",
            "epoch 054 | valid on 'valid' subset:  96% 26/27 [00:00<00:00, 68.47it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:14:52 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 7.895 | nll_loss 6.921 | ppl 121.17 | wps 146283 | wpb 2258.6 | bsz 74.1 | num_updates 1723 | best_loss 7.895\n",
            "2023-08-23 05:14:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 1723 updates\n",
            "2023-08-23 05:14:52 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint54.pt\n",
            "2023-08-23 05:14:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint54.pt\n",
            "2023-08-23 05:14:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint54.pt (epoch 54 @ 1723 updates, score 7.895) (writing took 0.12521569400018961 seconds)\n",
            "2023-08-23 05:14:52 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)\n",
            "2023-08-23 05:14:52 | INFO | train | epoch 054 | loss 6.984 | nll_loss 5.958 | ppl 62.16 | wps 100300 | ups 1.91 | wpb 52622.6 | bsz 1776 | num_updates 1723 | lr 0.000761829 | gnorm 0.237 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 940\n",
            "2023-08-23 05:14:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:14:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 055:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:14:52 | INFO | fairseq.trainer | begin training epoch 55\n",
            "2023-08-23 05:14:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 055:  97% 31/32 [00:16<00:00,  2.25it/s]2023-08-23 05:15:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:15:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 055 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 055 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 39.29it/s]\u001b[A\n",
            "epoch 055 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 34.77it/s]\u001b[A\n",
            "epoch 055 | valid on 'valid' subset:  48% 13/27 [00:00<00:00, 38.55it/s]\u001b[A\n",
            "epoch 055 | valid on 'valid' subset:  63% 17/27 [00:00<00:00, 36.60it/s]\u001b[A\n",
            "epoch 055 | valid on 'valid' subset:  89% 24/27 [00:00<00:00, 46.49it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:15:08 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 7.881 | nll_loss 6.907 | ppl 120.02 | wps 104428 | wpb 2258.6 | bsz 74.1 | num_updates 1755 | best_loss 7.881\n",
            "2023-08-23 05:15:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 1755 updates\n",
            "2023-08-23 05:15:08 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint55.pt\n",
            "2023-08-23 05:15:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint55.pt\n",
            "2023-08-23 05:15:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint55.pt (epoch 55 @ 1755 updates, score 7.881) (writing took 0.1715474699999504 seconds)\n",
            "2023-08-23 05:15:09 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)\n",
            "2023-08-23 05:15:09 | INFO | train | epoch 055 | loss 6.964 | nll_loss 5.935 | ppl 61.17 | wps 99014.1 | ups 1.88 | wpb 52622.6 | bsz 1776 | num_updates 1755 | lr 0.000754851 | gnorm 0.211 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 957\n",
            "2023-08-23 05:15:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:15:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 056:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:15:09 | INFO | fairseq.trainer | begin training epoch 56\n",
            "2023-08-23 05:15:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 056:  97% 31/32 [00:16<00:00,  1.74it/s]2023-08-23 05:15:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:15:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 056 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 056 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 29.46it/s]\u001b[A\n",
            "epoch 056 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 35.26it/s]\u001b[A\n",
            "epoch 056 | valid on 'valid' subset:  44% 12/27 [00:00<00:00, 33.53it/s]\u001b[A\n",
            "epoch 056 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 34.49it/s]\u001b[A\n",
            "epoch 056 | valid on 'valid' subset:  85% 23/27 [00:00<00:00, 44.34it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:15:26 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 7.88 | nll_loss 6.904 | ppl 119.78 | wps 99134.6 | wpb 2258.6 | bsz 74.1 | num_updates 1787 | best_loss 7.88\n",
            "2023-08-23 05:15:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 1787 updates\n",
            "2023-08-23 05:15:26 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint56.pt\n",
            "2023-08-23 05:15:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint56.pt\n",
            "2023-08-23 05:15:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint56.pt (epoch 56 @ 1787 updates, score 7.88) (writing took 0.17120141100008368 seconds)\n",
            "2023-08-23 05:15:26 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)\n",
            "2023-08-23 05:15:26 | INFO | train | epoch 056 | loss 6.946 | nll_loss 5.914 | ppl 60.29 | wps 97392.3 | ups 1.85 | wpb 52622.6 | bsz 1776 | num_updates 1787 | lr 0.000748062 | gnorm 0.247 | loss_scale 8 | train_wall 15 | gb_free 14.3 | wall 974\n",
            "2023-08-23 05:15:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:15:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 057:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:15:26 | INFO | fairseq.trainer | begin training epoch 57\n",
            "2023-08-23 05:15:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 057:  97% 31/32 [00:16<00:00,  1.82it/s, loss=6.966, nll_loss=5.937, ppl=61.25, wps=99292.6, ups=1.89, wpb=52450.6, bsz=1752.8, num_updates=1800, lr=0.000745356, gnorm=0.232, loss_scale=8, train_wall=45, gb_free=14.3, wall=981]2023-08-23 05:15:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:15:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 057 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 057 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 39.35it/s]\u001b[A\n",
            "epoch 057 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 36.88it/s]\u001b[A\n",
            "epoch 057 | valid on 'valid' subset:  44% 12/27 [00:00<00:00, 38.04it/s]\u001b[A\n",
            "epoch 057 | valid on 'valid' subset:  70% 19/27 [00:00<00:00, 48.36it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:15:43 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 7.87 | nll_loss 6.89 | ppl 118.62 | wps 126460 | wpb 2258.6 | bsz 74.1 | num_updates 1819 | best_loss 7.87\n",
            "2023-08-23 05:15:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 1819 updates\n",
            "2023-08-23 05:15:43 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint57.pt\n",
            "2023-08-23 05:15:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint57.pt\n",
            "2023-08-23 05:15:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint57.pt (epoch 57 @ 1819 updates, score 7.87) (writing took 0.11620402100015781 seconds)\n",
            "2023-08-23 05:15:43 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)\n",
            "2023-08-23 05:15:43 | INFO | train | epoch 057 | loss 6.929 | nll_loss 5.894 | ppl 59.47 | wps 98643.4 | ups 1.87 | wpb 52622.6 | bsz 1776 | num_updates 1819 | lr 0.000741453 | gnorm 0.228 | loss_scale 8 | train_wall 15 | gb_free 14.3 | wall 991\n",
            "2023-08-23 05:15:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:15:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 058:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:15:43 | INFO | fairseq.trainer | begin training epoch 58\n",
            "2023-08-23 05:15:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 058:  97% 31/32 [00:15<00:00,  2.08it/s]2023-08-23 05:15:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:15:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 058 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 058 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 32.78it/s]\u001b[A\n",
            "epoch 058 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 36.25it/s]\u001b[A\n",
            "epoch 058 | valid on 'valid' subset:  44% 12/27 [00:00<00:00, 35.31it/s]\u001b[A\n",
            "epoch 058 | valid on 'valid' subset:  81% 22/27 [00:00<00:00, 57.38it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:15:59 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 7.863 | nll_loss 6.881 | ppl 117.87 | wps 129813 | wpb 2258.6 | bsz 74.1 | num_updates 1851 | best_loss 7.863\n",
            "2023-08-23 05:15:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 1851 updates\n",
            "2023-08-23 05:15:59 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint58.pt\n",
            "2023-08-23 05:16:00 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint58.pt\n",
            "2023-08-23 05:16:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint58.pt (epoch 58 @ 1851 updates, score 7.863) (writing took 0.12940321199994287 seconds)\n",
            "2023-08-23 05:16:00 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)\n",
            "2023-08-23 05:16:00 | INFO | train | epoch 058 | loss 6.912 | nll_loss 5.873 | ppl 58.62 | wps 101577 | ups 1.93 | wpb 52622.6 | bsz 1776 | num_updates 1851 | lr 0.000735016 | gnorm 0.271 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 1008\n",
            "2023-08-23 05:16:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:16:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 059:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:16:00 | INFO | fairseq.trainer | begin training epoch 59\n",
            "2023-08-23 05:16:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 059:  97% 31/32 [00:15<00:00,  2.17it/s]2023-08-23 05:16:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:16:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 059 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 059 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 32.40it/s]\u001b[A\n",
            "epoch 059 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 42.84it/s]\u001b[A\n",
            "epoch 059 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 41.38it/s]\u001b[A\n",
            "epoch 059 | valid on 'valid' subset:  96% 26/27 [00:00<00:00, 64.10it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:16:16 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 7.811 | nll_loss 6.827 | ppl 113.56 | wps 132479 | wpb 2258.6 | bsz 74.1 | num_updates 1883 | best_loss 7.811\n",
            "2023-08-23 05:16:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 1883 updates\n",
            "2023-08-23 05:16:16 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint59.pt\n",
            "2023-08-23 05:16:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint59.pt\n",
            "2023-08-23 05:16:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint59.pt (epoch 59 @ 1883 updates, score 7.811) (writing took 0.13164940499996192 seconds)\n",
            "2023-08-23 05:16:16 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)\n",
            "2023-08-23 05:16:16 | INFO | train | epoch 059 | loss 6.893 | nll_loss 5.852 | ppl 57.76 | wps 100475 | ups 1.91 | wpb 52622.6 | bsz 1776 | num_updates 1883 | lr 0.000728744 | gnorm 0.239 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 1025\n",
            "2023-08-23 05:16:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:16:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 060:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:16:16 | INFO | fairseq.trainer | begin training epoch 60\n",
            "2023-08-23 05:16:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 060:  97% 31/32 [00:15<00:00,  2.21it/s, loss=6.901, nll_loss=5.861, ppl=58.13, wps=98699.2, ups=1.88, wpb=52550.4, bsz=1783, num_updates=1900, lr=0.000725476, gnorm=0.247, loss_scale=8, train_wall=46, gb_free=14.3, wall=1034]2023-08-23 05:16:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:16:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 060 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 060 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 44.97it/s]\u001b[A\n",
            "epoch 060 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 45.20it/s]\u001b[A\n",
            "epoch 060 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 47.84it/s]\u001b[A\n",
            "epoch 060 | valid on 'valid' subset: 100% 27/27 [00:00<00:00, 69.94it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:16:33 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 7.818 | nll_loss 6.827 | ppl 113.57 | wps 145526 | wpb 2258.6 | bsz 74.1 | num_updates 1915 | best_loss 7.811\n",
            "2023-08-23 05:16:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 1915 updates\n",
            "2023-08-23 05:16:33 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint60.pt\n",
            "2023-08-23 05:16:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint60.pt\n",
            "2023-08-23 05:16:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint60.pt (epoch 60 @ 1915 updates, score 7.818) (writing took 0.0786834909999925 seconds)\n",
            "2023-08-23 05:16:33 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)\n",
            "2023-08-23 05:16:33 | INFO | train | epoch 060 | loss 6.877 | nll_loss 5.833 | ppl 57 | wps 101688 | ups 1.93 | wpb 52622.6 | bsz 1776 | num_updates 1915 | lr 0.000722629 | gnorm 0.233 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 1041\n",
            "2023-08-23 05:16:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:16:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 061:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:16:33 | INFO | fairseq.trainer | begin training epoch 61\n",
            "2023-08-23 05:16:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 061:  97% 31/32 [00:15<00:00,  2.18it/s]2023-08-23 05:16:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:16:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 061 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 061 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 45.92it/s]\u001b[A\n",
            "epoch 061 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 43.99it/s]\u001b[A\n",
            "epoch 061 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 45.27it/s]\u001b[A\n",
            "epoch 061 | valid on 'valid' subset: 100% 27/27 [00:00<00:00, 71.74it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:16:49 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 7.812 | nll_loss 6.818 | ppl 112.79 | wps 146939 | wpb 2258.6 | bsz 74.1 | num_updates 1947 | best_loss 7.811\n",
            "2023-08-23 05:16:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 1947 updates\n",
            "2023-08-23 05:16:49 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint61.pt\n",
            "2023-08-23 05:16:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint61.pt\n",
            "2023-08-23 05:16:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint61.pt (epoch 61 @ 1947 updates, score 7.812) (writing took 0.09044493800001874 seconds)\n",
            "2023-08-23 05:16:49 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)\n",
            "2023-08-23 05:16:49 | INFO | train | epoch 061 | loss 6.861 | nll_loss 5.814 | ppl 56.27 | wps 101879 | ups 1.94 | wpb 52622.6 | bsz 1776 | num_updates 1947 | lr 0.000716666 | gnorm 0.305 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 1058\n",
            "2023-08-23 05:16:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:16:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 062:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:16:50 | INFO | fairseq.trainer | begin training epoch 62\n",
            "2023-08-23 05:16:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 062:  97% 31/32 [00:16<00:00,  1.86it/s]2023-08-23 05:17:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:17:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 062 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 062 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 24.07it/s]\u001b[A\n",
            "epoch 062 | valid on 'valid' subset:  26% 7/27 [00:00<00:00, 31.82it/s]\u001b[A\n",
            "epoch 062 | valid on 'valid' subset:  41% 11/27 [00:00<00:00, 34.62it/s]\u001b[A\n",
            "epoch 062 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 35.67it/s]\u001b[A\n",
            "epoch 062 | valid on 'valid' subset:  74% 20/27 [00:00<00:00, 39.58it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:17:06 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 7.787 | nll_loss 6.795 | ppl 111.04 | wps 101223 | wpb 2258.6 | bsz 74.1 | num_updates 1979 | best_loss 7.787\n",
            "2023-08-23 05:17:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 1979 updates\n",
            "2023-08-23 05:17:06 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint62.pt\n",
            "2023-08-23 05:17:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint62.pt\n",
            "2023-08-23 05:17:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint62.pt (epoch 62 @ 1979 updates, score 7.787) (writing took 0.17558784699986063 seconds)\n",
            "2023-08-23 05:17:07 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)\n",
            "2023-08-23 05:17:07 | INFO | train | epoch 062 | loss 6.843 | nll_loss 5.793 | ppl 55.46 | wps 98110.1 | ups 1.86 | wpb 52622.6 | bsz 1776 | num_updates 1979 | lr 0.000710849 | gnorm 0.257 | loss_scale 8 | train_wall 15 | gb_free 14.3 | wall 1075\n",
            "2023-08-23 05:17:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:17:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 063:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:17:07 | INFO | fairseq.trainer | begin training epoch 63\n",
            "2023-08-23 05:17:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 063:  97% 31/32 [00:16<00:00,  1.66it/s, loss=6.851, nll_loss=5.802, ppl=55.78, wps=102919, ups=1.94, wpb=52921.3, bsz=1780.4, num_updates=2000, lr=0.000707107, gnorm=0.268, loss_scale=8, train_wall=44, gb_free=14.3, wall=1085]2023-08-23 05:17:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:17:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 063 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 063 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 25.57it/s]\u001b[A\n",
            "epoch 063 | valid on 'valid' subset:  26% 7/27 [00:00<00:00, 32.62it/s]\u001b[A\n",
            "epoch 063 | valid on 'valid' subset:  48% 13/27 [00:00<00:00, 42.52it/s]\u001b[A\n",
            "epoch 063 | valid on 'valid' subset:  70% 19/27 [00:00<00:00, 48.15it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:17:24 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 7.772 | nll_loss 6.772 | ppl 109.28 | wps 126392 | wpb 2258.6 | bsz 74.1 | num_updates 2011 | best_loss 7.772\n",
            "2023-08-23 05:17:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 2011 updates\n",
            "2023-08-23 05:17:24 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint63.pt\n",
            "2023-08-23 05:17:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint63.pt\n",
            "2023-08-23 05:17:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint63.pt (epoch 63 @ 2011 updates, score 7.772) (writing took 0.12201472900005683 seconds)\n",
            "2023-08-23 05:17:24 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)\n",
            "2023-08-23 05:17:24 | INFO | train | epoch 063 | loss 6.828 | nll_loss 5.775 | ppl 54.77 | wps 98809.3 | ups 1.88 | wpb 52622.6 | bsz 1776 | num_updates 2011 | lr 0.00070517 | gnorm 0.247 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 1092\n",
            "2023-08-23 05:17:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:17:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 064:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:17:24 | INFO | fairseq.trainer | begin training epoch 64\n",
            "2023-08-23 05:17:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 064:  97% 31/32 [00:15<00:00,  2.06it/s]2023-08-23 05:17:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:17:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 064 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 064 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 37.06it/s]\u001b[A\n",
            "epoch 064 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 44.23it/s]\u001b[A\n",
            "epoch 064 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 42.18it/s]\u001b[A\n",
            "epoch 064 | valid on 'valid' subset:  78% 21/27 [00:00<00:00, 51.83it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:17:40 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 7.756 | nll_loss 6.755 | ppl 108.02 | wps 129245 | wpb 2258.6 | bsz 74.1 | num_updates 2043 | best_loss 7.756\n",
            "2023-08-23 05:17:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 2043 updates\n",
            "2023-08-23 05:17:40 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint64.pt\n",
            "2023-08-23 05:17:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint64.pt\n",
            "2023-08-23 05:17:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint64.pt (epoch 64 @ 2043 updates, score 7.756) (writing took 0.12787122899999304 seconds)\n",
            "2023-08-23 05:17:40 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)\n",
            "2023-08-23 05:17:40 | INFO | train | epoch 064 | loss 6.809 | nll_loss 5.753 | ppl 53.92 | wps 101302 | ups 1.93 | wpb 52622.6 | bsz 1776 | num_updates 2043 | lr 0.000699626 | gnorm 0.23 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 1108\n",
            "2023-08-23 05:17:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:17:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 065:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:17:40 | INFO | fairseq.trainer | begin training epoch 65\n",
            "2023-08-23 05:17:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 065:  97% 31/32 [00:17<00:00,  2.21it/s]2023-08-23 05:17:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:17:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 065 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 065 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 44.13it/s]\u001b[A\n",
            "epoch 065 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 43.96it/s]\u001b[A\n",
            "epoch 065 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 43.32it/s]\u001b[A\n",
            "epoch 065 | valid on 'valid' subset:  93% 25/27 [00:00<00:00, 62.91it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:17:58 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 7.76 | nll_loss 6.757 | ppl 108.17 | wps 139289 | wpb 2258.6 | bsz 74.1 | num_updates 2075 | best_loss 7.756\n",
            "2023-08-23 05:17:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 2075 updates\n",
            "2023-08-23 05:17:58 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint65.pt\n",
            "2023-08-23 05:17:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint65.pt\n",
            "2023-08-23 05:17:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint65.pt (epoch 65 @ 2075 updates, score 7.76) (writing took 0.08187178499997572 seconds)\n",
            "2023-08-23 05:17:58 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)\n",
            "2023-08-23 05:17:58 | INFO | train | epoch 065 | loss 6.794 | nll_loss 5.735 | ppl 53.25 | wps 93834.6 | ups 1.78 | wpb 52622.6 | bsz 1776 | num_updates 2075 | lr 0.00069421 | gnorm 0.219 | loss_scale 8 | train_wall 16 | gb_free 14.3 | wall 1126\n",
            "2023-08-23 05:17:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:17:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 066:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:17:58 | INFO | fairseq.trainer | begin training epoch 66\n",
            "2023-08-23 05:17:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 066:  97% 31/32 [00:15<00:00,  2.23it/s, loss=6.798, nll_loss=5.74, ppl=53.45, wps=96894, ups=1.84, wpb=52573.6, bsz=1779, num_updates=2100, lr=0.000690066, gnorm=0.228, loss_scale=8, train_wall=47, gb_free=14.3, wall=1140]2023-08-23 05:18:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:18:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 066 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 066 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 39.42it/s]\u001b[A\n",
            "epoch 066 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 38.47it/s]\u001b[A\n",
            "epoch 066 | valid on 'valid' subset:  48% 13/27 [00:00<00:00, 41.46it/s]\u001b[A\n",
            "epoch 066 | valid on 'valid' subset:  74% 20/27 [00:00<00:00, 52.21it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:18:15 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 7.744 | nll_loss 6.736 | ppl 106.61 | wps 133793 | wpb 2258.6 | bsz 74.1 | num_updates 2107 | best_loss 7.744\n",
            "2023-08-23 05:18:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 2107 updates\n",
            "2023-08-23 05:18:15 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint66.pt\n",
            "2023-08-23 05:18:15 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint66.pt\n",
            "2023-08-23 05:18:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint66.pt (epoch 66 @ 2107 updates, score 7.744) (writing took 0.1219213919998765 seconds)\n",
            "2023-08-23 05:18:15 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)\n",
            "2023-08-23 05:18:15 | INFO | train | epoch 066 | loss 6.778 | nll_loss 5.716 | ppl 52.58 | wps 101089 | ups 1.92 | wpb 52622.6 | bsz 1776 | num_updates 2107 | lr 0.000688918 | gnorm 0.238 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 1143\n",
            "2023-08-23 05:18:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:18:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 067:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:18:15 | INFO | fairseq.trainer | begin training epoch 67\n",
            "2023-08-23 05:18:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 067:  97% 31/32 [00:15<00:00,  2.18it/s]2023-08-23 05:18:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:18:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 067 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 067 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 35.14it/s]\u001b[A\n",
            "epoch 067 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 43.15it/s]\u001b[A\n",
            "epoch 067 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 46.02it/s]\u001b[A\n",
            "epoch 067 | valid on 'valid' subset:  70% 19/27 [00:00<00:00, 45.28it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:18:31 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 7.721 | nll_loss 6.714 | ppl 104.97 | wps 126203 | wpb 2258.6 | bsz 74.1 | num_updates 2139 | best_loss 7.721\n",
            "2023-08-23 05:18:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 2139 updates\n",
            "2023-08-23 05:18:31 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint67.pt\n",
            "2023-08-23 05:18:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint67.pt\n",
            "2023-08-23 05:18:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint67.pt (epoch 67 @ 2139 updates, score 7.721) (writing took 0.139300650999985 seconds)\n",
            "2023-08-23 05:18:31 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)\n",
            "2023-08-23 05:18:31 | INFO | train | epoch 067 | loss 6.76 | nll_loss 5.696 | ppl 51.84 | wps 101627 | ups 1.93 | wpb 52622.6 | bsz 1776 | num_updates 2139 | lr 0.000683746 | gnorm 0.233 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 1160\n",
            "2023-08-23 05:18:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:18:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 068:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:18:31 | INFO | fairseq.trainer | begin training epoch 68\n",
            "2023-08-23 05:18:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 068:  97% 31/32 [00:16<00:00,  1.95it/s]2023-08-23 05:18:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:18:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 068 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 068 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 26.84it/s]\u001b[A\n",
            "epoch 068 | valid on 'valid' subset:  26% 7/27 [00:00<00:00, 32.27it/s]\u001b[A\n",
            "epoch 068 | valid on 'valid' subset:  41% 11/27 [00:00<00:00, 33.23it/s]\u001b[A\n",
            "epoch 068 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 32.49it/s]\u001b[A\n",
            "epoch 068 | valid on 'valid' subset:  74% 20/27 [00:00<00:00, 37.00it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:18:49 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 7.7 | nll_loss 6.69 | ppl 103.27 | wps 96216.6 | wpb 2258.6 | bsz 74.1 | num_updates 2171 | best_loss 7.7\n",
            "2023-08-23 05:18:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 2171 updates\n",
            "2023-08-23 05:18:49 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint68.pt\n",
            "2023-08-23 05:18:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint68.pt\n",
            "2023-08-23 05:18:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint68.pt (epoch 68 @ 2171 updates, score 7.7) (writing took 0.17744424199986497 seconds)\n",
            "2023-08-23 05:18:49 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)\n",
            "2023-08-23 05:18:49 | INFO | train | epoch 068 | loss 6.744 | nll_loss 5.677 | ppl 51.15 | wps 97331.3 | ups 1.85 | wpb 52622.6 | bsz 1776 | num_updates 2171 | lr 0.000678688 | gnorm 0.239 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 1177\n",
            "2023-08-23 05:18:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:18:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 069:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:18:49 | INFO | fairseq.trainer | begin training epoch 69\n",
            "2023-08-23 05:18:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 069:  97% 31/32 [00:16<00:00,  1.68it/s, loss=6.744, nll_loss=5.676, ppl=51.14, wps=100357, ups=1.91, wpb=52677.1, bsz=1779, num_updates=2200, lr=0.0006742, gnorm=0.236, loss_scale=8, train_wall=45, gb_free=14.3, wall=1192]2023-08-23 05:19:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:19:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 069 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 069 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 24.70it/s]\u001b[A\n",
            "epoch 069 | valid on 'valid' subset:  26% 7/27 [00:00<00:00, 28.24it/s]\u001b[A\n",
            "epoch 069 | valid on 'valid' subset:  41% 11/27 [00:00<00:00, 31.83it/s]\u001b[A\n",
            "epoch 069 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 37.97it/s]\u001b[A\n",
            "epoch 069 | valid on 'valid' subset: 100% 27/27 [00:00<00:00, 61.97it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:19:06 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 7.692 | nll_loss 6.675 | ppl 102.16 | wps 118245 | wpb 2258.6 | bsz 74.1 | num_updates 2203 | best_loss 7.692\n",
            "2023-08-23 05:19:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 2203 updates\n",
            "2023-08-23 05:19:06 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint69.pt\n",
            "2023-08-23 05:19:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint69.pt\n",
            "2023-08-23 05:19:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint69.pt (epoch 69 @ 2203 updates, score 7.692) (writing took 0.12426946799996585 seconds)\n",
            "2023-08-23 05:19:06 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)\n",
            "2023-08-23 05:19:06 | INFO | train | epoch 069 | loss 6.726 | nll_loss 5.656 | ppl 50.41 | wps 97890.5 | ups 1.86 | wpb 52622.6 | bsz 1776 | num_updates 2203 | lr 0.000673741 | gnorm 0.239 | loss_scale 8 | train_wall 15 | gb_free 14.3 | wall 1194\n",
            "2023-08-23 05:19:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:19:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 070:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:19:06 | INFO | fairseq.trainer | begin training epoch 70\n",
            "2023-08-23 05:19:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 070:  97% 31/32 [00:15<00:00,  2.07it/s]2023-08-23 05:19:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:19:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 070 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 070 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 44.45it/s]\u001b[A\n",
            "epoch 070 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 43.70it/s]\u001b[A\n",
            "epoch 070 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 46.39it/s]\u001b[A\n",
            "epoch 070 | valid on 'valid' subset:  96% 26/27 [00:00<00:00, 64.63it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:19:22 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 7.688 | nll_loss 6.669 | ppl 101.75 | wps 138230 | wpb 2258.6 | bsz 74.1 | num_updates 2235 | best_loss 7.688\n",
            "2023-08-23 05:19:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 2235 updates\n",
            "2023-08-23 05:19:22 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint70.pt\n",
            "2023-08-23 05:19:22 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint70.pt\n",
            "2023-08-23 05:19:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint70.pt (epoch 70 @ 2235 updates, score 7.688) (writing took 0.11476517799997055 seconds)\n",
            "2023-08-23 05:19:23 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)\n",
            "2023-08-23 05:19:23 | INFO | train | epoch 070 | loss 6.709 | nll_loss 5.636 | ppl 49.73 | wps 101827 | ups 1.94 | wpb 52622.6 | bsz 1776 | num_updates 2235 | lr 0.0006689 | gnorm 0.253 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 1211\n",
            "2023-08-23 05:19:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:19:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 071:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:19:23 | INFO | fairseq.trainer | begin training epoch 71\n",
            "2023-08-23 05:19:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 071:  97% 31/32 [00:15<00:00,  2.22it/s]2023-08-23 05:19:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:19:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 071 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 071 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 36.35it/s]\u001b[A\n",
            "epoch 071 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 41.77it/s]\u001b[A\n",
            "epoch 071 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 47.79it/s]\u001b[A\n",
            "epoch 071 | valid on 'valid' subset:  85% 23/27 [00:00<00:00, 58.00it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:19:39 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 7.673 | nll_loss 6.652 | ppl 100.57 | wps 142604 | wpb 2258.6 | bsz 74.1 | num_updates 2267 | best_loss 7.673\n",
            "2023-08-23 05:19:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 2267 updates\n",
            "2023-08-23 05:19:39 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint71.pt\n",
            "2023-08-23 05:19:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint71.pt\n",
            "2023-08-23 05:19:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint71.pt (epoch 71 @ 2267 updates, score 7.673) (writing took 0.11046110400002362 seconds)\n",
            "2023-08-23 05:19:39 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)\n",
            "2023-08-23 05:19:39 | INFO | train | epoch 071 | loss 6.693 | nll_loss 5.616 | ppl 49.05 | wps 101425 | ups 1.93 | wpb 52622.6 | bsz 1776 | num_updates 2267 | lr 0.000664162 | gnorm 0.238 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 1227\n",
            "2023-08-23 05:19:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:19:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 072:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:19:39 | INFO | fairseq.trainer | begin training epoch 72\n",
            "2023-08-23 05:19:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 072:  97% 31/32 [00:15<00:00,  2.24it/s]2023-08-23 05:19:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:19:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 072 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 072 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 37.73it/s]\u001b[A\n",
            "epoch 072 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 43.02it/s]\u001b[A\n",
            "epoch 072 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 43.79it/s]\u001b[A\n",
            "epoch 072 | valid on 'valid' subset:  78% 21/27 [00:00<00:00, 52.62it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:19:56 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 7.644 | nll_loss 6.621 | ppl 98.41 | wps 132810 | wpb 2258.6 | bsz 74.1 | num_updates 2299 | best_loss 7.644\n",
            "2023-08-23 05:19:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 2299 updates\n",
            "2023-08-23 05:19:56 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint72.pt\n",
            "2023-08-23 05:19:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint72.pt\n",
            "2023-08-23 05:19:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint72.pt (epoch 72 @ 2299 updates, score 7.644) (writing took 0.11301295299995218 seconds)\n",
            "2023-08-23 05:19:56 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)\n",
            "2023-08-23 05:19:56 | INFO | train | epoch 072 | loss 6.676 | nll_loss 5.597 | ppl 48.4 | wps 101267 | ups 1.92 | wpb 52622.6 | bsz 1776 | num_updates 2299 | lr 0.000659524 | gnorm 0.247 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 1244\n",
            "2023-08-23 05:19:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:19:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 073:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:19:56 | INFO | fairseq.trainer | begin training epoch 73\n",
            "2023-08-23 05:19:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 073:  97% 31/32 [00:15<00:00,  2.17it/s, loss=6.692, nll_loss=5.616, ppl=49.04, wps=99721.7, ups=1.9, wpb=52347.7, bsz=1769.1, num_updates=2300, lr=0.00065938, gnorm=0.247, loss_scale=8, train_wall=44, gb_free=14.3, wall=1245]2023-08-23 05:20:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:20:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 073 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 073 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 38.87it/s]\u001b[A\n",
            "epoch 073 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 36.90it/s]\u001b[A\n",
            "epoch 073 | valid on 'valid' subset:  48% 13/27 [00:00<00:00, 41.50it/s]\u001b[A\n",
            "epoch 073 | valid on 'valid' subset:  85% 23/27 [00:00<00:00, 61.63it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:20:12 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 7.651 | nll_loss 6.626 | ppl 98.79 | wps 136154 | wpb 2258.6 | bsz 74.1 | num_updates 2331 | best_loss 7.644\n",
            "2023-08-23 05:20:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 2331 updates\n",
            "2023-08-23 05:20:12 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint73.pt\n",
            "2023-08-23 05:20:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint73.pt\n",
            "2023-08-23 05:20:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint73.pt (epoch 73 @ 2331 updates, score 7.651) (writing took 0.07525792399997044 seconds)\n",
            "2023-08-23 05:20:12 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)\n",
            "2023-08-23 05:20:12 | INFO | train | epoch 073 | loss 6.661 | nll_loss 5.579 | ppl 47.82 | wps 101846 | ups 1.94 | wpb 52622.6 | bsz 1776 | num_updates 2331 | lr 0.000654981 | gnorm 0.253 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 1260\n",
            "2023-08-23 05:20:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:20:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 074:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:20:12 | INFO | fairseq.trainer | begin training epoch 74\n",
            "2023-08-23 05:20:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 074:  97% 31/32 [00:15<00:00,  2.18it/s]2023-08-23 05:20:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:20:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 074 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 074 | valid on 'valid' subset:   7% 2/27 [00:00<00:01, 19.88it/s]\u001b[A\n",
            "epoch 074 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 23.91it/s]\u001b[A\n",
            "epoch 074 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 25.04it/s]\u001b[A\n",
            "epoch 074 | valid on 'valid' subset:  44% 12/27 [00:00<00:00, 29.35it/s]\u001b[A\n",
            "epoch 074 | valid on 'valid' subset:  70% 19/27 [00:00<00:00, 41.10it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:20:29 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 7.632 | nll_loss 6.602 | ppl 97.12 | wps 101676 | wpb 2258.6 | bsz 74.1 | num_updates 2363 | best_loss 7.632\n",
            "2023-08-23 05:20:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 2363 updates\n",
            "2023-08-23 05:20:29 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint74.pt\n",
            "2023-08-23 05:20:29 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint74.pt\n",
            "2023-08-23 05:20:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint74.pt (epoch 74 @ 2363 updates, score 7.632) (writing took 0.18459530800009816 seconds)\n",
            "2023-08-23 05:20:29 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)\n",
            "2023-08-23 05:20:29 | INFO | train | epoch 074 | loss 6.644 | nll_loss 5.56 | ppl 47.16 | wps 99684.2 | ups 1.89 | wpb 52622.6 | bsz 1776 | num_updates 2363 | lr 0.000650531 | gnorm 0.258 | loss_scale 8 | train_wall 14 | gb_free 14.3 | wall 1277\n",
            "2023-08-23 05:20:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:20:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 075:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:20:29 | INFO | fairseq.trainer | begin training epoch 75\n",
            "2023-08-23 05:20:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 075:  97% 31/32 [00:16<00:00,  1.69it/s]2023-08-23 05:20:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:20:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 075 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 075 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 26.71it/s]\u001b[A\n",
            "epoch 075 | valid on 'valid' subset:  26% 7/27 [00:00<00:00, 29.13it/s]\u001b[A\n",
            "epoch 075 | valid on 'valid' subset:  41% 11/27 [00:00<00:00, 30.55it/s]\u001b[A\n",
            "epoch 075 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 32.55it/s]\u001b[A\n",
            "epoch 075 | valid on 'valid' subset:  70% 19/27 [00:00<00:00, 33.74it/s]\u001b[A\n",
            "epoch 075 | valid on 'valid' subset: 100% 27/27 [00:00<00:00, 47.04it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:20:47 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 7.641 | nll_loss 6.606 | ppl 97.44 | wps 90720.9 | wpb 2258.6 | bsz 74.1 | num_updates 2395 | best_loss 7.632\n",
            "2023-08-23 05:20:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 2395 updates\n",
            "2023-08-23 05:20:47 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint75.pt\n",
            "2023-08-23 05:20:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint75.pt\n",
            "2023-08-23 05:20:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint75.pt (epoch 75 @ 2395 updates, score 7.641) (writing took 0.11760704500011343 seconds)\n",
            "2023-08-23 05:20:47 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)\n",
            "2023-08-23 05:20:47 | INFO | train | epoch 075 | loss 6.628 | nll_loss 5.541 | ppl 46.55 | wps 94360.6 | ups 1.79 | wpb 52622.6 | bsz 1776 | num_updates 2395 | lr 0.000646171 | gnorm 0.241 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1295\n",
            "2023-08-23 05:20:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:20:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 076:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:20:47 | INFO | fairseq.trainer | begin training epoch 76\n",
            "2023-08-23 05:20:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 076:  97% 31/32 [00:16<00:00,  1.97it/s, loss=6.642, nll_loss=5.557, ppl=47.09, wps=98707.6, ups=1.88, wpb=52582.7, bsz=1776.1, num_updates=2400, lr=0.000645497, gnorm=0.248, loss_scale=16, train_wall=45, gb_free=14.3, wall=1298]2023-08-23 05:21:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:21:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 076 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 076 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 41.91it/s]\u001b[A\n",
            "epoch 076 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 39.98it/s]\u001b[A\n",
            "epoch 076 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 41.55it/s]\u001b[A\n",
            "epoch 076 | valid on 'valid' subset: 100% 27/27 [00:00<00:00, 67.63it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:21:04 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 7.606 | nll_loss 6.567 | ppl 94.78 | wps 135921 | wpb 2258.6 | bsz 74.1 | num_updates 2427 | best_loss 7.606\n",
            "2023-08-23 05:21:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 2427 updates\n",
            "2023-08-23 05:21:04 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint76.pt\n",
            "2023-08-23 05:21:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint76.pt\n",
            "2023-08-23 05:21:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint76.pt (epoch 76 @ 2427 updates, score 7.606) (writing took 0.13746431599997777 seconds)\n",
            "2023-08-23 05:21:04 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)\n",
            "2023-08-23 05:21:04 | INFO | train | epoch 076 | loss 6.611 | nll_loss 5.52 | ppl 45.9 | wps 99575.2 | ups 1.89 | wpb 52622.6 | bsz 1776 | num_updates 2427 | lr 0.000641897 | gnorm 0.223 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1312\n",
            "2023-08-23 05:21:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:21:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 077:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:21:04 | INFO | fairseq.trainer | begin training epoch 77\n",
            "2023-08-23 05:21:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 077:  97% 31/32 [00:18<00:00,  2.01it/s]2023-08-23 05:21:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:21:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 077 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 077 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 37.15it/s]\u001b[A\n",
            "epoch 077 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 46.75it/s]\u001b[A\n",
            "epoch 077 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 47.47it/s]\u001b[A\n",
            "epoch 077 | valid on 'valid' subset:  89% 24/27 [00:00<00:00, 58.70it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:21:23 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 7.622 | nll_loss 6.583 | ppl 95.9 | wps 133664 | wpb 2258.6 | bsz 74.1 | num_updates 2459 | best_loss 7.606\n",
            "2023-08-23 05:21:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 2459 updates\n",
            "2023-08-23 05:21:23 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint77.pt\n",
            "2023-08-23 05:21:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint77.pt\n",
            "2023-08-23 05:21:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint77.pt (epoch 77 @ 2459 updates, score 7.622) (writing took 0.08757921699998406 seconds)\n",
            "2023-08-23 05:21:23 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)\n",
            "2023-08-23 05:21:23 | INFO | train | epoch 077 | loss 6.597 | nll_loss 5.503 | ppl 45.36 | wps 89643.5 | ups 1.7 | wpb 52622.6 | bsz 1776 | num_updates 2459 | lr 0.000637706 | gnorm 0.263 | loss_scale 16 | train_wall 16 | gb_free 14.3 | wall 1331\n",
            "2023-08-23 05:21:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:21:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 078:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:21:23 | INFO | fairseq.trainer | begin training epoch 78\n",
            "2023-08-23 05:21:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 078:  97% 31/32 [00:16<00:00,  2.07it/s]2023-08-23 05:21:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:21:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 078 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 078 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 45.37it/s]\u001b[A\n",
            "epoch 078 | valid on 'valid' subset:  41% 11/27 [00:00<00:00, 47.99it/s]\u001b[A\n",
            "epoch 078 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 45.07it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:21:40 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 7.589 | nll_loss 6.545 | ppl 93.36 | wps 144256 | wpb 2258.6 | bsz 74.1 | num_updates 2491 | best_loss 7.589\n",
            "2023-08-23 05:21:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 2491 updates\n",
            "2023-08-23 05:21:40 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint78.pt\n",
            "2023-08-23 05:21:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint78.pt\n",
            "2023-08-23 05:21:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint78.pt (epoch 78 @ 2491 updates, score 7.589) (writing took 0.13840344499999446 seconds)\n",
            "2023-08-23 05:21:40 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)\n",
            "2023-08-23 05:21:40 | INFO | train | epoch 078 | loss 6.583 | nll_loss 5.488 | ppl 44.88 | wps 95531.4 | ups 1.82 | wpb 52622.6 | bsz 1776 | num_updates 2491 | lr 0.000633597 | gnorm 0.295 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1349\n",
            "2023-08-23 05:21:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:21:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 079:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:21:40 | INFO | fairseq.trainer | begin training epoch 79\n",
            "2023-08-23 05:21:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 079:  97% 31/32 [00:16<00:00,  2.15it/s, loss=6.593, nll_loss=5.499, ppl=45.22, wps=94694.1, ups=1.8, wpb=52589, bsz=1784.7, num_updates=2500, lr=0.000632456, gnorm=0.26, loss_scale=16, train_wall=48, gb_free=14.3, wall=1353]2023-08-23 05:21:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:21:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 079 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 079 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 34.23it/s]\u001b[A\n",
            "epoch 079 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 34.84it/s]\u001b[A\n",
            "epoch 079 | valid on 'valid' subset:  48% 13/27 [00:00<00:00, 35.64it/s]\u001b[A\n",
            "epoch 079 | valid on 'valid' subset:  81% 22/27 [00:00<00:00, 53.52it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:21:57 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 7.562 | nll_loss 6.513 | ppl 91.34 | wps 122163 | wpb 2258.6 | bsz 74.1 | num_updates 2523 | best_loss 7.562\n",
            "2023-08-23 05:21:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 2523 updates\n",
            "2023-08-23 05:21:57 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint79.pt\n",
            "2023-08-23 05:21:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint79.pt\n",
            "2023-08-23 05:21:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint79.pt (epoch 79 @ 2523 updates, score 7.562) (writing took 0.1271240979999675 seconds)\n",
            "2023-08-23 05:21:57 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)\n",
            "2023-08-23 05:21:57 | INFO | train | epoch 079 | loss 6.565 | nll_loss 5.467 | ppl 44.23 | wps 99563.6 | ups 1.89 | wpb 52622.6 | bsz 1776 | num_updates 2523 | lr 0.000629566 | gnorm 0.237 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1365\n",
            "2023-08-23 05:21:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:21:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 080:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:21:57 | INFO | fairseq.trainer | begin training epoch 80\n",
            "2023-08-23 05:21:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 080:  97% 31/32 [00:17<00:00,  1.76it/s]2023-08-23 05:22:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:22:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 080 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 080 | valid on 'valid' subset:  11% 3/27 [00:00<00:01, 23.37it/s]\u001b[A\n",
            "epoch 080 | valid on 'valid' subset:  22% 6/27 [00:00<00:00, 26.67it/s]\u001b[A\n",
            "epoch 080 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 29.29it/s]\u001b[A\n",
            "epoch 080 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 31.14it/s]\u001b[A\n",
            "epoch 080 | valid on 'valid' subset:  67% 18/27 [00:00<00:00, 33.97it/s]\u001b[A\n",
            "epoch 080 | valid on 'valid' subset: 100% 27/27 [00:00<00:00, 49.52it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:22:15 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 7.563 | nll_loss 6.514 | ppl 91.42 | wps 94760.4 | wpb 2258.6 | bsz 74.1 | num_updates 2555 | best_loss 7.562\n",
            "2023-08-23 05:22:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 2555 updates\n",
            "2023-08-23 05:22:15 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint80.pt\n",
            "2023-08-23 05:22:15 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint80.pt\n",
            "2023-08-23 05:22:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint80.pt (epoch 80 @ 2555 updates, score 7.563) (writing took 0.12453161399980672 seconds)\n",
            "2023-08-23 05:22:15 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)\n",
            "2023-08-23 05:22:15 | INFO | train | epoch 080 | loss 6.551 | nll_loss 5.45 | ppl 43.7 | wps 93219.4 | ups 1.77 | wpb 52622.6 | bsz 1776 | num_updates 2555 | lr 0.000625611 | gnorm 0.278 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1384\n",
            "2023-08-23 05:22:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:22:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 081:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:22:15 | INFO | fairseq.trainer | begin training epoch 81\n",
            "2023-08-23 05:22:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 081:  97% 31/32 [00:17<00:00,  1.66it/s]2023-08-23 05:22:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:22:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 081 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 081 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 35.76it/s]\u001b[A\n",
            "epoch 081 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 41.79it/s]\u001b[A\n",
            "epoch 081 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 42.58it/s]\u001b[A\n",
            "epoch 081 | valid on 'valid' subset:  70% 19/27 [00:00<00:00, 44.85it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:22:33 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 7.536 | nll_loss 6.489 | ppl 89.83 | wps 122090 | wpb 2258.6 | bsz 74.1 | num_updates 2587 | best_loss 7.536\n",
            "2023-08-23 05:22:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 2587 updates\n",
            "2023-08-23 05:22:33 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint81.pt\n",
            "2023-08-23 05:22:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint81.pt\n",
            "2023-08-23 05:22:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint81.pt (epoch 81 @ 2587 updates, score 7.536) (writing took 0.12459952099970906 seconds)\n",
            "2023-08-23 05:22:33 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)\n",
            "2023-08-23 05:22:33 | INFO | train | epoch 081 | loss 6.536 | nll_loss 5.432 | ppl 43.17 | wps 93835.5 | ups 1.78 | wpb 52622.6 | bsz 1776 | num_updates 2587 | lr 0.00062173 | gnorm 0.257 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1401\n",
            "2023-08-23 05:22:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:22:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 082:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:22:33 | INFO | fairseq.trainer | begin training epoch 82\n",
            "2023-08-23 05:22:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 082:  97% 31/32 [00:16<00:00,  2.05it/s, loss=6.549, nll_loss=5.448, ppl=43.64, wps=96218, ups=1.82, wpb=52772.4, bsz=1766.1, num_updates=2600, lr=0.000620174, gnorm=0.263, loss_scale=16, train_wall=47, gb_free=14.3, wall=1408]2023-08-23 05:22:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:22:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 082 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 082 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 31.59it/s]\u001b[A\n",
            "epoch 082 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 44.52it/s]\u001b[A\n",
            "epoch 082 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 43.94it/s]\u001b[A\n",
            "epoch 082 | valid on 'valid' subset:  93% 25/27 [00:00<00:00, 63.84it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:22:51 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 7.548 | nll_loss 6.498 | ppl 90.36 | wps 136705 | wpb 2258.6 | bsz 74.1 | num_updates 2619 | best_loss 7.536\n",
            "2023-08-23 05:22:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 2619 updates\n",
            "2023-08-23 05:22:51 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint82.pt\n",
            "2023-08-23 05:22:51 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint82.pt\n",
            "2023-08-23 05:22:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint82.pt (epoch 82 @ 2619 updates, score 7.548) (writing took 0.08816663599964158 seconds)\n",
            "2023-08-23 05:22:51 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)\n",
            "2023-08-23 05:22:51 | INFO | train | epoch 082 | loss 6.522 | nll_loss 5.416 | ppl 42.7 | wps 95732.8 | ups 1.82 | wpb 52622.6 | bsz 1776 | num_updates 2619 | lr 0.00061792 | gnorm 0.279 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1419\n",
            "2023-08-23 05:22:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:22:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 083:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:22:51 | INFO | fairseq.trainer | begin training epoch 83\n",
            "2023-08-23 05:22:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 083:  97% 31/32 [00:16<00:00,  2.16it/s]2023-08-23 05:23:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:23:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 083 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 083 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 32.12it/s]\u001b[A\n",
            "epoch 083 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 40.27it/s]\u001b[A\n",
            "epoch 083 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 43.06it/s]\u001b[A\n",
            "epoch 083 | valid on 'valid' subset:  93% 25/27 [00:00<00:00, 62.88it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:23:08 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 7.529 | nll_loss 6.476 | ppl 89 | wps 131787 | wpb 2258.6 | bsz 74.1 | num_updates 2651 | best_loss 7.529\n",
            "2023-08-23 05:23:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 2651 updates\n",
            "2023-08-23 05:23:08 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint83.pt\n",
            "2023-08-23 05:23:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint83.pt\n",
            "2023-08-23 05:23:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint83.pt (epoch 83 @ 2651 updates, score 7.529) (writing took 0.13098055399996156 seconds)\n",
            "2023-08-23 05:23:08 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)\n",
            "2023-08-23 05:23:08 | INFO | train | epoch 083 | loss 6.509 | nll_loss 5.4 | ppl 42.22 | wps 96982 | ups 1.84 | wpb 52622.6 | bsz 1776 | num_updates 2651 | lr 0.000614179 | gnorm 0.272 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1436\n",
            "2023-08-23 05:23:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:23:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 084:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:23:08 | INFO | fairseq.trainer | begin training epoch 84\n",
            "2023-08-23 05:23:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 084:  97% 31/32 [00:16<00:00,  2.10it/s]2023-08-23 05:23:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:23:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 084 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 084 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 38.22it/s]\u001b[A\n",
            "epoch 084 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 41.25it/s]\u001b[A\n",
            "epoch 084 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 42.52it/s]\u001b[A\n",
            "epoch 084 | valid on 'valid' subset:  81% 22/27 [00:00<00:00, 55.22it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:23:26 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 7.509 | nll_loss 6.452 | ppl 87.57 | wps 132025 | wpb 2258.6 | bsz 74.1 | num_updates 2683 | best_loss 7.509\n",
            "2023-08-23 05:23:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 2683 updates\n",
            "2023-08-23 05:23:26 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint84.pt\n",
            "2023-08-23 05:23:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint84.pt\n",
            "2023-08-23 05:23:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint84.pt (epoch 84 @ 2683 updates, score 7.509) (writing took 0.11937620800017612 seconds)\n",
            "2023-08-23 05:23:26 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)\n",
            "2023-08-23 05:23:26 | INFO | train | epoch 084 | loss 6.495 | nll_loss 5.385 | ppl 41.79 | wps 96222.1 | ups 1.83 | wpb 52622.6 | bsz 1776 | num_updates 2683 | lr 0.000610506 | gnorm 0.283 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1454\n",
            "2023-08-23 05:23:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:23:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 085:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:23:26 | INFO | fairseq.trainer | begin training epoch 85\n",
            "2023-08-23 05:23:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 085:  97% 31/32 [00:17<00:00,  1.75it/s, loss=6.499, nll_loss=5.389, ppl=41.9, wps=94744.3, ups=1.8, wpb=52587.5, bsz=1784.2, num_updates=2700, lr=0.000608581, gnorm=0.276, loss_scale=16, train_wall=48, gb_free=14.3, wall=1464]2023-08-23 05:23:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:23:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 085 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 085 | valid on 'valid' subset:  11% 3/27 [00:00<00:01, 23.82it/s]\u001b[A\n",
            "epoch 085 | valid on 'valid' subset:  22% 6/27 [00:00<00:00, 24.87it/s]\u001b[A\n",
            "epoch 085 | valid on 'valid' subset:  44% 12/27 [00:00<00:00, 37.53it/s]\u001b[A\n",
            "epoch 085 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 37.93it/s]\u001b[A\n",
            "epoch 085 | valid on 'valid' subset:  89% 24/27 [00:00<00:00, 51.43it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:23:44 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 7.496 | nll_loss 6.437 | ppl 86.64 | wps 109264 | wpb 2258.6 | bsz 74.1 | num_updates 2715 | best_loss 7.496\n",
            "2023-08-23 05:23:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 2715 updates\n",
            "2023-08-23 05:23:44 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint85.pt\n",
            "2023-08-23 05:23:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint85.pt\n",
            "2023-08-23 05:23:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint85.pt (epoch 85 @ 2715 updates, score 7.496) (writing took 0.1767467299996497 seconds)\n",
            "2023-08-23 05:23:44 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)\n",
            "2023-08-23 05:23:44 | INFO | train | epoch 085 | loss 6.481 | nll_loss 5.367 | ppl 41.28 | wps 92924.4 | ups 1.77 | wpb 52622.6 | bsz 1776 | num_updates 2715 | lr 0.000606897 | gnorm 0.267 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1472\n",
            "2023-08-23 05:23:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:23:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 086:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:23:44 | INFO | fairseq.trainer | begin training epoch 86\n",
            "2023-08-23 05:23:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 086:  97% 31/32 [00:16<00:00,  1.76it/s]2023-08-23 05:24:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:24:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 086 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 086 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 34.85it/s]\u001b[A\n",
            "epoch 086 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 37.08it/s]\u001b[A\n",
            "epoch 086 | valid on 'valid' subset:  48% 13/27 [00:00<00:00, 39.93it/s]\u001b[A\n",
            "epoch 086 | valid on 'valid' subset:  78% 21/27 [00:00<00:00, 53.91it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:24:01 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 7.521 | nll_loss 6.459 | ppl 87.98 | wps 129799 | wpb 2258.6 | bsz 74.1 | num_updates 2747 | best_loss 7.496\n",
            "2023-08-23 05:24:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 2747 updates\n",
            "2023-08-23 05:24:01 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint86.pt\n",
            "2023-08-23 05:24:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint86.pt\n",
            "2023-08-23 05:24:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint86.pt (epoch 86 @ 2747 updates, score 7.521) (writing took 0.08752017400001932 seconds)\n",
            "2023-08-23 05:24:01 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)\n",
            "2023-08-23 05:24:01 | INFO | train | epoch 086 | loss 6.468 | nll_loss 5.352 | ppl 40.84 | wps 95315.6 | ups 1.81 | wpb 52622.6 | bsz 1776 | num_updates 2747 | lr 0.000603352 | gnorm 0.25 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1490\n",
            "2023-08-23 05:24:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:24:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 087:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:24:02 | INFO | fairseq.trainer | begin training epoch 87\n",
            "2023-08-23 05:24:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 087:  97% 31/32 [00:16<00:00,  2.11it/s]2023-08-23 05:24:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:24:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 087 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 087 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 36.62it/s]\u001b[A\n",
            "epoch 087 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 36.99it/s]\u001b[A\n",
            "epoch 087 | valid on 'valid' subset:  44% 12/27 [00:00<00:00, 37.72it/s]\u001b[A\n",
            "epoch 087 | valid on 'valid' subset:  70% 19/27 [00:00<00:00, 49.07it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:24:19 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 7.484 | nll_loss 6.419 | ppl 85.58 | wps 127968 | wpb 2258.6 | bsz 74.1 | num_updates 2779 | best_loss 7.484\n",
            "2023-08-23 05:24:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 2779 updates\n",
            "2023-08-23 05:24:19 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint87.pt\n",
            "2023-08-23 05:24:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint87.pt\n",
            "2023-08-23 05:24:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint87.pt (epoch 87 @ 2779 updates, score 7.484) (writing took 0.1175309569998717 seconds)\n",
            "2023-08-23 05:24:19 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)\n",
            "2023-08-23 05:24:19 | INFO | train | epoch 087 | loss 6.454 | nll_loss 5.336 | ppl 40.4 | wps 96805.9 | ups 1.84 | wpb 52622.6 | bsz 1776 | num_updates 2779 | lr 0.000599868 | gnorm 0.234 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1507\n",
            "2023-08-23 05:24:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:24:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 088:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:24:19 | INFO | fairseq.trainer | begin training epoch 88\n",
            "2023-08-23 05:24:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 088:  97% 31/32 [00:16<00:00,  2.08it/s, loss=6.466, nll_loss=5.35, ppl=40.78, wps=95611.7, ups=1.82, wpb=52624.1, bsz=1764.9, num_updates=2800, lr=0.000597614, gnorm=0.248, loss_scale=16, train_wall=47, gb_free=14.3, wall=1519]2023-08-23 05:24:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:24:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 088 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 088 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 36.66it/s]\u001b[A\n",
            "epoch 088 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 39.95it/s]\u001b[A\n",
            "epoch 088 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 44.11it/s]\u001b[A\n",
            "epoch 088 | valid on 'valid' subset:  89% 24/27 [00:00<00:00, 58.85it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:24:36 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 7.488 | nll_loss 6.426 | ppl 85.97 | wps 131204 | wpb 2258.6 | bsz 74.1 | num_updates 2811 | best_loss 7.484\n",
            "2023-08-23 05:24:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 2811 updates\n",
            "2023-08-23 05:24:36 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint88.pt\n",
            "2023-08-23 05:24:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint88.pt\n",
            "2023-08-23 05:24:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint88.pt (epoch 88 @ 2811 updates, score 7.488) (writing took 0.08336157200028538 seconds)\n",
            "2023-08-23 05:24:36 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)\n",
            "2023-08-23 05:24:36 | INFO | train | epoch 088 | loss 6.445 | nll_loss 5.325 | ppl 40.08 | wps 97120.4 | ups 1.85 | wpb 52622.6 | bsz 1776 | num_updates 2811 | lr 0.000596444 | gnorm 0.268 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1524\n",
            "2023-08-23 05:24:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:24:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 089:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:24:36 | INFO | fairseq.trainer | begin training epoch 89\n",
            "2023-08-23 05:24:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 089:  97% 31/32 [00:18<00:00,  2.12it/s]2023-08-23 05:24:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:24:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 089 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 089 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 43.93it/s]\u001b[A\n",
            "epoch 089 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 45.03it/s]\u001b[A\n",
            "epoch 089 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 43.53it/s]\u001b[A\n",
            "epoch 089 | valid on 'valid' subset:  89% 24/27 [00:00<00:00, 60.32it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:24:55 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 7.456 | nll_loss 6.389 | ppl 83.8 | wps 135517 | wpb 2258.6 | bsz 74.1 | num_updates 2843 | best_loss 7.456\n",
            "2023-08-23 05:24:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 2843 updates\n",
            "2023-08-23 05:24:55 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint89.pt\n",
            "2023-08-23 05:24:55 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint89.pt\n",
            "2023-08-23 05:24:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint89.pt (epoch 89 @ 2843 updates, score 7.456) (writing took 0.11967939499982094 seconds)\n",
            "2023-08-23 05:24:55 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)\n",
            "2023-08-23 05:24:55 | INFO | train | epoch 089 | loss 6.431 | nll_loss 5.309 | ppl 39.65 | wps 87915.2 | ups 1.67 | wpb 52622.6 | bsz 1776 | num_updates 2843 | lr 0.000593078 | gnorm 0.26 | loss_scale 16 | train_wall 17 | gb_free 14.3 | wall 1544\n",
            "2023-08-23 05:24:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:24:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 090:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:24:55 | INFO | fairseq.trainer | begin training epoch 90\n",
            "2023-08-23 05:24:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 090:  97% 31/32 [00:17<00:00,  1.76it/s]2023-08-23 05:25:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:25:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 090 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 090 | valid on 'valid' subset:  11% 3/27 [00:00<00:01, 22.62it/s]\u001b[A\n",
            "epoch 090 | valid on 'valid' subset:  26% 7/27 [00:00<00:00, 29.03it/s]\u001b[A\n",
            "epoch 090 | valid on 'valid' subset:  41% 11/27 [00:00<00:00, 32.29it/s]\u001b[A\n",
            "epoch 090 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 31.21it/s]\u001b[A\n",
            "epoch 090 | valid on 'valid' subset:  78% 21/27 [00:00<00:00, 39.70it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:25:13 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 7.477 | nll_loss 6.413 | ppl 85.19 | wps 95252.7 | wpb 2258.6 | bsz 74.1 | num_updates 2875 | best_loss 7.456\n",
            "2023-08-23 05:25:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 2875 updates\n",
            "2023-08-23 05:25:13 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint90.pt\n",
            "2023-08-23 05:25:13 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint90.pt\n",
            "2023-08-23 05:25:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint90.pt (epoch 90 @ 2875 updates, score 7.477) (writing took 0.12253654000005554 seconds)\n",
            "2023-08-23 05:25:13 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)\n",
            "2023-08-23 05:25:13 | INFO | train | epoch 090 | loss 6.419 | nll_loss 5.295 | ppl 39.26 | wps 93006.8 | ups 1.77 | wpb 52622.6 | bsz 1776 | num_updates 2875 | lr 0.000589768 | gnorm 0.267 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1562\n",
            "2023-08-23 05:25:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:25:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 091:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:25:14 | INFO | fairseq.trainer | begin training epoch 91\n",
            "2023-08-23 05:25:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 091:  97% 31/32 [00:17<00:00,  1.67it/s, loss=6.416, nll_loss=5.292, ppl=39.18, wps=93650.5, ups=1.77, wpb=52777.9, bsz=1793.9, num_updates=2900, lr=0.00058722, gnorm=0.27, loss_scale=16, train_wall=48, gb_free=14.3, wall=1575]2023-08-23 05:25:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:25:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 091 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 091 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 36.28it/s]\u001b[A\n",
            "epoch 091 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 34.98it/s]\u001b[A\n",
            "epoch 091 | valid on 'valid' subset:  44% 12/27 [00:00<00:00, 35.58it/s]\u001b[A\n",
            "epoch 091 | valid on 'valid' subset:  81% 22/27 [00:00<00:00, 58.29it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:25:31 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 7.436 | nll_loss 6.365 | ppl 82.45 | wps 128652 | wpb 2258.6 | bsz 74.1 | num_updates 2907 | best_loss 7.436\n",
            "2023-08-23 05:25:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 2907 updates\n",
            "2023-08-23 05:25:31 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint91.pt\n",
            "2023-08-23 05:25:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint91.pt\n",
            "2023-08-23 05:25:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint91.pt (epoch 91 @ 2907 updates, score 7.436) (writing took 0.1337895690003279 seconds)\n",
            "2023-08-23 05:25:32 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)\n",
            "2023-08-23 05:25:32 | INFO | train | epoch 091 | loss 6.407 | nll_loss 5.281 | ppl 38.88 | wps 93256.3 | ups 1.77 | wpb 52622.6 | bsz 1776 | num_updates 2907 | lr 0.000586513 | gnorm 0.276 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1580\n",
            "2023-08-23 05:25:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:25:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 092:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:25:32 | INFO | fairseq.trainer | begin training epoch 92\n",
            "2023-08-23 05:25:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 092:  97% 31/32 [00:16<00:00,  2.01it/s]2023-08-23 05:25:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:25:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 092 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 092 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 32.83it/s]\u001b[A\n",
            "epoch 092 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 39.70it/s]\u001b[A\n",
            "epoch 092 | valid on 'valid' subset:  48% 13/27 [00:00<00:00, 38.12it/s]\u001b[A\n",
            "epoch 092 | valid on 'valid' subset:  78% 21/27 [00:00<00:00, 52.35it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:25:49 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 7.427 | nll_loss 6.358 | ppl 82.01 | wps 126972 | wpb 2258.6 | bsz 74.1 | num_updates 2939 | best_loss 7.427\n",
            "2023-08-23 05:25:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 2939 updates\n",
            "2023-08-23 05:25:49 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint92.pt\n",
            "2023-08-23 05:25:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint92.pt\n",
            "2023-08-23 05:25:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint92.pt (epoch 92 @ 2939 updates, score 7.427) (writing took 0.11614267899994957 seconds)\n",
            "2023-08-23 05:25:49 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)\n",
            "2023-08-23 05:25:49 | INFO | train | epoch 092 | loss 6.397 | nll_loss 5.269 | ppl 38.55 | wps 95362.9 | ups 1.81 | wpb 52622.6 | bsz 1776 | num_updates 2939 | lr 0.000583311 | gnorm 0.301 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1597\n",
            "2023-08-23 05:25:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:25:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 093:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:25:49 | INFO | fairseq.trainer | begin training epoch 93\n",
            "2023-08-23 05:25:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 093:  97% 31/32 [00:16<00:00,  2.21it/s]2023-08-23 05:26:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:26:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 093 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 093 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 33.12it/s]\u001b[A\n",
            "epoch 093 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 42.26it/s]\u001b[A\n",
            "epoch 093 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 43.66it/s]\u001b[A\n",
            "epoch 093 | valid on 'valid' subset:  85% 23/27 [00:00<00:00, 59.85it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:26:06 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 7.436 | nll_loss 6.365 | ppl 82.44 | wps 137724 | wpb 2258.6 | bsz 74.1 | num_updates 2971 | best_loss 7.427\n",
            "2023-08-23 05:26:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 2971 updates\n",
            "2023-08-23 05:26:06 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint93.pt\n",
            "2023-08-23 05:26:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint93.pt\n",
            "2023-08-23 05:26:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint93.pt (epoch 93 @ 2971 updates, score 7.436) (writing took 0.08545933899995362 seconds)\n",
            "2023-08-23 05:26:06 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)\n",
            "2023-08-23 05:26:06 | INFO | train | epoch 093 | loss 6.384 | nll_loss 5.254 | ppl 38.17 | wps 97712 | ups 1.86 | wpb 52622.6 | bsz 1776 | num_updates 2971 | lr 0.000580161 | gnorm 0.253 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1615\n",
            "2023-08-23 05:26:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:26:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 094:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:26:06 | INFO | fairseq.trainer | begin training epoch 94\n",
            "2023-08-23 05:26:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 094:  97% 31/32 [00:16<00:00,  2.18it/s, loss=6.389, nll_loss=5.26, ppl=38.31, wps=95954.1, ups=1.82, wpb=52659.3, bsz=1769.3, num_updates=3000, lr=0.00057735, gnorm=0.265, loss_scale=16, train_wall=47, gb_free=14.3, wall=1630]2023-08-23 05:26:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:26:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 094 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 094 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 43.94it/s]\u001b[A\n",
            "epoch 094 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 42.26it/s]\u001b[A\n",
            "epoch 094 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 47.90it/s]\u001b[A\n",
            "epoch 094 | valid on 'valid' subset: 100% 27/27 [00:00<00:00, 68.46it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:26:23 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 7.425 | nll_loss 6.35 | ppl 81.56 | wps 140568 | wpb 2258.6 | bsz 74.1 | num_updates 3003 | best_loss 7.425\n",
            "2023-08-23 05:26:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 3003 updates\n",
            "2023-08-23 05:26:23 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint94.pt\n",
            "2023-08-23 05:26:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint94.pt\n",
            "2023-08-23 05:26:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint94.pt (epoch 94 @ 3003 updates, score 7.425) (writing took 0.11771336600031646 seconds)\n",
            "2023-08-23 05:26:23 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)\n",
            "2023-08-23 05:26:23 | INFO | train | epoch 094 | loss 6.372 | nll_loss 5.24 | ppl 37.78 | wps 99094.5 | ups 1.88 | wpb 52622.6 | bsz 1776 | num_updates 3003 | lr 0.000577062 | gnorm 0.236 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1632\n",
            "2023-08-23 05:26:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:26:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 095:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:26:23 | INFO | fairseq.trainer | begin training epoch 95\n",
            "2023-08-23 05:26:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 095:  97% 31/32 [00:16<00:00,  1.92it/s]2023-08-23 05:26:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:26:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 095 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 095 | valid on 'valid' subset:  11% 3/27 [00:00<00:01, 21.45it/s]\u001b[A\n",
            "epoch 095 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 32.85it/s]\u001b[A\n",
            "epoch 095 | valid on 'valid' subset:  44% 12/27 [00:00<00:00, 30.19it/s]\u001b[A\n",
            "epoch 095 | valid on 'valid' subset:  63% 17/27 [00:00<00:00, 35.41it/s]\u001b[A\n",
            "epoch 095 | valid on 'valid' subset:  81% 22/27 [00:00<00:00, 38.95it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:26:41 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 7.441 | nll_loss 6.367 | ppl 82.54 | wps 93910.7 | wpb 2258.6 | bsz 74.1 | num_updates 3035 | best_loss 7.425\n",
            "2023-08-23 05:26:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 3035 updates\n",
            "2023-08-23 05:26:41 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint95.pt\n",
            "2023-08-23 05:26:41 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint95.pt\n",
            "2023-08-23 05:26:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint95.pt (epoch 95 @ 3035 updates, score 7.441) (writing took 0.12189341599969339 seconds)\n",
            "2023-08-23 05:26:41 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)\n",
            "2023-08-23 05:26:41 | INFO | train | epoch 095 | loss 6.362 | nll_loss 5.228 | ppl 37.48 | wps 96751.6 | ups 1.84 | wpb 52622.6 | bsz 1776 | num_updates 3035 | lr 0.000574012 | gnorm 0.27 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1649\n",
            "2023-08-23 05:26:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:26:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 096:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:26:41 | INFO | fairseq.trainer | begin training epoch 96\n",
            "2023-08-23 05:26:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 096:  97% 31/32 [00:16<00:00,  1.66it/s]2023-08-23 05:26:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:26:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 096 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 096 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 27.62it/s]\u001b[A\n",
            "epoch 096 | valid on 'valid' subset:  22% 6/27 [00:00<00:00, 28.14it/s]\u001b[A\n",
            "epoch 096 | valid on 'valid' subset:  41% 11/27 [00:00<00:00, 37.56it/s]\u001b[A\n",
            "epoch 096 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 35.86it/s]\u001b[A\n",
            "epoch 096 | valid on 'valid' subset:  96% 26/27 [00:00<00:00, 60.11it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:26:59 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 7.401 | nll_loss 6.323 | ppl 80.08 | wps 124078 | wpb 2258.6 | bsz 74.1 | num_updates 3067 | best_loss 7.401\n",
            "2023-08-23 05:26:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 3067 updates\n",
            "2023-08-23 05:26:59 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint96.pt\n",
            "2023-08-23 05:26:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint96.pt\n",
            "2023-08-23 05:26:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint96.pt (epoch 96 @ 3067 updates, score 7.401) (writing took 0.12809561099993516 seconds)\n",
            "2023-08-23 05:26:59 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)\n",
            "2023-08-23 05:26:59 | INFO | train | epoch 096 | loss 6.353 | nll_loss 5.217 | ppl 37.2 | wps 94554.7 | ups 1.8 | wpb 52622.6 | bsz 1776 | num_updates 3067 | lr 0.000571009 | gnorm 0.305 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1667\n",
            "2023-08-23 05:26:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:26:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 097:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:26:59 | INFO | fairseq.trainer | begin training epoch 97\n",
            "2023-08-23 05:26:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 097:  97% 31/32 [00:16<00:00,  2.05it/s]2023-08-23 05:27:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:27:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 097 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 097 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 37.78it/s]\u001b[A\n",
            "epoch 097 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 40.16it/s]\u001b[A\n",
            "epoch 097 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 41.81it/s]\u001b[A\n",
            "epoch 097 | valid on 'valid' subset:  78% 21/27 [00:00<00:00, 51.87it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:27:16 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 7.392 | nll_loss 6.312 | ppl 79.46 | wps 128141 | wpb 2258.6 | bsz 74.1 | num_updates 3099 | best_loss 7.392\n",
            "2023-08-23 05:27:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 3099 updates\n",
            "2023-08-23 05:27:16 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint97.pt\n",
            "2023-08-23 05:27:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint97.pt\n",
            "2023-08-23 05:27:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint97.pt (epoch 97 @ 3099 updates, score 7.392) (writing took 0.13415623599985338 seconds)\n",
            "2023-08-23 05:27:16 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)\n",
            "2023-08-23 05:27:16 | INFO | train | epoch 097 | loss 6.343 | nll_loss 5.205 | ppl 36.9 | wps 98770 | ups 1.88 | wpb 52622.6 | bsz 1776 | num_updates 3099 | lr 0.000568053 | gnorm 0.274 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1684\n",
            "2023-08-23 05:27:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:27:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 098:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:27:16 | INFO | fairseq.trainer | begin training epoch 98\n",
            "2023-08-23 05:27:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 098:  97% 31/32 [00:16<00:00,  2.09it/s, loss=6.351, nll_loss=5.215, ppl=37.14, wps=95928.9, ups=1.83, wpb=52364.9, bsz=1770.4, num_updates=3100, lr=0.000567962, gnorm=0.282, loss_scale=16, train_wall=46, gb_free=14.3, wall=1685]2023-08-23 05:27:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:27:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 098 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 098 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 35.75it/s]\u001b[A\n",
            "epoch 098 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 38.20it/s]\u001b[A\n",
            "epoch 098 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 38.03it/s]\u001b[A\n",
            "epoch 098 | valid on 'valid' subset:  81% 22/27 [00:00<00:00, 51.84it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:27:33 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 7.424 | nll_loss 6.342 | ppl 81.12 | wps 123328 | wpb 2258.6 | bsz 74.1 | num_updates 3131 | best_loss 7.392\n",
            "2023-08-23 05:27:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 3131 updates\n",
            "2023-08-23 05:27:33 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint98.pt\n",
            "2023-08-23 05:27:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint98.pt\n",
            "2023-08-23 05:27:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint98.pt (epoch 98 @ 3131 updates, score 7.424) (writing took 0.08636098299984951 seconds)\n",
            "2023-08-23 05:27:33 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)\n",
            "2023-08-23 05:27:33 | INFO | train | epoch 098 | loss 6.33 | nll_loss 5.19 | ppl 36.5 | wps 98259.6 | ups 1.87 | wpb 52622.6 | bsz 1776 | num_updates 3131 | lr 0.000565143 | gnorm 0.254 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1701\n",
            "2023-08-23 05:27:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:27:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 099:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:27:33 | INFO | fairseq.trainer | begin training epoch 99\n",
            "2023-08-23 05:27:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 099:  97% 31/32 [00:16<00:00,  2.09it/s]2023-08-23 05:27:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:27:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 099 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 099 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 28.02it/s]\u001b[A\n",
            "epoch 099 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 40.07it/s]\u001b[A\n",
            "epoch 099 | valid on 'valid' subset:  48% 13/27 [00:00<00:00, 37.92it/s]\u001b[A\n",
            "epoch 099 | valid on 'valid' subset:  70% 19/27 [00:00<00:00, 44.60it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:27:50 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 7.39 | nll_loss 6.303 | ppl 78.95 | wps 120319 | wpb 2258.6 | bsz 74.1 | num_updates 3163 | best_loss 7.39\n",
            "2023-08-23 05:27:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 3163 updates\n",
            "2023-08-23 05:27:50 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint99.pt\n",
            "2023-08-23 05:27:51 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint99.pt\n",
            "2023-08-23 05:27:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint99.pt (epoch 99 @ 3163 updates, score 7.39) (writing took 0.12156040199988638 seconds)\n",
            "2023-08-23 05:27:51 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)\n",
            "2023-08-23 05:27:51 | INFO | train | epoch 099 | loss 6.32 | nll_loss 5.179 | ppl 36.22 | wps 94699.8 | ups 1.8 | wpb 52622.6 | bsz 1776 | num_updates 3163 | lr 0.000562277 | gnorm 0.251 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1719\n",
            "2023-08-23 05:27:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:27:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 100:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:27:51 | INFO | fairseq.trainer | begin training epoch 100\n",
            "2023-08-23 05:27:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 100:  97% 31/32 [00:17<00:00,  1.77it/s]2023-08-23 05:28:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:28:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 100 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 100 | valid on 'valid' subset:   7% 2/27 [00:00<00:01, 16.52it/s]\u001b[A\n",
            "epoch 100 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 23.44it/s]\u001b[A\n",
            "epoch 100 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 27.02it/s]\u001b[A\n",
            "epoch 100 | valid on 'valid' subset:  44% 12/27 [00:00<00:00, 27.75it/s]\u001b[A\n",
            "epoch 100 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 28.04it/s]\u001b[A\n",
            "epoch 100 | valid on 'valid' subset:  67% 18/27 [00:00<00:00, 28.07it/s]\u001b[A\n",
            "epoch 100 | valid on 'valid' subset:  89% 24/27 [00:00<00:00, 37.71it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:28:09 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 7.38 | nll_loss 6.296 | ppl 78.58 | wps 78372.5 | wpb 2258.6 | bsz 74.1 | num_updates 3195 | best_loss 7.38\n",
            "2023-08-23 05:28:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 3195 updates\n",
            "2023-08-23 05:28:09 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint100.pt\n",
            "2023-08-23 05:28:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint100.pt\n",
            "2023-08-23 05:28:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint100.pt (epoch 100 @ 3195 updates, score 7.38) (writing took 0.20532926999976553 seconds)\n",
            "2023-08-23 05:28:09 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)\n",
            "2023-08-23 05:28:09 | INFO | train | epoch 100 | loss 6.312 | nll_loss 5.169 | ppl 35.97 | wps 91235.9 | ups 1.73 | wpb 52622.6 | bsz 1776 | num_updates 3195 | lr 0.000559454 | gnorm 0.256 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1737\n",
            "2023-08-23 05:28:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:28:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 101:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:28:09 | INFO | fairseq.trainer | begin training epoch 101\n",
            "2023-08-23 05:28:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 101:  97% 31/32 [00:18<00:00,  1.83it/s, loss=6.319, nll_loss=5.178, ppl=36.2, wps=92257, ups=1.75, wpb=52640.7, bsz=1775, num_updates=3200, lr=0.000559017, gnorm=0.256, loss_scale=16, train_wall=49, gb_free=14.3, wall=1742]2023-08-23 05:28:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:28:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 101 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 101 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 33.33it/s]\u001b[A\n",
            "epoch 101 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 33.58it/s]\u001b[A\n",
            "epoch 101 | valid on 'valid' subset:  48% 13/27 [00:00<00:00, 40.16it/s]\u001b[A\n",
            "epoch 101 | valid on 'valid' subset:  67% 18/27 [00:00<00:00, 35.09it/s]\u001b[A\n",
            "epoch 101 | valid on 'valid' subset:  93% 25/27 [00:00<00:00, 45.11it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:28:28 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 7.373 | nll_loss 6.286 | ppl 78.02 | wps 103493 | wpb 2258.6 | bsz 74.1 | num_updates 3227 | best_loss 7.373\n",
            "2023-08-23 05:28:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 3227 updates\n",
            "2023-08-23 05:28:28 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint101.pt\n",
            "2023-08-23 05:28:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint101.pt\n",
            "2023-08-23 05:28:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint101.pt (epoch 101 @ 3227 updates, score 7.373) (writing took 0.19219064900016747 seconds)\n",
            "2023-08-23 05:28:28 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)\n",
            "2023-08-23 05:28:28 | INFO | train | epoch 101 | loss 6.305 | nll_loss 5.161 | ppl 35.77 | wps 87237.8 | ups 1.66 | wpb 52622.6 | bsz 1776 | num_updates 3227 | lr 0.000556673 | gnorm 0.286 | loss_scale 16 | train_wall 16 | gb_free 14.3 | wall 1757\n",
            "2023-08-23 05:28:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:28:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 102:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:28:28 | INFO | fairseq.trainer | begin training epoch 102\n",
            "2023-08-23 05:28:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 102:  97% 31/32 [00:16<00:00,  1.65it/s]2023-08-23 05:28:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:28:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 102 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 102 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 38.71it/s]\u001b[A\n",
            "epoch 102 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 43.93it/s]\u001b[A\n",
            "epoch 102 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 42.51it/s]\u001b[A\n",
            "epoch 102 | valid on 'valid' subset:  85% 23/27 [00:00<00:00, 57.99it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:28:46 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 7.397 | nll_loss 6.314 | ppl 79.57 | wps 131444 | wpb 2258.6 | bsz 74.1 | num_updates 3259 | best_loss 7.373\n",
            "2023-08-23 05:28:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 3259 updates\n",
            "2023-08-23 05:28:46 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint102.pt\n",
            "2023-08-23 05:28:46 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint102.pt\n",
            "2023-08-23 05:28:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint102.pt (epoch 102 @ 3259 updates, score 7.397) (writing took 0.09056687300017074 seconds)\n",
            "2023-08-23 05:28:46 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)\n",
            "2023-08-23 05:28:46 | INFO | train | epoch 102 | loss 6.294 | nll_loss 5.148 | ppl 35.47 | wps 96025 | ups 1.82 | wpb 52622.6 | bsz 1776 | num_updates 3259 | lr 0.000553934 | gnorm 0.276 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1774\n",
            "2023-08-23 05:28:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:28:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 103:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:28:46 | INFO | fairseq.trainer | begin training epoch 103\n",
            "2023-08-23 05:28:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 103:  97% 31/32 [00:16<00:00,  2.05it/s]2023-08-23 05:29:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:29:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 103 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 103 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 29.24it/s]\u001b[A\n",
            "epoch 103 | valid on 'valid' subset:  26% 7/27 [00:00<00:00, 33.09it/s]\u001b[A\n",
            "epoch 103 | valid on 'valid' subset:  44% 12/27 [00:00<00:00, 39.99it/s]\u001b[A\n",
            "epoch 103 | valid on 'valid' subset:  70% 19/27 [00:00<00:00, 50.43it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:29:03 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 7.368 | nll_loss 6.281 | ppl 77.77 | wps 125742 | wpb 2258.6 | bsz 74.1 | num_updates 3291 | best_loss 7.368\n",
            "2023-08-23 05:29:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 3291 updates\n",
            "2023-08-23 05:29:03 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint103.pt\n",
            "2023-08-23 05:29:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint103.pt\n",
            "2023-08-23 05:29:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint103.pt (epoch 103 @ 3291 updates, score 7.368) (writing took 0.13628042699974685 seconds)\n",
            "2023-08-23 05:29:03 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)\n",
            "2023-08-23 05:29:03 | INFO | train | epoch 103 | loss 6.287 | nll_loss 5.14 | ppl 35.25 | wps 98173.4 | ups 1.87 | wpb 52622.6 | bsz 1776 | num_updates 3291 | lr 0.000551234 | gnorm 0.272 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1791\n",
            "2023-08-23 05:29:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:29:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 104:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:29:03 | INFO | fairseq.trainer | begin training epoch 104\n",
            "2023-08-23 05:29:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 104:  97% 31/32 [00:16<00:00,  2.15it/s, loss=6.295, nll_loss=5.149, ppl=35.49, wps=97453.6, ups=1.85, wpb=52699.8, bsz=1774.3, num_updates=3300, lr=0.000550482, gnorm=0.274, loss_scale=16, train_wall=46, gb_free=14.3, wall=1796]2023-08-23 05:29:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:29:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 104 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 104 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 27.93it/s]\u001b[A\n",
            "epoch 104 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 41.39it/s]\u001b[A\n",
            "epoch 104 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 47.82it/s]\u001b[A\n",
            "epoch 104 | valid on 'valid' subset: 100% 27/27 [00:00<00:00, 67.99it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:29:20 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 7.359 | nll_loss 6.269 | ppl 77.13 | wps 136113 | wpb 2258.6 | bsz 74.1 | num_updates 3323 | best_loss 7.359\n",
            "2023-08-23 05:29:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 3323 updates\n",
            "2023-08-23 05:29:20 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint104.pt\n",
            "2023-08-23 05:29:20 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint104.pt\n",
            "2023-08-23 05:29:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint104.pt (epoch 104 @ 3323 updates, score 7.359) (writing took 0.13844450999977198 seconds)\n",
            "2023-08-23 05:29:20 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)\n",
            "2023-08-23 05:29:20 | INFO | train | epoch 104 | loss 6.275 | nll_loss 5.126 | ppl 34.91 | wps 98348.6 | ups 1.87 | wpb 52622.6 | bsz 1776 | num_updates 3323 | lr 0.000548574 | gnorm 0.272 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1808\n",
            "2023-08-23 05:29:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:29:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 105:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:29:20 | INFO | fairseq.trainer | begin training epoch 105\n",
            "2023-08-23 05:29:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 105:  97% 31/32 [00:16<00:00,  2.19it/s]2023-08-23 05:29:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:29:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 105 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 105 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 40.16it/s]\u001b[A\n",
            "epoch 105 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 40.61it/s]\u001b[A\n",
            "epoch 105 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 41.45it/s]\u001b[A\n",
            "epoch 105 | valid on 'valid' subset:  89% 24/27 [00:00<00:00, 57.96it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:29:37 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 7.364 | nll_loss 6.274 | ppl 77.39 | wps 128987 | wpb 2258.6 | bsz 74.1 | num_updates 3355 | best_loss 7.359\n",
            "2023-08-23 05:29:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 3355 updates\n",
            "2023-08-23 05:29:37 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint105.pt\n",
            "2023-08-23 05:29:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint105.pt\n",
            "2023-08-23 05:29:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint105.pt (epoch 105 @ 3355 updates, score 7.364) (writing took 0.09031665600014094 seconds)\n",
            "2023-08-23 05:29:37 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)\n",
            "2023-08-23 05:29:37 | INFO | train | epoch 105 | loss 6.268 | nll_loss 5.117 | ppl 34.71 | wps 99202.2 | ups 1.89 | wpb 52622.6 | bsz 1776 | num_updates 3355 | lr 0.000545951 | gnorm 0.277 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1825\n",
            "2023-08-23 05:29:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:29:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 106:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:29:37 | INFO | fairseq.trainer | begin training epoch 106\n",
            "2023-08-23 05:29:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 106:  97% 31/32 [00:16<00:00,  2.12it/s]2023-08-23 05:29:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:29:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 106 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 106 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 27.05it/s]\u001b[A\n",
            "epoch 106 | valid on 'valid' subset:  26% 7/27 [00:00<00:00, 31.46it/s]\u001b[A\n",
            "epoch 106 | valid on 'valid' subset:  41% 11/27 [00:00<00:00, 31.81it/s]\u001b[A\n",
            "epoch 106 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 32.46it/s]\u001b[A\n",
            "epoch 106 | valid on 'valid' subset:  78% 21/27 [00:00<00:00, 40.70it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:29:54 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 7.366 | nll_loss 6.273 | ppl 77.31 | wps 99136.5 | wpb 2258.6 | bsz 74.1 | num_updates 3387 | best_loss 7.359\n",
            "2023-08-23 05:29:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 3387 updates\n",
            "2023-08-23 05:29:54 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint106.pt\n",
            "2023-08-23 05:29:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint106.pt\n",
            "2023-08-23 05:29:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint106.pt (epoch 106 @ 3387 updates, score 7.366) (writing took 0.12903921400038598 seconds)\n",
            "2023-08-23 05:29:55 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)\n",
            "2023-08-23 05:29:55 | INFO | train | epoch 106 | loss 6.26 | nll_loss 5.108 | ppl 34.49 | wps 96948.3 | ups 1.84 | wpb 52622.6 | bsz 1776 | num_updates 3387 | lr 0.000543366 | gnorm 0.268 | loss_scale 16 | train_wall 15 | gb_free 14.3 | wall 1843\n",
            "2023-08-23 05:29:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:29:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 107:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:29:55 | INFO | fairseq.trainer | begin training epoch 107\n",
            "2023-08-23 05:29:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 107:  97% 31/32 [00:16<00:00,  1.71it/s, loss=6.263, nll_loss=5.112, ppl=34.58, wps=96878.4, ups=1.84, wpb=52634.2, bsz=1778.5, num_updates=3400, lr=0.000542326, gnorm=0.273, loss_scale=16, train_wall=46, gb_free=14.3, wall=1850]2023-08-23 05:30:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:30:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 107 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 107 | valid on 'valid' subset:  11% 3/27 [00:00<00:01, 22.23it/s]\u001b[A\n",
            "epoch 107 | valid on 'valid' subset:  26% 7/27 [00:00<00:00, 29.25it/s]\u001b[A\n",
            "epoch 107 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 28.41it/s]\u001b[A\n",
            "epoch 107 | valid on 'valid' subset:  48% 13/27 [00:00<00:00, 28.40it/s]\u001b[A\n",
            "epoch 107 | valid on 'valid' subset:  63% 17/27 [00:00<00:00, 31.60it/s]\u001b[A\n",
            "epoch 107 | valid on 'valid' subset:  93% 25/27 [00:00<00:00, 45.76it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:30:12 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 7.35 | nll_loss 6.255 | ppl 76.38 | wps 91483.8 | wpb 2258.6 | bsz 74.1 | num_updates 3419 | best_loss 7.35\n",
            "2023-08-23 05:30:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 3419 updates\n",
            "2023-08-23 05:30:12 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint107.pt\n",
            "2023-08-23 05:30:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint107.pt\n",
            "2023-08-23 05:30:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint107.pt (epoch 107 @ 3419 updates, score 7.35) (writing took 0.19025060400008442 seconds)\n",
            "2023-08-23 05:30:12 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)\n",
            "2023-08-23 05:30:12 | INFO | train | epoch 107 | loss 6.249 | nll_loss 5.096 | ppl 34.2 | wps 94555.4 | ups 1.8 | wpb 52622.6 | bsz 1776 | num_updates 3419 | lr 0.000540817 | gnorm 0.25 | loss_scale 32 | train_wall 15 | gb_free 14.3 | wall 1861\n",
            "2023-08-23 05:30:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:30:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 108:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:30:12 | INFO | fairseq.trainer | begin training epoch 108\n",
            "2023-08-23 05:30:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 108:  97% 31/32 [00:16<00:00,  1.97it/s]2023-08-23 05:30:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:30:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 108 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 108 | valid on 'valid' subset:  19% 5/27 [00:00<00:00, 41.56it/s]\u001b[A\n",
            "epoch 108 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 42.28it/s]\u001b[A\n",
            "epoch 108 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 40.33it/s]\u001b[A\n",
            "epoch 108 | valid on 'valid' subset:  93% 25/27 [00:00<00:00, 59.56it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:30:29 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 7.33 | nll_loss 6.236 | ppl 75.39 | wps 129313 | wpb 2258.6 | bsz 74.1 | num_updates 3451 | best_loss 7.33\n",
            "2023-08-23 05:30:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 3451 updates\n",
            "2023-08-23 05:30:29 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint108.pt\n",
            "2023-08-23 05:30:29 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint108.pt\n",
            "2023-08-23 05:30:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint108.pt (epoch 108 @ 3451 updates, score 7.33) (writing took 0.13689343099986218 seconds)\n",
            "2023-08-23 05:30:29 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)\n",
            "2023-08-23 05:30:29 | INFO | train | epoch 108 | loss 6.242 | nll_loss 5.087 | ppl 33.99 | wps 98430.3 | ups 1.87 | wpb 52622.6 | bsz 1776 | num_updates 3451 | lr 0.000538304 | gnorm 0.263 | loss_scale 32 | train_wall 15 | gb_free 14.3 | wall 1878\n",
            "2023-08-23 05:30:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:30:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 109:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:30:29 | INFO | fairseq.trainer | begin training epoch 109\n",
            "2023-08-23 05:30:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 109:  97% 31/32 [00:16<00:00,  2.14it/s]2023-08-23 05:30:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:30:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 109 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 109 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 28.47it/s]\u001b[A\n",
            "epoch 109 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 40.69it/s]\u001b[A\n",
            "epoch 109 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 46.14it/s]\u001b[A\n",
            "epoch 109 | valid on 'valid' subset:  85% 23/27 [00:00<00:00, 62.23it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:30:46 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 7.344 | nll_loss 6.249 | ppl 76.04 | wps 137966 | wpb 2258.6 | bsz 74.1 | num_updates 3483 | best_loss 7.33\n",
            "2023-08-23 05:30:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 3483 updates\n",
            "2023-08-23 05:30:46 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint109.pt\n",
            "2023-08-23 05:30:46 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint109.pt\n",
            "2023-08-23 05:30:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint109.pt (epoch 109 @ 3483 updates, score 7.344) (writing took 0.08242546299970854 seconds)\n",
            "2023-08-23 05:30:46 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)\n",
            "2023-08-23 05:30:46 | INFO | train | epoch 109 | loss 6.235 | nll_loss 5.079 | ppl 33.79 | wps 99799.8 | ups 1.9 | wpb 52622.6 | bsz 1776 | num_updates 3483 | lr 0.000535825 | gnorm 0.263 | loss_scale 32 | train_wall 14 | gb_free 14.3 | wall 1895\n",
            "2023-08-23 05:30:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:30:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 110:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:30:46 | INFO | fairseq.trainer | begin training epoch 110\n",
            "2023-08-23 05:30:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 110:  97% 31/32 [00:16<00:00,  2.20it/s, loss=6.239, nll_loss=5.084, ppl=33.91, wps=98348.7, ups=1.87, wpb=52707.8, bsz=1781.4, num_updates=3500, lr=0.000534522, gnorm=0.26, loss_scale=32, train_wall=46, gb_free=14.3, wall=1904]2023-08-23 05:31:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:31:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 110 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 110 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 38.46it/s]\u001b[A\n",
            "epoch 110 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 36.33it/s]\u001b[A\n",
            "epoch 110 | valid on 'valid' subset:  48% 13/27 [00:00<00:00, 39.80it/s]\u001b[A\n",
            "epoch 110 | valid on 'valid' subset:  70% 19/27 [00:00<00:00, 47.07it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:31:03 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 7.35 | nll_loss 6.25 | ppl 76.11 | wps 124148 | wpb 2258.6 | bsz 74.1 | num_updates 3515 | best_loss 7.33\n",
            "2023-08-23 05:31:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 3515 updates\n",
            "2023-08-23 05:31:03 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint110.pt\n",
            "2023-08-23 05:31:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint110.pt\n",
            "2023-08-23 05:31:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint110.pt (epoch 110 @ 3515 updates, score 7.35) (writing took 0.08675581199986482 seconds)\n",
            "2023-08-23 05:31:03 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)\n",
            "2023-08-23 05:31:03 | INFO | train | epoch 110 | loss 6.227 | nll_loss 5.07 | ppl 33.59 | wps 100100 | ups 1.9 | wpb 52622.6 | bsz 1776 | num_updates 3515 | lr 0.000533381 | gnorm 0.277 | loss_scale 32 | train_wall 14 | gb_free 14.3 | wall 1911\n",
            "2023-08-23 05:31:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:31:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 111:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:31:03 | INFO | fairseq.trainer | begin training epoch 111\n",
            "2023-08-23 05:31:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 111:  97% 31/32 [00:16<00:00,  2.14it/s]2023-08-23 05:31:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:31:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 111 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 111 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 29.95it/s]\u001b[A\n",
            "epoch 111 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 43.41it/s]\u001b[A\n",
            "epoch 111 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 41.25it/s]\u001b[A\n",
            "epoch 111 | valid on 'valid' subset:  89% 24/27 [00:00<00:00, 58.08it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:31:20 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 7.339 | nll_loss 6.243 | ppl 75.73 | wps 127946 | wpb 2258.6 | bsz 74.1 | num_updates 3547 | best_loss 7.33\n",
            "2023-08-23 05:31:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 3547 updates\n",
            "2023-08-23 05:31:20 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint111.pt\n",
            "2023-08-23 05:31:20 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint111.pt\n",
            "2023-08-23 05:31:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint111.pt (epoch 111 @ 3547 updates, score 7.339) (writing took 0.0867718680001417 seconds)\n",
            "2023-08-23 05:31:20 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)\n",
            "2023-08-23 05:31:20 | INFO | train | epoch 111 | loss 6.219 | nll_loss 5.061 | ppl 33.38 | wps 99639.9 | ups 1.89 | wpb 52622.6 | bsz 1776 | num_updates 3547 | lr 0.000530969 | gnorm 0.288 | loss_scale 32 | train_wall 15 | gb_free 14.3 | wall 1928\n",
            "2023-08-23 05:31:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:31:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 112:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:31:20 | INFO | fairseq.trainer | begin training epoch 112\n",
            "2023-08-23 05:31:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 112:  97% 31/32 [00:16<00:00,  1.96it/s]2023-08-23 05:31:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:31:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 112 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 112 | valid on 'valid' subset:   7% 2/27 [00:00<00:01, 14.42it/s]\u001b[A\n",
            "epoch 112 | valid on 'valid' subset:  22% 6/27 [00:00<00:00, 24.65it/s]\u001b[A\n",
            "epoch 112 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 27.33it/s]\u001b[A\n",
            "epoch 112 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 29.10it/s]\u001b[A\n",
            "epoch 112 | valid on 'valid' subset:  63% 17/27 [00:00<00:00, 29.22it/s]\u001b[A\n",
            "epoch 112 | valid on 'valid' subset:  89% 24/27 [00:00<00:00, 40.57it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:31:37 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 7.34 | nll_loss 6.242 | ppl 75.67 | wps 86790.9 | wpb 2258.6 | bsz 74.1 | num_updates 3579 | best_loss 7.33\n",
            "2023-08-23 05:31:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 3579 updates\n",
            "2023-08-23 05:31:37 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint112.pt\n",
            "2023-08-23 05:31:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint112.pt\n",
            "2023-08-23 05:31:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint112.pt (epoch 112 @ 3579 updates, score 7.34) (writing took 0.1274339130000044 seconds)\n",
            "2023-08-23 05:31:38 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)\n",
            "2023-08-23 05:31:38 | INFO | train | epoch 112 | loss 6.213 | nll_loss 5.053 | ppl 33.2 | wps 96321.3 | ups 1.83 | wpb 52622.6 | bsz 1776 | num_updates 3579 | lr 0.00052859 | gnorm 0.319 | loss_scale 32 | train_wall 15 | gb_free 14.3 | wall 1946\n",
            "2023-08-23 05:31:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:31:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 113:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:31:38 | INFO | fairseq.trainer | begin training epoch 113\n",
            "2023-08-23 05:31:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 113:  97% 31/32 [00:18<00:00,  1.84it/s, loss=6.214, nll_loss=5.055, ppl=33.23, wps=95468.4, ups=1.82, wpb=52523.9, bsz=1778.2, num_updates=3600, lr=0.000527046, gnorm=0.291, loss_scale=32, train_wall=47, gb_free=14.3, wall=1959]2023-08-23 05:31:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:31:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 113 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 113 | valid on 'valid' subset:   7% 2/27 [00:00<00:01, 18.37it/s]\u001b[A\n",
            "epoch 113 | valid on 'valid' subset:  22% 6/27 [00:00<00:00, 27.27it/s]\u001b[A\n",
            "epoch 113 | valid on 'valid' subset:  44% 12/27 [00:00<00:00, 37.14it/s]\u001b[A\n",
            "epoch 113 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 34.66it/s]\u001b[A\n",
            "epoch 113 | valid on 'valid' subset:  78% 21/27 [00:00<00:00, 39.45it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:31:57 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 7.315 | nll_loss 6.214 | ppl 74.23 | wps 94411.5 | wpb 2258.6 | bsz 74.1 | num_updates 3611 | best_loss 7.315\n",
            "2023-08-23 05:31:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 3611 updates\n",
            "2023-08-23 05:31:57 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint113.pt\n",
            "2023-08-23 05:31:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint113.pt\n",
            "2023-08-23 05:31:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint113.pt (epoch 113 @ 3611 updates, score 7.315) (writing took 0.19248515200024485 seconds)\n",
            "2023-08-23 05:31:57 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)\n",
            "2023-08-23 05:31:57 | INFO | train | epoch 113 | loss 6.203 | nll_loss 5.042 | ppl 32.94 | wps 87810 | ups 1.67 | wpb 52622.6 | bsz 1776 | num_updates 3611 | lr 0.000526243 | gnorm 0.252 | loss_scale 32 | train_wall 16 | gb_free 14.3 | wall 1965\n",
            "2023-08-23 05:31:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:31:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 114:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:31:57 | INFO | fairseq.trainer | begin training epoch 114\n",
            "2023-08-23 05:31:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 114:  97% 31/32 [00:17<00:00,  1.61it/s]2023-08-23 05:32:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:32:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 114 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 114 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 39.10it/s]\u001b[A\n",
            "epoch 114 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 41.46it/s]\u001b[A\n",
            "epoch 114 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 43.05it/s]\u001b[A\n",
            "epoch 114 | valid on 'valid' subset:  81% 22/27 [00:00<00:00, 55.45it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:32:14 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 7.324 | nll_loss 6.224 | ppl 74.75 | wps 130006 | wpb 2258.6 | bsz 74.1 | num_updates 3643 | best_loss 7.315\n",
            "2023-08-23 05:32:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 3643 updates\n",
            "2023-08-23 05:32:14 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint114.pt\n",
            "2023-08-23 05:32:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint114.pt\n",
            "2023-08-23 05:32:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint114.pt (epoch 114 @ 3643 updates, score 7.324) (writing took 0.08959883199986507 seconds)\n",
            "2023-08-23 05:32:15 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)\n",
            "2023-08-23 05:32:15 | INFO | train | epoch 114 | loss 6.196 | nll_loss 5.034 | ppl 32.77 | wps 94638.8 | ups 1.8 | wpb 52622.6 | bsz 1776 | num_updates 3643 | lr 0.000523927 | gnorm 0.286 | loss_scale 32 | train_wall 15 | gb_free 14.3 | wall 1983\n",
            "2023-08-23 05:32:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:32:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 115:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:32:15 | INFO | fairseq.trainer | begin training epoch 115\n",
            "2023-08-23 05:32:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 115:  97% 31/32 [00:16<00:00,  1.95it/s]2023-08-23 05:32:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:32:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 115 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 115 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 38.49it/s]\u001b[A\n",
            "epoch 115 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 40.55it/s]\u001b[A\n",
            "epoch 115 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 44.54it/s]\u001b[A\n",
            "epoch 115 | valid on 'valid' subset:  81% 22/27 [00:00<00:00, 56.89it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:32:32 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 7.318 | nll_loss 6.214 | ppl 74.22 | wps 130900 | wpb 2258.6 | bsz 74.1 | num_updates 3675 | best_loss 7.315\n",
            "2023-08-23 05:32:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 3675 updates\n",
            "2023-08-23 05:32:32 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint115.pt\n",
            "2023-08-23 05:32:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint115.pt\n",
            "2023-08-23 05:32:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint115.pt (epoch 115 @ 3675 updates, score 7.318) (writing took 0.08713598999975147 seconds)\n",
            "2023-08-23 05:32:32 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)\n",
            "2023-08-23 05:32:32 | INFO | train | epoch 115 | loss 6.189 | nll_loss 5.026 | ppl 32.58 | wps 98396 | ups 1.87 | wpb 52622.6 | bsz 1776 | num_updates 3675 | lr 0.000521641 | gnorm 0.273 | loss_scale 32 | train_wall 15 | gb_free 14.3 | wall 2000\n",
            "2023-08-23 05:32:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:32:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 116:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:32:32 | INFO | fairseq.trainer | begin training epoch 116\n",
            "2023-08-23 05:32:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 116:  97% 31/32 [00:17<00:00,  2.08it/s, loss=6.19, nll_loss=5.026, ppl=32.59, wps=95124.8, ups=1.8, wpb=52778.2, bsz=1779.2, num_updates=3700, lr=0.000519875, gnorm=0.264, loss_scale=32, train_wall=48, gb_free=14.3, wall=2014]2023-08-23 05:32:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:32:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 116 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 116 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 33.57it/s]\u001b[A\n",
            "epoch 116 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 35.15it/s]\u001b[A\n",
            "epoch 116 | valid on 'valid' subset:  48% 13/27 [00:00<00:00, 37.98it/s]\u001b[A\n",
            "epoch 116 | valid on 'valid' subset:  78% 21/27 [00:00<00:00, 51.06it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:32:49 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 7.305 | nll_loss 6.201 | ppl 73.57 | wps 120919 | wpb 2258.6 | bsz 74.1 | num_updates 3707 | best_loss 7.305\n",
            "2023-08-23 05:32:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 3707 updates\n",
            "2023-08-23 05:32:49 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint116.pt\n",
            "2023-08-23 05:32:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint116.pt\n",
            "2023-08-23 05:32:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint116.pt (epoch 116 @ 3707 updates, score 7.305) (writing took 0.1228164970002581 seconds)\n",
            "2023-08-23 05:32:50 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)\n",
            "2023-08-23 05:32:50 | INFO | train | epoch 116 | loss 6.182 | nll_loss 5.017 | ppl 32.39 | wps 93503.8 | ups 1.78 | wpb 52622.6 | bsz 1776 | num_updates 3707 | lr 0.000519384 | gnorm 0.237 | loss_scale 32 | train_wall 16 | gb_free 14.3 | wall 2018\n",
            "2023-08-23 05:32:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:32:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 117:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:32:50 | INFO | fairseq.trainer | begin training epoch 117\n",
            "2023-08-23 05:32:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 117:  97% 31/32 [00:16<00:00,  2.11it/s]2023-08-23 05:33:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:33:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 117 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 117 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 36.04it/s]\u001b[A\n",
            "epoch 117 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 43.43it/s]\u001b[A\n",
            "epoch 117 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 42.61it/s]\u001b[A\n",
            "epoch 117 | valid on 'valid' subset:  81% 22/27 [00:00<00:00, 51.40it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:33:07 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 7.293 | nll_loss 6.187 | ppl 72.87 | wps 124891 | wpb 2258.6 | bsz 74.1 | num_updates 3739 | best_loss 7.293\n",
            "2023-08-23 05:33:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 3739 updates\n",
            "2023-08-23 05:33:07 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint117.pt\n",
            "2023-08-23 05:33:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint117.pt\n",
            "2023-08-23 05:33:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint117.pt (epoch 117 @ 3739 updates, score 7.293) (writing took 0.1235613049998392 seconds)\n",
            "2023-08-23 05:33:07 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)\n",
            "2023-08-23 05:33:07 | INFO | train | epoch 117 | loss 6.174 | nll_loss 5.008 | ppl 32.18 | wps 94622.7 | ups 1.8 | wpb 52622.6 | bsz 1776 | num_updates 3739 | lr 0.000517157 | gnorm 0.252 | loss_scale 32 | train_wall 15 | gb_free 14.3 | wall 2036\n",
            "2023-08-23 05:33:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:33:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 118:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:33:07 | INFO | fairseq.trainer | begin training epoch 118\n",
            "2023-08-23 05:33:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 118:  97% 31/32 [00:18<00:00,  1.66it/s]2023-08-23 05:33:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:33:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 118 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 118 | valid on 'valid' subset:  11% 3/27 [00:00<00:01, 22.01it/s]\u001b[A\n",
            "epoch 118 | valid on 'valid' subset:  22% 6/27 [00:00<00:00, 23.94it/s]\u001b[A\n",
            "epoch 118 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 28.44it/s]\u001b[A\n",
            "epoch 118 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 30.47it/s]\u001b[A\n",
            "epoch 118 | valid on 'valid' subset:  67% 18/27 [00:00<00:00, 29.51it/s]\u001b[A\n",
            "epoch 118 | valid on 'valid' subset:  93% 25/27 [00:00<00:00, 40.39it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:33:27 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 7.301 | nll_loss 6.196 | ppl 73.3 | wps 82295.4 | wpb 2258.6 | bsz 74.1 | num_updates 3771 | best_loss 7.293\n",
            "2023-08-23 05:33:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 3771 updates\n",
            "2023-08-23 05:33:27 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint118.pt\n",
            "2023-08-23 05:33:27 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint118.pt\n",
            "2023-08-23 05:33:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint118.pt (epoch 118 @ 3771 updates, score 7.301) (writing took 0.139153947000068 seconds)\n",
            "2023-08-23 05:33:27 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)\n",
            "2023-08-23 05:33:27 | INFO | train | epoch 118 | loss 6.168 | nll_loss 5.001 | ppl 32.02 | wps 87161.4 | ups 1.66 | wpb 52622.6 | bsz 1776 | num_updates 3771 | lr 0.000514958 | gnorm 0.259 | loss_scale 32 | train_wall 16 | gb_free 14.3 | wall 2055\n",
            "2023-08-23 05:33:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:33:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 119:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:33:27 | INFO | fairseq.trainer | begin training epoch 119\n",
            "2023-08-23 05:33:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 119:  97% 31/32 [00:17<00:00,  1.73it/s, loss=6.173, nll_loss=5.006, ppl=32.13, wps=92410.6, ups=1.75, wpb=52669.6, bsz=1771.6, num_updates=3800, lr=0.000512989, gnorm=0.256, loss_scale=32, train_wall=49, gb_free=14.3, wall=2071]2023-08-23 05:33:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:33:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 119 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 119 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 38.96it/s]\u001b[A\n",
            "epoch 119 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 37.77it/s]\u001b[A\n",
            "epoch 119 | valid on 'valid' subset:  44% 12/27 [00:00<00:00, 35.35it/s]\u001b[A\n",
            "epoch 119 | valid on 'valid' subset:  63% 17/27 [00:00<00:00, 40.08it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:33:45 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 7.287 | nll_loss 6.18 | ppl 72.51 | wps 119476 | wpb 2258.6 | bsz 74.1 | num_updates 3803 | best_loss 7.287\n",
            "2023-08-23 05:33:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 3803 updates\n",
            "2023-08-23 05:33:45 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint119.pt\n",
            "2023-08-23 05:33:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint119.pt\n",
            "2023-08-23 05:33:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint119.pt (epoch 119 @ 3803 updates, score 7.287) (writing took 0.13382487699982448 seconds)\n",
            "2023-08-23 05:33:45 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)\n",
            "2023-08-23 05:33:45 | INFO | train | epoch 119 | loss 6.161 | nll_loss 4.993 | ppl 31.85 | wps 93308.5 | ups 1.77 | wpb 52622.6 | bsz 1776 | num_updates 3803 | lr 0.000512787 | gnorm 0.267 | loss_scale 32 | train_wall 15 | gb_free 14.3 | wall 2073\n",
            "2023-08-23 05:33:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:33:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 120:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:33:45 | INFO | fairseq.trainer | begin training epoch 120\n",
            "2023-08-23 05:33:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 120:  97% 31/32 [00:16<00:00,  2.02it/s]2023-08-23 05:34:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:34:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 120 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 120 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 38.11it/s]\u001b[A\n",
            "epoch 120 | valid on 'valid' subset:  30% 8/27 [00:00<00:00, 35.81it/s]\u001b[A\n",
            "epoch 120 | valid on 'valid' subset:  44% 12/27 [00:00<00:00, 37.20it/s]\u001b[A\n",
            "epoch 120 | valid on 'valid' subset:  63% 17/27 [00:00<00:00, 40.42it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:34:02 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 7.283 | nll_loss 6.172 | ppl 72.13 | wps 120896 | wpb 2258.6 | bsz 74.1 | num_updates 3835 | best_loss 7.283\n",
            "2023-08-23 05:34:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 3835 updates\n",
            "2023-08-23 05:34:02 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint120.pt\n",
            "2023-08-23 05:34:02 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint120.pt\n",
            "2023-08-23 05:34:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint120.pt (epoch 120 @ 3835 updates, score 7.283) (writing took 0.13257617099998242 seconds)\n",
            "2023-08-23 05:34:02 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)\n",
            "2023-08-23 05:34:02 | INFO | train | epoch 120 | loss 6.155 | nll_loss 4.986 | ppl 31.69 | wps 95762.2 | ups 1.82 | wpb 52622.6 | bsz 1776 | num_updates 3835 | lr 0.000510643 | gnorm 0.272 | loss_scale 32 | train_wall 15 | gb_free 14.3 | wall 2091\n",
            "2023-08-23 05:34:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:34:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 121:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:34:02 | INFO | fairseq.trainer | begin training epoch 121\n",
            "2023-08-23 05:34:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 121:  97% 31/32 [00:16<00:00,  2.14it/s]2023-08-23 05:34:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:34:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 121 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 121 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 39.53it/s]\u001b[A\n",
            "epoch 121 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 42.85it/s]\u001b[A\n",
            "epoch 121 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 39.59it/s]\u001b[A\n",
            "epoch 121 | valid on 'valid' subset:  81% 22/27 [00:00<00:00, 53.20it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:34:20 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 7.287 | nll_loss 6.178 | ppl 72.39 | wps 126305 | wpb 2258.6 | bsz 74.1 | num_updates 3867 | best_loss 7.283\n",
            "2023-08-23 05:34:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 3867 updates\n",
            "2023-08-23 05:34:20 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint121.pt\n",
            "2023-08-23 05:34:20 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint121.pt\n",
            "2023-08-23 05:34:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint121.pt (epoch 121 @ 3867 updates, score 7.287) (writing took 0.09823769099966739 seconds)\n",
            "2023-08-23 05:34:20 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)\n",
            "2023-08-23 05:34:20 | INFO | train | epoch 121 | loss 6.147 | nll_loss 4.976 | ppl 31.46 | wps 95009.9 | ups 1.81 | wpb 52622.6 | bsz 1776 | num_updates 3867 | lr 0.000508526 | gnorm 0.248 | loss_scale 32 | train_wall 15 | gb_free 14.3 | wall 2108\n",
            "2023-08-23 05:34:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:34:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 122:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:34:20 | INFO | fairseq.trainer | begin training epoch 122\n",
            "2023-08-23 05:34:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 122:  97% 31/32 [00:16<00:00,  2.12it/s]2023-08-23 05:34:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:34:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 122 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 122 | valid on 'valid' subset:  11% 3/27 [00:00<00:00, 28.26it/s]\u001b[A\n",
            "epoch 122 | valid on 'valid' subset:  22% 6/27 [00:00<00:00, 27.11it/s]\u001b[A\n",
            "epoch 122 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 31.89it/s]\u001b[A\n",
            "epoch 122 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 29.82it/s]\u001b[A\n",
            "epoch 122 | valid on 'valid' subset:  67% 18/27 [00:00<00:00, 31.44it/s]\u001b[A\n",
            "epoch 122 | valid on 'valid' subset:  96% 26/27 [00:00<00:00, 45.45it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:34:38 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 7.279 | nll_loss 6.168 | ppl 71.88 | wps 89439.5 | wpb 2258.6 | bsz 74.1 | num_updates 3899 | best_loss 7.279\n",
            "2023-08-23 05:34:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 3899 updates\n",
            "2023-08-23 05:34:38 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint122.pt\n",
            "2023-08-23 05:34:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint122.pt\n",
            "2023-08-23 05:34:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint122.pt (epoch 122 @ 3899 updates, score 7.279) (writing took 0.230185180999797 seconds)\n",
            "2023-08-23 05:34:38 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)\n",
            "2023-08-23 05:34:38 | INFO | train | epoch 122 | loss 6.143 | nll_loss 4.971 | ppl 31.37 | wps 93711.4 | ups 1.78 | wpb 52622.6 | bsz 1776 | num_updates 3899 | lr 0.000506435 | gnorm 0.288 | loss_scale 32 | train_wall 15 | gb_free 14.3 | wall 2126\n",
            "2023-08-23 05:34:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:34:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 123:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:34:38 | INFO | fairseq.trainer | begin training epoch 123\n",
            "2023-08-23 05:34:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 123:  97% 31/32 [00:17<00:00,  1.67it/s, loss=6.147, nll_loss=4.976, ppl=31.48, wps=93505.5, ups=1.79, wpb=52380.9, bsz=1768.6, num_updates=3900, lr=0.00050637, gnorm=0.271, loss_scale=32, train_wall=47, gb_free=14.3, wall=2127]2023-08-23 05:34:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:34:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 123 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 123 | valid on 'valid' subset:   7% 2/27 [00:00<00:01, 17.65it/s]\u001b[A\n",
            "epoch 123 | valid on 'valid' subset:  22% 6/27 [00:00<00:00, 26.18it/s]\u001b[A\n",
            "epoch 123 | valid on 'valid' subset:  41% 11/27 [00:00<00:00, 31.05it/s]\u001b[A\n",
            "epoch 123 | valid on 'valid' subset:  56% 15/27 [00:00<00:00, 30.60it/s]\u001b[A\n",
            "epoch 123 | valid on 'valid' subset:  78% 21/27 [00:00<00:00, 38.82it/s]\u001b[A\n",
            "epoch 123 | valid on 'valid' subset: 100% 27/27 [00:00<00:00, 44.60it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:34:56 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 7.291 | nll_loss 6.181 | ppl 72.55 | wps 90498.8 | wpb 2258.6 | bsz 74.1 | num_updates 3931 | best_loss 7.279\n",
            "2023-08-23 05:34:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 3931 updates\n",
            "2023-08-23 05:34:56 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint123.pt\n",
            "2023-08-23 05:34:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint123.pt\n",
            "2023-08-23 05:34:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint123.pt (epoch 123 @ 3931 updates, score 7.291) (writing took 0.13696751799989215 seconds)\n",
            "2023-08-23 05:34:56 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)\n",
            "2023-08-23 05:34:56 | INFO | train | epoch 123 | loss 6.133 | nll_loss 4.96 | ppl 31.12 | wps 92657.7 | ups 1.76 | wpb 52622.6 | bsz 1776 | num_updates 3931 | lr 0.000504369 | gnorm 0.26 | loss_scale 32 | train_wall 15 | gb_free 14.3 | wall 2144\n",
            "2023-08-23 05:34:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:34:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 124:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:34:56 | INFO | fairseq.trainer | begin training epoch 124\n",
            "2023-08-23 05:34:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 124:  97% 31/32 [00:17<00:00,  1.41it/s]2023-08-23 05:35:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:35:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 124 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 124 | valid on 'valid' subset:  11% 3/27 [00:00<00:01, 21.38it/s]\u001b[A\n",
            "epoch 124 | valid on 'valid' subset:  22% 6/27 [00:00<00:00, 24.91it/s]\u001b[A\n",
            "epoch 124 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 28.99it/s]\u001b[A\n",
            "epoch 124 | valid on 'valid' subset:  52% 14/27 [00:00<00:00, 30.09it/s]\u001b[A\n",
            "epoch 124 | valid on 'valid' subset:  67% 18/27 [00:00<00:00, 30.11it/s]\u001b[A\n",
            "epoch 124 | valid on 'valid' subset:  93% 25/27 [00:00<00:00, 41.58it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:35:15 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 7.28 | nll_loss 6.171 | ppl 72.06 | wps 84236.8 | wpb 2258.6 | bsz 74.1 | num_updates 3963 | best_loss 7.279\n",
            "2023-08-23 05:35:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 3963 updates\n",
            "2023-08-23 05:35:15 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint124.pt\n",
            "2023-08-23 05:35:15 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint124.pt\n",
            "2023-08-23 05:35:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint124.pt (epoch 124 @ 3963 updates, score 7.28) (writing took 0.1395468699997764 seconds)\n",
            "2023-08-23 05:35:15 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)\n",
            "2023-08-23 05:35:15 | INFO | train | epoch 124 | loss 6.129 | nll_loss 4.955 | ppl 31.02 | wps 89686.1 | ups 1.7 | wpb 52622.6 | bsz 1776 | num_updates 3963 | lr 0.000502329 | gnorm 0.257 | loss_scale 32 | train_wall 16 | gb_free 14.3 | wall 2163\n",
            "2023-08-23 05:35:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:35:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 125:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:35:15 | INFO | fairseq.trainer | begin training epoch 125\n",
            "2023-08-23 05:35:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 125:  97% 31/32 [00:16<00:00,  1.97it/s]2023-08-23 05:35:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:35:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 125 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 125 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 37.60it/s]\u001b[A\n",
            "epoch 125 | valid on 'valid' subset:  37% 10/27 [00:00<00:00, 46.88it/s]\u001b[A\n",
            "epoch 125 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 49.47it/s]\u001b[A\n",
            "epoch 125 | valid on 'valid' subset:  89% 24/27 [00:00<00:00, 60.05it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:35:32 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 7.264 | nll_loss 6.151 | ppl 71.05 | wps 137508 | wpb 2258.6 | bsz 74.1 | num_updates 3995 | best_loss 7.264\n",
            "2023-08-23 05:35:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 3995 updates\n",
            "2023-08-23 05:35:32 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint125.pt\n",
            "2023-08-23 05:35:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint125.pt\n",
            "2023-08-23 05:35:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint125.pt (epoch 125 @ 3995 updates, score 7.264) (writing took 0.13570308500038664 seconds)\n",
            "2023-08-23 05:35:32 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)\n",
            "2023-08-23 05:35:32 | INFO | train | epoch 125 | loss 6.121 | nll_loss 4.946 | ppl 30.83 | wps 99012.5 | ups 1.88 | wpb 52622.6 | bsz 1776 | num_updates 3995 | lr 0.000500313 | gnorm 0.322 | loss_scale 32 | train_wall 15 | gb_free 14.3 | wall 2180\n",
            "2023-08-23 05:35:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-08-23 05:35:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32\n",
            "epoch 126:   0% 0/32 [00:00<?, ?it/s]2023-08-23 05:35:32 | INFO | fairseq.trainer | begin training epoch 126\n",
            "2023-08-23 05:35:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 126:  12% 4/32 [00:02<00:14,  1.92it/s]2023-08-23 05:35:35 | INFO | fairseq_cli.train | Stopping training due to num_updates: 4000 >= max_update: 4000\n",
            "2023-08-23 05:35:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-23 05:35:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 126 | valid on 'valid' subset:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 126 | valid on 'valid' subset:  15% 4/27 [00:00<00:00, 39.58it/s]\u001b[A\n",
            "epoch 126 | valid on 'valid' subset:  33% 9/27 [00:00<00:00, 45.16it/s]\u001b[A\n",
            "epoch 126 | valid on 'valid' subset:  59% 16/27 [00:00<00:00, 48.84it/s]\u001b[A\n",
            "epoch 126 | valid on 'valid' subset:  85% 23/27 [00:00<00:00, 55.09it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-08-23 05:35:35 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 7.278 | nll_loss 6.164 | ppl 71.7 | wps 131480 | wpb 2258.6 | bsz 74.1 | num_updates 4000 | best_loss 7.264\n",
            "2023-08-23 05:35:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 4000 updates\n",
            "2023-08-23 05:35:35 | INFO | fairseq.trainer | Saving checkpoint to /content/trained_models/checkpoint_last.pt\n",
            "2023-08-23 05:35:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/trained_models/checkpoint_last.pt\n",
            "2023-08-23 05:35:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint trained_models/checkpoint_last.pt (epoch 126 @ 4000 updates, score 7.278) (writing took 0.06514570899980754 seconds)\n",
            "2023-08-23 05:35:35 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)\n",
            "2023-08-23 05:35:35 | INFO | train | epoch 126 | loss 6.041 | nll_loss 4.855 | ppl 28.94 | wps 85341.6 | ups 1.57 | wpb 54228.8 | bsz 2024 | num_updates 4000 | lr 0.0005 | gnorm 0.306 | loss_scale 32 | train_wall 2 | gb_free 14.3 | wall 2183\n",
            "2023-08-23 05:35:35 | INFO | fairseq_cli.train | done training in 2182.9 seconds\n"
          ]
        }
      ],
      "source": [
        "! python fairseq/fairseq_cli/train.py --fp16 \\\n",
        "    data-bin/trial \\\n",
        "    --source-lang en --target-lang hi \\\n",
        "    --arch transformer_iwslt_de_en --share-all-embeddings \\\n",
        "    --encoder-embed-dim\t128 --encoder-ffn-embed-dim\t128 \\\n",
        "    --encoder-layers\t2 --encoder-attention-heads\t2 \\\n",
        "    --decoder-embed-dim\t128 --decoder-ffn-embed-dim\t128 \\\n",
        "    --decoder-layers\t2 --decoder-attention-heads\t2 \\\n",
        "    --dropout 0.3 --weight-decay 0.0 \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
        "    --lr 0.01 --lr-scheduler inverse_sqrt --warmup-updates 10 \\\n",
        "    --max-tokens 4096 --update-freq 16 \\\n",
        "    --max-update 4000 \\\n",
        "    --keep-last-epochs\t10 \\\n",
        "    --save-dir trained_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6XcUdq02OGe",
        "outputId": "04c90b49-028c-4d30-b641-d8b86e32d828"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "checkpoint116.pt  checkpoint119.pt  checkpoint122.pt  checkpoint125.pt\n",
            "checkpoint117.pt  checkpoint120.pt  checkpoint123.pt  checkpoint_best.pt\n",
            "checkpoint118.pt  checkpoint121.pt  checkpoint124.pt  checkpoint_last.pt\n"
          ]
        }
      ],
      "source": [
        "ls trained_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G6KuRjp4HOb",
        "outputId": "510e3c01-862c-4c4f-a9f1-f966e41392fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-08-23 05:35:49.112739: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-23 05:35:50.034290: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
            "INFO:fairseq_cli.interactive:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'trained_models/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': None, 'batch_size': 64, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 64, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 10, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': '@@ ', 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 2500, 'input': 'valid.en'}, 'model': None, 'task': {'_name': 'translation', 'data': 'data-bin/trial', 'source_lang': 'en', 'target_lang': 'hi', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "INFO:fairseq.tasks.translation:[en] dictionary: 7776 types\n",
            "INFO:fairseq.tasks.translation:[hi] dictionary: 7776 types\n",
            "INFO:fairseq_cli.interactive:loading model(s) from trained_models/checkpoint_best.pt\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/fairseq/fairseq_cli/interactive.py\", line 317, in <module>\n",
            "    cli_main()\n",
            "  File \"/content/fairseq/fairseq_cli/interactive.py\", line 313, in cli_main\n",
            "    distributed_utils.call_main(convert_namespace_to_omegaconf(args), main)\n",
            "  File \"/content/fairseq/fairseq/distributed/utils.py\", line 404, in call_main\n",
            "    main(cfg, **kwargs)\n",
            "  File \"/content/fairseq/fairseq_cli/interactive.py\", line 191, in main\n",
            "    align_dict = utils.load_align_dict(cfg.generation.replace_unk)\n",
            "  File \"/content/fairseq/fairseq/utils.py\", line 170, in load_align_dict\n",
            "    with open(replace_unk, \"r\") as f:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '@@ '\n"
          ]
        }
      ],
      "source": [
        "! python fairseq/fairseq_cli/interactive.py  data-bin/trial \\\n",
        "    -s en -t hi \\\n",
        "    --distributed-world-size 1  \\\n",
        "    --path trained_models/checkpoint_best.pt \\\n",
        "    --batch-size 64  --buffer-size 2500 --beam 10 --replace-unk \\\n",
        "    --skip-invalid-size-inputs-valid-test \\\n",
        "    --input valid.en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKPJgrmg6UCR",
        "outputId": "7617a526-42e8-4664-8bf4-4aa072ef808d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "others must also have experi@@ mented with initiatives similar to those undertaken by the government .\n",
            "if we tri@@ ed all that har@@ der , then by the 75@@ th year of our independence , we would have car@@ ved a place for our@@ selves am@@ id@@ st the major tourist dest@@ in@@ ations of the world .\n",
            "the aim of this go@@ b@@ ar - dhan scheme is ensuring cleanliness in villages and gener@@ ating wealth and energy by conver@@ ting c@@ att@@ le d@@ un@@ g and sol@@ id agricultural waste into com@@ post and bi@@ o gas .\n",
            "not only this , what re@@ ally sur@@ prised me was the fact that the ath@@ le@@ te , who fin@@ ished four@@ th in this event am@@ ong@@ st div@@ yan@@ g persons and thus mis@@ sed w@@ inning any med@@ al , ac@@ tu@@ ally took less time than the gold med@@ al@@ ist of general categ@@ ory in comple@@ ting the r@@ ace .\n",
            "i have hear@@ d that in c@@ ud@@ d@@ al@@ ore district of tamil nadu , child mar@@ ri@@ age has been ban@@ ned under a special campaign .\n",
            "in the past , the or@@ is@@ s@@ a government especially facilit@@ ated an ash@@ a work@@ er on independence day .\n",
            "these te@@ a workers get their w@@ ages on wee@@ k@@ ly basis .\n",
            "he further states that desp@@ ite both person@@ al@@ ities ha@@ il@@ ing from di@@ verse family back@@ gr@@ oun@@ ds , they en@@ ric@@ h@@ ed the leg@@ acy and the history of jharkhand .\n",
            "and my dear countrymen , you will be happy to learn that the administration and the people together did construc@@ t 10@@ ,000 to@@ ile@@ ts in hundred hours successfully .\n",
            "and bhag@@ at , that 12 - year old bo@@ y in 19@@ 19 , ev@@ ol@@ ved to be the marty@@ r bhag@@ at singh , our dear her@@ o and inspiration .\n",
            "my dear countrymen , while some time ago , we were concerned about a d@@ rought like situation , these days , on the one hand , we are en@@ jo@@ ying the ra@@ ins , but on the other , re@@ ports of flo@@ ods are also coming in .\n",
            "they are very e@@ ag@@ er to do their b@@ it and are just see@@ king an opportunity where they can do their b@@ it .\n",
            "the constitution of india , the du@@ ties of citizens , the rights of citizens and our commitment to democracy - these in a way make republic day also a festival of san@@ sk@@ a@@ ars , which mak@@ es our future generations cogn@@ iz@@ ant of democracy and their democratic responsi@@ bilities , and also im@@ parts to them the culture , mor@@ al values and norms in@@ her@@ ent in our democracy .\n",
            "i was looking into the suggestions received for mann ki baat .\n",
            "my dear countrymen , it is a matter of pride not only for india but for the entire world today , while we are celebr@@ ating gandhi 150 , our 1@@ 30 crore countrymen have ple@@ d@@ ged to be ri@@ d from the men@@ ace of single use pl@@ as@@ tic .\n",
            "it fe@@ els great to see your interest in india ' s history .\n",
            "there exist@@ s a small village called ke@@ sh@@ la in raj@@ n@@ and@@ ga@@ on , chhattisgarh .\n",
            "the entire country welcomed them with open arm@@ s , with high spir@@ its .\n",
            "i initi@@ ally thought if a busin@@ ess@@ man comes , he will defin@@ it@@ ely talk of things of his personal interest .\n",
            "i will add my views on some ex@@ am - related issues about which i feel strongly .\n",
            "it is gener@@ ally beli@@ ev@@ ed that such a big tax reform , in a huge country like ours with such a large population tak@@ es 5 to 7 years for effective adop@@ tion . however within a year , the enthusias@@ m of the hon@@ est people of this nation , the celebr@@ ation of integr@@ ity in the country and the participation of people resul@@ ted in this new tax system man@@ aging to create a space for itself , has achieved stability and accord@@ ing to the need , it will bring reform through its in@@ built arran@@ gement .\n",
            "thousands of children with their par@@ ents used to come and att@@ end , i , too , used to att@@ end the ol@@ y@@ mp@@ ics .\n",
            "however , india has no@@ thing but faith and faith only and su@@ pre@@ me confidence in her hundred and twenty five crore countrymen that they will cer@@ t@@ ain@@ ly ful@@ f@@ ill their resolve .\n",
            "one thing is for sure , whether it is your stud@@ ent days or any other phase of your life , yoga is a major key to the development of your inn@@ er mind .\n",
            "let us create such an atmo@@ sphere in the nation .\n",
            "she would reach out to every house@@ hold to use mo@@ s@@ qu@@ i@@ to re@@ p@@ ell@@ ent and mo@@ s@@ qu@@ i@@ to ne@@ ts .\n",
            "indi@@ as tradition of celebr@@ ating festivals has been one to strengthen our lo@@ ve for nature and to develop each individu@@ al , right from chil@@ d@@ hood , as a cul@@ tu@@ red person .\n",
            "he has given a bri@@ ef but very be@@ au@@ ti@@ ful message to all students .\n",
            "ram@@ esh ji , respec@@ t@@ ful greetings to your mother .\n",
            "my dear countrymen , today is the hol@@ y festival of vi@@ j@@ ay d@@ as@@ ham@@ i .\n",
            "i salute these val@@ i@@ ant soldiers and pay my tributes to them .\n",
            "that p@@ ic@@ ture re@@ ally tou@@ ches ones heart .\n",
            "but u@@ st@@ ad b@@ is@@ mil@@ la@@ h k@@ h@@ ans ma@@ st@@ ery over the sh@@ e@@ hn@@ ai made it one of the fin@@ est mu@@ sical instru@@ ments in the world\n",
            "we should think about what more can be done in this direction .\n",
            "from 18@@ 57 to 194@@ 2 , the people of india , with their ar@@ d@@ ent desire for freedom , came together , f@@ ou@@ ght together , and suf@@ fer@@ ed har@@ d@@ ships\n",
            "systems are getting technology driven .\n",
            "this for@@ ged a strong b@@ ond between si@@ kh follow@@ ers and kashmir .\n",
            "by reducing the p@@ aper work , some of the states have made a good effort in spe@@ ed@@ ing up the process involved in or@@ g@@ an d@@ on@@ ation .\n",
            "it used to be played in almost every state .\n",
            "the st@@ ory of mu@@ k@@ t@@ ab@@ en pan@@ k@@ aj@@ kum@@ ar d@@ ag@@ al@@ i of gujarat will f@@ ill you with inspiration desp@@ ite being a div@@ yan@@ g her@@ self , it is difficul@@ t to find an example than what mu@@ k@@ t@@ ab@@ en did for the up@@ lif@@ t@@ ment of div@@ yan@@ g women by establishing the institution nam@@ ed cha@@ k@@ sh@@ u ma@@ h@@ il@@ a sev@@ ak@@ un@@ j , through the medium of which she has been eng@@ aged in a vir@@ tu@@ ous endeav@@ our to make vis@@ ually challeng@@ ed children self - reli@@ ant .\n",
            "they can inspire us to do a lot .\n",
            "i speci@@ ally want to sugg@@ est a scheme to my young friends who are curr@@ ently technology sav@@ v@@ y .\n",
            "but today that electr@@ ic bul@@ b ill@@ u@@ min@@ ates our lives .\n",
            "when there is a talk of new india , its cri@@ tic@@ ism , its an@@ aly@@ sis , it counter views , are but natural , and that is a fund@@ a@@ mental of democracy .\n",
            "i am even recei@@ ving sugg@@ estion on my@@ go@@ v that what should be my spe@@ ech on 15@@ th of august .\n",
            "indi@@ as economy has in itself an el@@ ement of social econom@@ ics .\n",
            "she left us in the col@@ um@@ bi@@ a space sh@@ utt@@ le mis@@ ha@@ p , but not without becoming a source of inspiration for millions of young people the world over .\n",
            "friends together , on october 2 , 2014 , we had em@@ bar@@ ked on a memor@@ able journey together to clean our country and to get ri@@ d of open de@@ fec@@ ation .\n",
            "this has g@@ ained wide accep@@ t@@ ance .\n",
            "ash@@ wan@@ i ji has written : i celebrate the festival , feel happy and s@@ mil@@ e@@ i celebrate the festival , fe@@ e happy and s@@ mil@@ e@@ all this is because you are there , i want to tel@@ l you today that you are the gu@@ ar@@ di@@ an of my freedom , my gi@@ ft of joy is there just because you are there@@ i s@@ le@@ ep peac@@ ef@@ ul@@ ly i s@@ le@@ ep peac@@ ef@@ ul@@ ly , because you are gu@@ arding the b@@ ord@@ ers there@@ the m@@ oun@@ t@@ ains , the sk@@ ies and the country bo@@ w to y@@ ou@@ the m@@ oun@@ t@@ ains , the sk@@ ies and the country bo@@ w to y@@ ou@@ i too bo@@ w in grati@@ tude to you o brave soldi@@ er\n",
            "and we are re@@ ap@@ ing the benefits of the efforts made by our anc@@ est@@ ors to sa@@ ve the environment .\n",
            "some@@ times we do in@@ justice to our youth gu@@ ided by a certain per@@ c@@ ep@@ tion .\n",
            "and remember , the international yoga day was not a single day initiative .\n",
            "we will keep meeting through mann ki baat , and will keep sharing matters close to our hear@@ ts .\n",
            "and those who op@@ ened their accounts under the pradhan mantri jan d@@ han@@ yo@@ j@@ na , have received the benefit of insurance as well .\n",
            "friends , in every field of life and when taking ex@@ ams , if you were able to study peac@@ ef@@ ul@@ ly for two hours earlier , then are you now able to do so for three hours\n",
            "i hope that the media will show the path to the people on how to sa@@ ve water , start a campaign , and also share the responsibility to free us from the water cr@@ is@@ is fore@@ ver i inv@@ ite then as well .\n",
            "25 billion countrymen lo@@ k@@ man@@ ya ti@@ la@@ k ji started the sar@@ v@@ jan@@ i@@ k gan@@ e@@ sho@@ t@@ sa@@ v with the basic aim to in@@ cul@@ c@@ ate the spirit of unity , enhance awareness in society , and promote the culture of toge@@ th@@ ern@@ ess .\n",
            "d@@ eng@@ ue first ent@@ ers af@@ fl@@ u@@ ent loc@@ al@@ ities and we should try and understand it .\n",
            "wh@@ en@@ ever you so@@ w a plant , place an ear@@ then po@@ t near it .\n",
            "i hope you will not do that .\n",
            "also , ple@@ ase see to it that during the celebr@@ ation of di@@ wal@@ i , fi@@ rec@@ r@@ ack@@ ers do not lead to in@@ cid@@ ents of fir@@ e or loss of a life .\n",
            "there could be a la@@ dies cl@@ u@@ b\n",
            "the country is re@@ ally proud of these div@@ yan@@ g friends for their achiev@@ ement .\n",
            "to know where i stood y@@ est@@ er@@ day and where i stand today .\n",
            "people of north india may not be know@@ ing what all is there in the south\n",
            "the concl@@ ud@@ ing line of this st@@ ory mak@@ es one very em@@ o@@ tional since it holds a vital tru@@ th about life , young ha@@ mi@@ d played the role of aged ha@@ mi@@ d aged am@@ e@@ en@@ a had tur@@ ned into child am@@ e@@ en@@ a .\n",
            "the whole world celebr@@ ates 8@@ th march as w@@ o@@ men@@ s day .\n",
            "and i would cer@@ t@@ ain@@ ly ur@@ ge the youth of our country to go to pun@@ ja@@ b , wh@@ en@@ ever they get the ch@@ ance , and visit the sam@@ ad@@ hi of bhag@@ at singh , su@@ k@@ h@@ de@@ v , raj@@ guru , bhag@@ at sin@@ gh@@ s mother and b@@ at@@ u@@ ke@@ sh@@ war d@@ ut@@ t .\n",
            "as i am tal@@ king to you today , the hol@@ y month of ram@@ z@@ an has already commen@@ ced .\n",
            "come , let us all build a healthy india .\n",
            "these states are making a very big effort to re@@ vi@@ ve these as water t@@ em@@ ples .\n",
            "we have to expand our reach to the world and make india known to the world .\n",
            "s@@ ant ra@@ vid@@ as ji tri@@ ed to expl@@ ain the importance of labour and the work@@ er throu@@ gh@@ out his life vi@@ a his mess@@ ages .\n",
            "and if we go on just using old technology without rec@@ our@@ se to research and innovation , we will become out@@ d@@ ated in this rap@@ id@@ ly changing world and age . and that is why the government too has taken steps to attr@@ act the new generation to@@ ward science and research innovation in the field of technology .\n",
            "i congratulate the government of me@@ gh@@ al@@ aya .\n",
            "on the narendra modi app , shri on@@ kar sh@@ et@@ ty ji of ud@@ up@@ i , karnataka has expressed his happiness on the completion of the national war memorial .\n",
            "y@@ ash nagar writ@@ es on my mobile app that when he read a question p@@ aper for the first time he f@@ ound it qu@@ ite difficul@@ t , but when he read the same p@@ aper with self - confidence t@@ ell@@ ing him@@ self , this is the only p@@ aper i have got and no other qu@@ estions are going to be given , i have to de@@ al with just these many qu@@ estions , and so when i started thin@@ king over them again , he writ@@ es , i was able to understand this p@@ aper qu@@ ite eas@@ il@@ y@@ .@@ . .@@ .@@ .\n",
            "af@@ ter@@ wards when she sho@@ wed me a small th@@ read - sp@@ un khadi hand@@ ker@@ chief , her e@@ y@@ es li@@ t up . with great respect and in an em@@ o@@ tion fill@@ ed voice , she said , that mahatma gandhi had s@@ ent this hand@@ ker@@ chief to her as a w@@ ed@@ ding gi@@ ft .\n",
            "the disp@@ lay of the ind@@ om@@ itable ar@@ d@@ our of the de@@ fe@@ ated and the sh@@ e@@ er hum@@ il@@ ity of the vic@@ t@@ ori@@ ous in the same fr@@ ame was a si@@ ght to be@@ hold .\n",
            "hal@@ k@@ u the farmer is happy even after his cr@@ ops are dest@@ ro@@ yed by fro@@ st , because now he will not be for@@ ced to s@@ le@@ ep in his fields in the col@@ d w@@ inter .\n",
            "i had appe@@ al@@ ed to those young people to think over how could they use their talent to indi@@ as benefit and do something in that direction wh@@ en@@ ever they f@@ ound time .\n",
            "in new y@@ or@@ k people did yoga at times s@@ qu@@ are .\n",
            "the waste from our k@@ it@@ ch@@ ens , be it ve@@ get@@ able pe@@ els , le@@ f@@ t@@ over food , e@@ g@@ g sh@@ ell@@ s or le@@ a@@ ves are all part of li@@ qu@@ id waste and are to be plac@@ ed in gre@@ en l@@ it@@ ter b@@ ins .\n",
            "this is not true at all , we need to mo@@ dif@@ y it accord@@ ing to the needs of india . for example , when a poor person works some@@ where as a lab@@ ou@@ re@@ r and there is a lot of physical effort involved but if a young man innov@@ ates something which can reduce the physical effort and help the lab@@ ou@@ re@@ r - i will call it as a start - up . i will ask the bank to help such an individu@@ al and i will tel@@ l him to move ahead with courage .\n",
            "the government of india will make an in dep@@ th in - house an@@ aly@@ sis and will study the practices being follow@@ ed around the world .\n",
            "not only the two of you but each one of our countrymen has felt an em@@ o@@ tional att@@ ach@@ ment with our ath@@ le@@ tes who particip@@ ated at the par@@ aly@@ mp@@ ics .\n",
            "last month on@@ wards , they have started broad@@ c@@ ast@@ ing ver@@ sions in regional langu@@ ages , immedi@@ ately after the broad@@ c@@ ast of my mann ki baat .\n",
            "i requ@@ est you to see it .\n",
            "all sections of society turn up in f@@ airs in large num@@ bers .\n",
            "i firm@@ ly believe that the sh@@ e@@ er gr@@ it and courage of the people of the state will see kerala rise again .\n",
            "there is no need to take out any money from your po@@ c@@ k@@ et and coun@@ t it\n",
            "you should cer@@ t@@ ain@@ ly s@@ end the p@@ ic@@ tures of three generations doing yoga together to me on narendra mo@@ di@@ app or on my@@ go@@ v .\n",
            "you must have seen that along with the jan dhan account people have been given a rup@@ ay card .\n",
            "a book has been comp@@ iled on these women achiev@@ ers , first la@@ dies , so that , the entire country comes to know about the power of these women and der@@ i@@ ve inspiration from their life work .\n",
            "my effort is to s@@ end le@@ t@@ ters to more than one crore people .\n",
            "accord@@ ing to ve@@ d@@ as , the p@@ urity within us is because of the earth .\n",
            "e@@ al@@ ous citizens of hund@@ re@@ ds of lands over@@ loo@@ ked di@@ visi@@ ons of c@@ aste , religi@@ on , region , col@@ our and g@@ ender to transform this occasion into a mas@@ sive festival .\n",
            "when we talk about the creation of a new india , then determin@@ ation exhi@@ b@@ ited by our youth is the ver@@ itable example of what is new india\n",
            "she has not been ha@@ iled as ' g@@ od@@ d@@ ess of women empowerment ' just for no@@ thing\n",
            "i am happy that the farmers of my country he@@ eded the needs of our poor people and cul@@ tiv@@ ated various pul@@ se cr@@ ops on about 2@@ 90 lakh h@@ ec@@ t@@ are land .\n",
            "it is true that festivals express the v@@ it@@ ality of a society .\n",
            "as a con@@ sequ@@ ence of the pul@@ w@@ ama terror attack and the sacrific@@ e of the brave jaw@@ ans , people across the country are ag@@ on@@ ized and en@@ r@@ aged .\n",
            "on the nar@@ end@@ ram@@ odi@@ app , on t@@ wit@@ ter , on fac@@ e@@ book , by post - i thank all of you for that .\n",
            "today , doctors pos@@ s@@ ess not just medical expertise they have a vast experience on the co - rel@@ ation between general lif@@ est@@ y@@ le tr@@ ends and their effect on our health .\n",
            "this time 19 ath@@ le@@ tes , including three women , took part in par@@ aly@@ mp@@ ics from our country .\n",
            "the festival of on@@ am is being celebrated in ker@@ el@@ a .\n",
            "with a str@@ ing of fes@@ tive events such as gov@@ ar@@ dhan pu@@ j@@ a , bha@@ i do@@ o@@ j , la@@ ab@@ h pan@@ ch@@ m@@ i till kar@@ ti@@ k pur@@ n@@ im@@ a this festival of li@@ ghts go@@ es on for qu@@ ite a long period of time .\n",
            "officials brief@@ ing the vill@@ ag@@ ers about the schemes and programmes implemented by the government and also in@@ qu@@ ired whether these facilities were available to them or not .\n",
            "today , the entire country is celebr@@ ating r@@ ak@@ sha@@ band@@ han .\n",
            "when i wan@@ ted to know the an@@ aly@@ sis , as to whether its only the young people who come forward or there are el@@ ders persons too , i was hear@@ t@@ ened to learn that among the re@@ cip@@ i@@ ents , there were 15 year old young persons , as well as el@@ der@@ ly people aged 65 - 70 .\n",
            "some eur@@ op@@ ean countries and china use anim@@ al d@@ un@@ g and other bi@@ o - waste to produce energy . but india was l@@ ac@@ king full capacity utiliz@@ ation .\n",
            "o gl@@ ori@@ ous children of mother india , you may remember that for the last few years , we have been r@@ un@@ ning a country@@ wide campaign swach@@ ch@@ ata hi se@@ w@@ a , the qu@@ est for cleanliness is service , around a c@@ ou@@ ple of wee@@ ks before the 2nd of october .\n",
            "one daugh@@ ter at sur@@ at in gujarat serv@@ ed only te@@ a to all the gu@@ ests who had come to her w@@ ed@@ ding . there was no big function , no fe@@ ast or ban@@ qu@@ et because there was a shor@@ t@@ age of cash due to de@@ mone@@ tis@@ ation .\n",
            "this trust rema@@ ined associated with farmers , dre@@ w plans and made successful efforts to increase the income of farmers .\n",
            "i also thank sh@@ r@@ im@@ an am@@ it@@ ab@@ h ji to connect in such a warm manner with mann ki baat and for taking forward the cleanliness campaign .\n",
            "every place has been bl@@ essed with a sain@@ t , a l@@ u@@ min@@ ary or a great , fam@@ ous person@@ ality who has contribu@@ ted through his s@@ ag@@ ac@@ ity .\n",
            "wher@@ e@@ as february and march get consum@@ ed in ex@@ ams , p@@ ap@@ ers and an@@ sw@@ ers , april may are me@@ ant for en@@ jo@@ ying v@@ ac@@ ations , follow@@ ed by results and there@@ after , sha@@ ping a course for ones life through c@@ are@@ er cho@@ ices .\n",
            "bro@@ ther har@@ sh@@ v@@ ar@@ dhan , i am happy to know that desp@@ ite this atmo@@ sphere char@@ ged with an@@ ger , you are able to think in a healthy manner at such a young age .\n",
            "the welfare of farmers was a priority in his life .\n",
            "my dear countrymen , from next week fes@@ tive se@@ ason will be ush@@ ered in with nav@@ r@@ at@@ ri and d@@ ur@@ ga pu@@ j@@ a , vi@@ jay@@ ad@@ ash@@ m@@ i , prepar@@ ations for deep@@ av@@ al@@ i and all such activities .\n",
            "that day , that moment , insti@@ l@@ ls in us all a sense of d@@ uty .\n",
            "there are technological me@@ th@@ ods which are safe , secure and inst@@ an@@ tan@@ e@@ ous .\n",
            "this time the theme is ' be@@ at pl@@ as@@ tic poll@@ ution ' .\n",
            "farmers and c@@ att@@ le her@@ ders will be helped in aug@@ menting their income .\n",
            "now when there are talks of such cr@@ is@@ is all around , we feel the need to bring in a lot of change .\n",
            "it is my bel@@ ief that you will su@@ re@@ ly pu@@ bl@@ ic@@ ise and spread awareness about this qu@@ i@@ z .\n",
            "what should the government do\n",
            "in cal@@ am@@ ities , they ris@@ k their lives to sa@@ ve our life .\n",
            "today , the whole world is tal@@ king about the environment .\n",
            "my dear countrymen , i will be wa@@ iting for your participation on 2@@ 9@@ th august in ' f@@ it india movement ' , in po@@ sha@@ n abhi@@ y@@ a@@ an during the month of september and especially in the ' swach@@ hat@@ a abhiyan ' beginning from the 1@@ 1@@ th of september to the 2nd of october .\n",
            "i w@@ ro@@ te a le@@ tter to the sar@@ pan@@ ch@@ s and gram prad@@ h@@ ans across the country .\n",
            "can we , this time , during the festival se@@ ason , di@@ spe@@ l some of the dar@@ k@@ ness ben@@ e@@ ath the l@@ am@@ p with due awareness and resolve\n",
            "it is because of them that we were able to celebrate di@@ wal@@ i , so i wan@@ ted to be with them .\n",
            "many a time we see them t@@ ell@@ ing el@@ ders not to l@@ it@@ ter on the roads .\n",
            "you all know that our own scientists have been successful in reach@@ ing mar@@ s , with least expenditure .\n",
            "i have to remain deeply ab@@ s@@ or@@ bed in fil@@ es , but for my own self , i have developed a hab@@ it of read@@ ing d@@ ail@@ y , at least a few of the le@@ t@@ ters i recei@@ ve and because of that i get a ch@@ ance to connect with the common man .\n",
            "in a similar manner , we have to join h@@ ands in cur@@ b@@ ing single use pl@@ as@@ tic .\n",
            "our scientists are doing some excell@@ ent work .\n",
            "my dear countrymen , connec@@ ting with all of you , cour@@ t@@ es@@ y the mann ki baat program has been a re@@ ally wond@@ erful experience for me .\n",
            "such is the st@@ ory of d . pr@@ ak@@ ash ra@@ o , living in the sl@@ um@@ s of c@@ utt@@ ack , or@@ is@@ s@@ a , who gave up every@@ thing of his in order to adop@@ t and re@@ alize the dreams of others\n",
            "a few days back a sar@@ pan@@ ch in a small remote village of haryana , sri s@@ uni@@ l j@@ ag@@ l@@ an ji launched sel@@ fi@@ e with daugh@@ ter campaign .\n",
            "today , as india and australia get ready to play , i conve@@ y my best wishes to both the te@@ ams .\n",
            "when the entire nation stand@@ s by the armed forces , their mi@@ ght mu@@ ti@@ p@@ li@@ es a hundred and twenty five crore times .\n",
            "on the last day before ex@@ ams beg@@ in , i would not re@@ ally ur@@ ge you to start doing p@@ ush - ups , get down on the field , or go for a 3 to 5 km wal@@ k .\n",
            "scientists have lab@@ o@@ red hard , generations of them have pers@@ ever@@ ed , and after nearly 100 years they have g@@ ained a huge success .\n",
            "this mat@@ ch will remain et@@ ched in our memor@@ ies for a long time .\n",
            "the month of june is so ho@@ t that people an@@ x@@ i@@ ously w@@ ai@@ t for the ra@@ ins , ga@@ z@@ ing towards the sk@@ y for the cl@@ ou@@ ds to appe@@ ar .\n",
            "and i had also fo@@ res@@ een that this would result in all of us having to face various new difficul@@ ties in our day - to - day lives .\n",
            "since then , people have been offer@@ ing boo@@ ks at many a place .\n",
            "so while it is good to care about society , this is beneficial to the family as well .\n",
            "but it is the wor@@ st in haryana .\n",
            "but , you did not find any dam@@ age any@@ where .\n",
            "women have also been added in this activity and they su@@ per@@ vis@@ e the work of this se@@ ed - bank .\n",
            "i tel@@ l you , water is ph@@ il@@ os@@ op@@ her ' s stone and its m@@ ere tou@@ ch cre@@ ates and reg@@ ener@@ ates life\n",
            "this will be the third year of its observ@@ ance .\n",
            "when i express my@@ self through mann ki baat , the one speaking is me , the words are min@@ e , the voice is min@@ e , but the st@@ ory is y@@ ours , the pur@@ ush@@ ar@@ th per@@ t@@ aining to your pursu@@ its and goals is y@@ ours , the par@@ ak@@ ram , the achiev@@ ement is y@@ ours .\n",
            "we all know that our lif@@ est@@ y@@ le is the biggest cause for di@@ ab@@ et@@ es .\n",
            "the marty@@ r@@ dom of these brave soldiers brought to the fore , through the media , tou@@ ching , inspir@@ ing st@@ ories of their k@@ in , which give hope and strength to the entire country .\n",
            "sardar sa@@ he@@ bs journey started with a struggle for farmers .\n",
            "i would like you to he@@ ar one such ph@@ one call : nam@@ as@@ kar , pradhan mantri ji , this is ne@@ et@@ u gar@@ g from gur@@ ga@@ on .\n",
            "the l@@ u@@ min@@ es@@ c@@ ence of guru nan@@ ak de@@ v j@@ is infl@@ u@@ ence can be felt not only in india but around the world .\n",
            "many of our festivals are linked str@@ ai@@ gh@@ ta@@ way with farmers and fisher@@ men .\n",
            "new im@@ ag@@ ination and new conc@@ ep@@ ts have been linked and connected with it .\n",
            "just a few days ago , i was wat@@ ching the st@@ ory of sh@@ r@@ im@@ an yo@@ g@@ esh sain@@ i and his team on the media .\n",
            "i felt , had i read the book before my ex@@ ams , i w@@ oul@@ d@@ ve benef@@ itted a great de@@ al .\n",
            "you can s@@ end in your suggestions through my@@ go@@ v , le@@ t@@ ters to radio or wr@@ ite le@@ t@@ ters to the p@@ ms office .\n",
            "modi ji : l@@ ata di@@ di , pr@@ an@@ a@@ am .\n",
            "there was a time when netaji used to we@@ ar it .\n",
            "they tra@@ ver@@ sed a multi@@ tude of oc@@ e@@ ans , many a sea , over a dist@@ ance of almost twenty two thousand n@@ au@@ tical mil@@ es .\n",
            "can you im@@ ag@@ ine how hard their task would have been to fr@@ ame the constitution of our country which has such big di@@ versities\n",
            "these daugh@@ ters have become self - reli@@ ant today and are living their lives with respect and have become a strong support to their families .\n",
            "when i talk of wond@@ ers in the engineering world , i am re@@ min@@ ded of an in@@ cident of 2001 when a de@@ v@@ ast@@ ating earth qu@@ ake h@@ it k@@ ut@@ ch in gujarat .\n",
            "it has been one month since gst was implemented and its benefits can be seen already .\n",
            "d@@ an@@ te@@ w@@ ada in ch@@ at@@ tisgarh is a ma@@ o@@ ist inf@@ ested region .\n",
            "while we are tal@@ king of the month of sa@@ w@@ an , you will be g@@ la@@ d to learn that the number of pil@@ gr@@ ims in the am@@ ar@@ nath y@@ at@@ ra this year has been the gre@@ atest in the past 4 years .\n",
            "i lear@@ n@@ t that on 1@@ 7@@ th november , a mar@@ ri@@ age was sol@@ em@@ n@@ ised with cha@@ i p@@ ar char@@ ch@@ a in sur@@ at .\n",
            "this is a great initiative to go far - off from delhi to east , and this act is called act east policy .\n",
            "this has now tru@@ ly taken the form of a movement .\n",
            "and you will be happy to know that har@@ d@@ ly four or five countries in the world pos@@ s@@ ess this cap@@ ability .\n",
            "the tri@@ ple tal@@ a@@ q bill has been pas@@ sed by the lok sabha al@@ thou@@ gh it could not be pas@@ sed in the raj@@ ya sabha , i ass@@ ure the mus@@ li@@ m women that the whole country stand@@ s by them to provide them social justice .\n",
            "recently , just a few days ago , when the board examin@@ ations were held , 9@@ 5 of the s@@ ons and daugh@@ ters of kashmir , the young students appe@@ ared in the examin@@ ations .\n",
            "the prime minister of singapore has po@@ sted a p@@ ic@@ ture on inst@@ ag@@ ram and has shared it prou@@ d@@ ly with the whole world .\n",
            "the money can be used produc@@ tively in the days to come .\n",
            "do you know why k@@ ab@@ ir d@@ as ji ch@@ ose to go to m@@ ag@@ har\n",
            "gan@@ esh ch@@ at@@ ur@@ th@@ i is a ten - day festival .\n",
            "you all know very well , that el@@ ep@@ han@@ ta is loc@@ ated 10 k@@ ms by the sea from mumbai .\n",
            "people will w@@ ai@@ t for the mo@@ on in a few days from now on@@ wards .\n",
            "but , i did not know how to go about it as what could a young stud@@ ent like me possi@@ b@@ ly do .\n",
            "d@@ on@@ t get tr@@ ap@@ ped in this sc@@ ore - keeping .\n",
            "my dear countrymen , bharat r@@ at@@ na mother ter@@ es@@ a will be can@@ on@@ ized , that is , accord@@ ed sain@@ th@@ o@@ od on 4@@ th september .\n",
            "in the process , he gets him@@ self dedicated to this cause and becom@@ es a soldi@@ er of that movement .\n",
            "i consid@@ er this to be a good om@@ en .\n",
            "if we are not aware of safety in d@@ ail@@ y life , if we are not able to at@@ tain a certain level , it will get extremely difficul@@ t during the time of dis@@ ast@@ ers .\n",
            "they had sub@@ lim@@ ated all their dreams for the freedom of mother india .\n",
            "my dear fel@@ low citizens , nam@@ as@@ kar .\n",
            "today is the 2@@ 6th of march .\n",
            "and some day , the p@@ und@@ its of econom@@ ics , p@@ und@@ its of management and p@@ und@@ its of technology , will cer@@ t@@ ain@@ ly under@@ take resear@@ ches and wr@@ ite about indi@@ as gst experi@@ ment as a model for the world .\n",
            "we all know the b@@ ond that india has with cr@@ ick@@ et , but i sa@@ w the same pas@@ sion for foo@@ t@@ b@@ all as well , and this by itself her@@ al@@ ds a very positive sign@@ al for the future .\n",
            "accep@@ t@@ ance br@@ ings about new av@@ en@@ ues in fin@@ ding solutions to problems .\n",
            "thus , it came as a sur@@ pr@@ ise even to the officers of the central government that some states have indeed made some very commend@@ able efforts related to water , environment , t@@ ack@@ ling d@@ rought , car@@ ing for anim@@ als as well as affected human be@@ ings . based on in@@ pu@@ ts from all cor@@ n@@ ers of the country , ir@@ respective of the rul@@ ing party there , we f@@ ound that we had to give a thought to fin@@ ding not only permanent solutions but also de@@ vis@@ ing practi@@ cal ways and means to de@@ al with this long - standing proble@@ m .\n",
            "there is continu@@ ous news of natural cal@@ am@@ ities from various parts of the world .\n",
            "the u@@ it india movement was an important mil@@ est@@ one in the indian freedom movement .\n",
            "and i see cle@@ arly that the news about cleanliness ke@@ eps p@@ our@@ ing in .\n",
            "my dear countrymen , we have to build a modern india .\n",
            "and , no one can do this better than you .\n",
            "ex@@ ams in themselves , should be a jo@@ y@@ ous occasion .\n",
            "a big cleanliness drive was conduc@@ ted there .\n",
            "few days back , when i twe@@ eted a p@@ ho@@ to of a viet@@ nam@@ ese child doing yoga , it was such a sw@@ e@@ et p@@ ho@@ to that it got a lot of attention from the entire world .\n",
            "andhra pradesh , too has decided to increase its gre@@ en cover by 50 by the year 20@@ 29 .\n",
            "work for every hand , water to every field through only these two words he presented his entire economic agenda .\n",
            "while l@@ ist@@ ening to this months mann ki baat , my inn@@ er thoughts , your mind must also be on your chil@@ dr@@ ens ex@@ ams which beg@@ in soon . some of them have their 10@@ th and 12@@ th board ex@@ ams star@@ ting from the 1st of march .\n",
            "ple@@ ase change this situation . ra@@ vi ji has expressed his an@@ ger but i think he is not v@@ ent@@ ing his an@@ ger on me but on the prev@@ ail@@ ing conditions .\n",
            "the world wat@@ ched the t@@ ap@@ as@@ ya , the aw@@ es@@ ome pers@@ ever@@ ance of our scientists .\n",
            "their will power , their struggle with life and their z@@ e@@ al to transform cr@@ is@@ is into opportunity is worth lot of pra@@ ise .\n",
            "the atmo@@ sphere was im@@ bu@@ ed with mat@@ ernal warm@@ th , and i was serv@@ ed with great affec@@ tion .\n",
            "during the last two years this programme involved the common man it has for@@ ced people from all cor@@ n@@ ers of the country to think and p@@ ond@@ er over this bur@@ ning issue and brought about a change in peop@@ les thin@@ king about our traditional beli@@ ef@@ s and custom@@ s which have been pre@@ val@@ ent for many years .\n",
            "we will not add to the existing di@@ r@@ t p@@ ile .\n",
            "wh@@ at@@ ever med@@ als we got have been ear@@ ned by our daugh@@ ters .\n",
            "gujarat too has an ill@@ u@@ stri@@ ous tradition of observ@@ ing v@@ an ma@@ ho@@ t@@ sa@@ v .\n",
            "he just s@@ at down with no re@@ ference to the ail@@ ment or to the situation . he started cr@@ ac@@ king jo@@ k@@ es and li@@ ght@@ ened the atmo@@ sphere in a matter of just a few min@@ utes .\n",
            "i had the opportunity to he@@ ar them and i was very happy to know that india is doing commend@@ able research in the field of science .\n",
            "in a way , the sel@@ ec@@ tion of these awards has been trans@@ formed comple@@ t@@ ely .\n",
            "and two other ministries , the ministry of shipping and the ministry of water resources , river development and ganga re@@ ju@@ ven@@ ation will take the cleanliness campaign forward during the last two wee@@ ks of march .\n",
            "the ch@@ ec@@ k post has become ext@@ inc@@ t after the arri@@ val of the gst scheme and the movement of goods has become faster , which not only sa@@ ves time but is also ac@@ cru@@ ing benefits in the are@@ a@@ of log@@ is@@ tics .\n",
            "in continu@@ ance with his vision , smart city mission and urban mission were k@@ ic@@ k - started in the country so that all k@@ in@@ ds of a@@ men@@ ities whether good roads , water supply , health facilities , education or digital connectivity are available in the big cities and small town@@ s of the country .\n",
            "so the last g@@ ame is signific@@ antly important .\n",
            "but i have seen that there is always an emphasis on the t@@ op@@ ic of cleanliness .\n",
            "they do not rest till they ob@@ tain an an@@ sw@@ er .\n",
            "so you must be positive and go@@ d will su@@ re@@ ly give you good results . i am hop@@ eful of that .\n",
            "share op@@ en@@ ly what you are going through , with your colle@@ ag@@ ues , friends , par@@ ents , brothers , and teachers .\n",
            "hear@@ ti@@ est fel@@ ic@@ it@@ ations to all of you .\n",
            "dr . ram man@@ o@@ har l@@ al ji had tal@@ ked of creating a mass aw@@ ak@@ ening on an ext@@ ensive scale about the necessary measures to ensure a better income for our farmers and provide better irrigation facilities and to increase food and mil@@ k production .\n",
            "our sain@@ ts have conveyed the message of go@@ od@@ will , equality and social empowerment through their thoughts and de@@ eds .\n",
            "however if you con@@ centr@@ ate and focus only on getting marks , then you grad@@ ually go on lim@@ iting y@@ our@@ self and con@@ fin@@ e y@@ our@@ self to certain areas for ear@@ ning more marks .\n",
            "in a very short time , 21st june has got wor@@ l@@ d@@ wide recogn@@ ition as world yoga day and is connec@@ ting people .\n",
            "i can never for@@ get my visits to gur@@ ud@@ war@@ as in v@@ anc@@ ou@@ ver and te@@ h@@ r@@ an .\n",
            "s . kar@@ ti@@ k has written on nar@@ end@@ ram@@ odi@@ app that our ath@@ le@@ tes who particip@@ ated in the par@@ aly@@ mp@@ ics have created a new history and their performance is a tri@@ um@@ p@@ h of the human spirit .\n",
            "for my part i wish good l@@ uc@@ k to all the village head@@ s and all the sar@@ pan@@ ch@@ s for their d@@ y@@ nam@@ ism .\n",
            "in the year 2019 , 100 years of the hor@@ ri@@ fic in@@ cident of j@@ alli@@ an@@ w@@ ala b@@ ag@@ h will come to a full cir@@ c@@ le , it was an in@@ cident that em@@ bar@@ r@@ as@@ sed the entire humanity .\n",
            "if they are tal@@ king to any friend of the@@ ir@@ s , ple@@ ase d@@ on@@ t st@@ op them .\n",
            "the country@@ s wom@@ an power has contribu@@ ted a lot in the positive transformation being witn@@ essed in our country society these days .\n",
            "wher@@ ever possible , my colle@@ ag@@ ues in the council of ministers are also person@@ ally visiting affected areas .\n",
            "and how the facilities r@@ end@@ ered by panchay@@ ats can affec@@ t the day to day life , were t@@ op@@ ics that were discussed and people too interacted in a particip@@ atory fa@@ shi@@ on t@@ ell@@ ing about their problems .\n",
            "my dear countrymen , we celebrate 5@@ th september as teachers day .\n",
            "people spoke about their determin@@ ation , their happiness and their achievements .\n",
            "i view the fif@@ a event as a great opportunity to establish india as a br@@ and at a global level .\n",
            "it is becoming e@@ vid@@ ent that along with stu@@ dies , our new generations can see a future in sports as well . and our spor@@ t@@ sper@@ s@@ ons , through their pro@@ w@@ ess , skills and ac@@ comp@@ lish@@ ments win la@@ u@@ rel@@ s for the country as well .\n",
            "just like last year , c@@ ant you gi@@ ft on the occasion of r@@ ak@@ sha band@@ han pradhan mantri sur@@ ak@@ sha bim@@ a yo@@ j@@ na or je@@ ev@@ an j@@ yo@@ ti bim@@ a yo@@ j@@ na to mo@@ thers and sisters of our country .\n",
            "i will try , that wh@@ en@@ ever i talk , it is s@@ und@@ ay , around 11 am .\n",
            "but i have g@@ ained a lot , in the sense that based on your cor@@ respon@@ d@@ ence here , i got opportunities to know about the them@@ es of many k@@ in@@ ds of boo@@ ks .\n",
            "on the one hand , when we experience the delivery in of sw@@ e@@ et@@ me@@ ats , app@@ are@@ l , gi@@ f@@ ts and so on , let us think for a moment on the process of delivery out .\n",
            "de@@ v@@ esh from delhi gas also s@@ ent me a similar message .\n",
            "there was also loss of life and property .\n",
            "the name of this unique cont@@ est is \" clean be@@ au@@ ti@@ ful to@@ il@@ et \" .\n",
            "my dear country men , through mann ki baat , i connect with you regul@@ arly .\n",
            "we shall prep@@ are a road@@ map on how we can do better .\n",
            "the task of f@@ it@@ ting hear@@ ing ai@@ ds to 6@@ 00 hear@@ ing imp@@ ai@@ red div@@ yan@@ g people in just eight hours was completed successfully .\n",
            "when i would l@@ ist@@ en to the st@@ ories of a common man , his gu@@ il@@ el@@ ess words nar@@ r@@ ating his experience would tou@@ ch my heart .\n",
            "this is a prime example of the great transformation , which can be brought about through peop@@ les participation .\n",
            "i also got an opportunity to meet , the young and d@@ y@@ nam@@ ic students , bo@@ ys and gir@@ ls over te@@ a who had won med@@ als in sports .\n",
            "other festivals of our country too , provide an opportunity to attr@@ act foreign vis@@ it@@ ors .\n",
            "they can sp@@ on@@ s@@ or young professionals w@@ ill@@ ing to work for swach@@ chhat@@ a mission .\n",
            "i speci@@ ally call upon schools , par@@ ents , enthusias@@ tic teachers ment@@ ors to stri@@ ve hard to ensure vic@@ tory for their respective schools .\n",
            "this process is going to ga@@ in f@@ res@@ h momentum and strength with india jo@@ ining h@@ ands with others .\n",
            "our scientists will be involved in the future as well to take this disc@@ overy forward\n",
            "you know what an app is , what online banking is and how online tic@@ k@@ et boo@@ king is done .\n",
            "bhag@@ w@@ an bir@@ s@@ a m@@ und@@ a not only w@@ aged a struggle against the bri@@ tis@@ h for political freedom he also acti@@ vely f@@ ou@@ ght for the social economic rights of the tribal fol@@ k .\n",
            "i am inde@@ b@@ ted to ak@@ ash@@ v@@ an@@ i because they broad@@ c@@ ast this program of mann ki baat by 8 oc@@ loc@@ k in every state in their regional langu@@ ages .\n",
            "our culture has a continu@@ um spread over many m@@ ill@@ en@@ n@@ ia and it is a part of our heritage .\n",
            "no indian can ever for@@ get .\n",
            "when the mor@@ tal remains of marty@@ r vi@@ j@@ ay so@@ re@@ n , dr@@ ap@@ ed in the tr@@ ic@@ ol@@ or reached g@@ um@@ la , jharkhand , his in@@ no@@ cent son iter@@ ated that he too would join the armed forces .\n",
            "the government of india has taken a very important decision .\n",
            "in this manner an atmo@@ sphere of competition for cleanliness has been created .\n",
            "during the freedom movement , mahatma gandhi also used to m@@ oun@@ t high@@ ly successful experi@@ ments to di@@ ver@@ t the strong energy of that movement towards construc@@ tive social work , wh@@ en@@ ever a l@@ ul@@ l was ne@@ eded am@@ id@@ st the ext@@ re@@ me int@@ ens@@ ity in that movement .\n",
            "how att@@ ach@@ ed was mahatma gandhi att@@ ach@@ ed to the soil , they farm@@ s and the farmer is refl@@ ected from this s@@ ent@@ ence in which he said - to for@@ get how to di@@ g earth and to t@@ end the soil , is to for@@ get our@@ selves .\n",
            "he beli@@ ev@@ ed that the person see@@ ing his pa@@ int@@ ing should compreh@@ end the pa@@ int@@ ing in to@@ t@@ ality , from his own per@@ spec@@ tive , understand the message im@@ par@@ ted by him in the particul@@ ar@@ pa@@ int@@ ing .\n",
            "can you post on incre@@ di@@ b@@ le@@ india\n",
            "on the completion of a hundred days , a gr@@ and clo@@ sing cer@@ em@@ ony will be held .\n",
            "recently , i conver@@ ed at l@@ eng@@ th with the chief ministers of el@@ even states , re@@ el@@ ing under sev@@ ere d@@ rought - uttar pradesh , rajasthan , gujarat , maharashtra , madhya pradesh , chhattisgarh , jharkhand , karnataka , andhra pradesh , tel@@ an@@ gan@@ a and , odisha .\n",
            "connec@@ ting with nature means nur@@ t@@ uring a better plan@@ et .\n",
            "when on the one hand , a sense of fes@@ tivity per@@ v@@ ades the land , and on the other , news of violence comes in , from one part of the country , it is only natural of be concerned .\n",
            "some@@ times one is not able to sc@@ ore good marks in ex@@ ams , but he or she sur@@ ges ahead in sports , or does well in mu@@ sic , or ex@@ cel@@ s in the fin@@ e ar@@ ts , or for@@ ges ahead in business .\n",
            "the way we jointly r@@ an a me@@ ga campaign sel@@ f@@ i with daugh@@ ter , which spread globally .\n",
            "di@@ ab@@ et@@ es is of two ty@@ p@@ es ty@@ pe 1 and ty@@ pe 2 . ty@@ pe 1 is her@@ ed@@ itary\n",
            "through the atal gr@@ and challenges we have exhor@@ ted the young generation of the country that if they see problems , they should se@@ ar@@ ch for solutions taking the path of technology , doing research , ap@@ pl@@ ying innov@@ ations and bring those on board .\n",
            "my dear countrymen , the month of august is significant because it is fill@@ ed with histor@@ ically important d@@ ates and festivals , but due to the we@@ a@@ ther some@@ times si@@ c@@ k@@ ness also ent@@ ers the house .\n",
            "what i me@@ an to say is that the festival of li@@ ghts , deep@@ aw@@ al@@ i is becoming a festival of inspiration for the world community also to move from dar@@ k@@ ness to light .\n",
            "we also get to he@@ ar of a few ac@@ cid@@ ents around the time of di@@ wal@@ i .\n",
            "in 19@@ 21 , in the cong@@ ress session in ahmedabad , thousands of deleg@@ ates from across the country were sl@@ ated to participate .\n",
            "pen@@ n@@ ind@@ a bro@@ o@@ h@@ u per@@ ch@@ id@@ an@@ up@@ en@@ n@@ ind@@ a jan@@ ak@@ ar@@ a@@ ay@@ an@@ u j@@ as@@ u@@ val@@ end@@ an@@ u ) which means , him@@ want , lord m@@ oun@@ t att@@ ained f@@ ame on account of daugh@@ ter par@@ v@@ ati , rish@@ i bri@@ gh@@ u on account of his daugh@@ ter lak@@ sh@@ m@@ i and king jan@@ ak because of daugh@@ ter sit@@ a . our daugh@@ ters are our pri@@ de@@ .@@ . .@@ .@@ .\n",
            "they have sho@@ wed that if once the targets are set , spir@@ its are high and with a strong resolve , the set targets can most cer@@ t@@ ain@@ ly be att@@ ained .\n",
            "in india also more importance needs to be given to our daugh@@ ters along with increased awareness and sensi@@ tivity towards them .\n",
            "united nations has resol@@ ved to provide univers@@ al health coverage for all by 2030 .\n",
            "come , let us proc@@ e@@ ed with a new z@@ e@@ al , new resolve and renew@@ ed strength .\n",
            "our festivals are now becoming great attr@@ actions for tourism .\n",
            "we will be able to ga@@ ther deta@@ iled information knowledge . but if you ask me what the two gre@@ atest l@@ ess@@ ons i have received from chand@@ ra@@ y@@ a@@ an ii , i shall say they are faith f@@ ear@@ l@@ ess@@ ness .\n",
            "it is these festivals which bring happiness in our d@@ ail@@ y lives from time to time . be it poor or rich , people from village or from urban areas , festivals hold a different signific@@ ance in every@@ ones lives .\n",
            "they use f@@ ake le@@ tter p@@ ad@@ s while s@@ ending these le@@ t@@ ters . they ob@@ tain your credit card number and deb@@ it card number and emp@@ ty your bank account through technology .\n",
            "but i am g@@ la@@ d , sh@@ il@@ p@@ a , that you have observ@@ ed these things .\n",
            "this will defin@@ it@@ ely prove beneficial to our farmer brothers and sisters in so@@ wing of k@@ har@@ if cr@@ ops .\n",
            "i see g@@ op@@ i ch@@ and ji today as an excell@@ ent teach@@ er besides being a very ac@@ comp@@ lish@@ ed play@@ er .\n",
            "the state governments have br@@ ac@@ ed themselves up for res@@ c@@ ue and relief work .\n",
            "it will become very eas@@ y to do money transactions through your mobile ph@@ one . and im happy to tel@@ l you that the n .\n",
            "i always tel@@ l people , that if you go to por@@ band@@ er , the birth place of mahatma gandhi , you will be able to see a two hundred year old under@@ ground water tan@@ k which got directly re@@ char@@ ged with the ra@@ in water . you can still see it .\n",
            "this question may be there in the world community .\n",
            "my dear countrymen , i too am an ordinary citizen like you and like any ordinary citizen i too am infl@@ u@@ enc@@ ed by good or b@@ ad things .\n",
            "some are lear@@ ning co@@ o@@ king , some others are lear@@ ning d@@ ance and d@@ ram@@ a some have written that they have now started to wr@@ ite po@@ ems .\n",
            "sub@@ ha@@ sh bab@@ u will always be remembered as a brave soldi@@ er and a@@ dep@@ t organ@@ is@@ er\n",
            "and now the festival of ei@@ d is here .\n",
            "but i do int@@ end to try this time to shor@@ ten my spe@@ ech .\n",
            "i had the charge of the chief minister of guj@@ r@@ at at a period of time when the hab@@ it@@ at of lions in the fore@@ sts of gi@@ r was sh@@ r@@ in@@ king .\n",
            "how to develop systems for the dis@@ ab@@ led in government offic@@ es and public sp@@ ac@@ es\n",
            "social organ@@ is@@ ations , cultural organizations and common citizens char@@ ged with the spirit of service are also making their ut@@ most efforts to help the affected people in this situation .\n",
            "under m@@ n@@ reg@@ a also a st@@ ress has been given to create as@@ sets for water conservation .\n",
            "a gent@@ le@@ man mr bharat gu@@ pt@@ a has s@@ ent me a m@@ ail on my@@ go@@ v . in from sat@@ na , madhya pradesh .\n",
            "and they have requ@@ ested that i talk about ec@@ o - friendly gan@@ e@@ sha id@@ ols , well in time for people to plan for cl@@ ay gan@@ e@@ sha id@@ ols during the festival of gan@@ esh ch@@ at@@ ur@@ th@@ i .\n",
            "he said , no , but it is linked with your voice proble@@ m brought on by deli@@ ver@@ ing so many election spe@@ ec@@ h@@ es .\n",
            "i am tal@@ king about those students who s@@ log@@ ged for the entire year , those of 10@@ th and 12@@ th class .\n",
            "mad@@ ur@@ ai ch@@ inn@@ a pil@@ la@@ i is the same person who at first tri@@ ed to em@@ power the do@@ wn@@ tr@@ od@@ den and the exp@@ lo@@ ited through the kal@@ an@@ ji@@ y@@ am movement in tamil nadu and initi@@ ated community based mic@@ ro financing .\n",
            "so , we are introduc@@ ing this scheme now .\n",
            "positive power is the biggest energy .\n",
            "ours is the country of buddha and gandhi , it is the land of sardar patel who gave up his all for the unity of the nation .\n",
            "on this day , we celebrate the un@@ bre@@ ak@@ able b@@ ond that exist@@ s between indians in india and indians living around the glo@@ be .\n",
            "i have been told that this movement f@@ ound maximum leadership am@@ id@@ st women .\n",
            "it has contribu@@ ted a great de@@ al in fo@@ st@@ ering better relations with many countries of the world .\n",
            "gr@@ and par@@ ents , par@@ ents and children should together per@@ form yoga and also up@@ lo@@ ad their p@@ ho@@ to@@ s .\n",
            "where do we want to reach as a nation\n",
            "you can find something around you . le@@ ts say you dec@@ ide to make the best out of waste .\n",
            "at the same time , we also come to know about our culture and traditions .\n",
            "the disc@@ overy chan@@ nel plans to broad@@ c@@ ast this programme in 1@@ 65 countries in their respective langu@@ ages .\n",
            "today when i talk to you , i want to men@@ tion that around a year back the jan dhan yojana was started at a large scale .\n",
            "co@@ in@@ cid@@ ent@@ ally , i am fortun@@ ate today to be in a@@ ur@@ ov@@ il@@ le , the land , the k@@ arm@@ ab@@ ho@@ om@@ i of mahar@@ sh@@ i ar@@ v@@ ind .\n",
            "if only for five min@@ utes , give y@@ our@@ self a bre@@ ak .\n",
            "my dear countrymen , the month of august is the month of revolution .\n",
            "ex@@ ams are not just for the students , they also put the students families , schools and teachers to t@@ est .\n",
            "my ex@@ am gets over tomorrow and y@@ ours beg@@ in the d@@ ay@@ after .\n",
            "work@@ sh@@ ops should be held at different places .\n",
            "a small experi@@ ment can turn l@@ uc@@ r@@ ative , our farmers are no less than scientists .\n",
            "people bring to the fore challenges fac@@ ing the country and society they also come out with solutions for the same .\n",
            "and i will continue to do such things because of your lo@@ ve .\n",
            "an effort was initi@@ ated which was nam@@ ed go@@ b@@ ar - dhan - g@@ al@@ v@@ an@@ izing organ@@ ic bi@@ o ag@@ ro resources .\n",
            "these are port@@ ents of bo@@ om@@ ing tourism in the future .\n",
            "and i know it very well that all of you are acti@@ vely contribu@@ ting in this great task .\n",
            "thousands of people have expressed similar s@@ enti@@ ments .\n",
            "you will be sur@@ prised that only 8 of the water received from ra@@ ins in the entire year is har@@ vest@@ ed in our country .\n",
            "my first requ@@ est is that just like cleanliness drive has been given the sha@@ pe of a mass movement by the countrymen , let ' s also start a mass movement for water conservation .\n",
            "just a few days ago , in the j@@ uni@@ or under - 20 world ath@@ le@@ tics cham@@ pi@@ on@@ ship in fin@@ land , india ' s brave daugh@@ ter and a farmer daugh@@ ter him@@ a d@@ as made history by w@@ inning the gold med@@ al in the 400 - me@@ ter r@@ ace event .\n",
            "first one is that the siz@@ e of the cabinet in states was re@@ str@@ ic@@ ted to 15 of the total se@@ ats in the state assembly .\n",
            "for example , take japan yoga ni@@ k@@ et@@ an ' , which has made yoga popul@@ ar throu@@ gh@@ out japan .\n",
            "friends , i tou@@ ched upon the subject of water conservation in mann ki baat and today i feel that it was an issue close to your heart even before i referred to it , a matter of univers@@ al human interest .\n",
            "this was a first of its kind event in the entire world .\n",
            "if you d@@ on@@ t know how to s@@ wi@@ m , can you resolve to learn sw@@ im@@ m@@ ing during the hol@@ id@@ ays if you d@@ on@@ t know how to cy@@ c@@ le , can you resolve to learn cy@@ cl@@ ing during the hol@@ id@@ ays\n",
            "what an acc@@ ur@@ ate dep@@ ic@@ tion of sav@@ ar@@ kar by atal ji\n",
            "this will be possible because of the efforts of the people , and by the time we celebrate the 15@@ 0th anniversary of mahatma gandhi , the 125 crore people of the country should be able to prou@@ d@@ ly pro@@ cl@@ aim that we have made india clean .\n",
            "service as a vir@@ tu@@ e is me@@ an@@ ing@@ ful when it is per@@ formed with a sense of joy - sev@@ a par@@ m@@ o d@@ har@@ ma@@ h .\n",
            "i believe that you will s@@ end your good suggestions .\n",
            "once again , i express my grati@@ tude to our l@@ it@@ t@@ le friend , stud@@ ent al@@ e@@ en@@ a ta@@ ay@@ an@@ g from arunachal .\n",
            "some people c@@ ross all lim@@ its of dec@@ ency and say and wr@@ ite things that infl@@ ict p@@ ain and h@@ ur@@ t .\n",
            "on this day , discussions expres@@ sing concern on the issue are held all over the world for sav@@ ing the environment .\n",
            "for the first time , senior officials reached the villages , directly , even those officials who had never visited a village were available to the vill@@ ag@@ ers at their do@@ or@@ steps , to j@@ ud@@ ge for themselves to g@@ au@@ ge the ob@@ st@@ ac@@ les in the path of progress and to re@@ move the h@@ ur@@ d@@ les .\n",
            "two or five rupees make no dif@@ ference to your life , but have you thought how much hear@@ t@@ ac@@ he this pe@@ t@@ ty hab@@ it of y@@ ours can cause him .\n",
            "once we start , we will move ahead with great ease .\n",
            "we will not get the people to hold our f@@ ing@@ ers and help us in wal@@ king .\n",
            "mother nature gi@@ ves us life and nur@@ tures us , but at times natural c@@ at@@ a@@ str@@ op@@ h@@ es such as flo@@ ods and ear@@ th@@ qu@@ ak@@ es w@@ re@@ ak ha@@ vo@@ c on a mas@@ sive scale .\n",
            "in the beginning it may appe@@ ar to be a b@@ it difficul@@ t , but once we get used to it , then this arran@@ gement will see@@ m very eas@@ y for us .\n",
            "i was under@@ going a b@@ out of emp@@ t@@ iness .\n",
            "we are famil@@ i@@ ar with this wor@@ d , de@@ pres@@ sion .\n",
            "that energy is gener@@ ated from within on its own .\n",
            "l@@ ist@@ en to my voice and the way i talk .\n",
            "one fe@@ els a sense of new@@ ness in wh@@ at@@ ever he does . and th@@ ats why today , as i conver@@ se with you , my attention is dra@@ wn towards two mo@@ h@@ ans .\n",
            "similarly , un peace keeping force did a spec@@ t@@ ac@@ ul@@ ar disp@@ lay of yoga .\n",
            "to@@ il@@ et p@@ it emp@@ t@@ ying exerc@@ ise was carried out in hy@@ der@@ abad on 1@@ 7@@ th and 18@@ th february .\n",
            "all of us have experi@@ enc@@ ed this but we do not regist@@ er this , we do not connect this in a single th@@ read of sequ@@ ence .\n",
            "san@@ sk@@ r@@ it langu@@ age l@@ iter@@ ature en@@ comp@@ ass@@ es a sto@@ re@@ house of knowledge per@@ t@@ aining to every fac@@ et of life .\n",
            "for their resol@@ ute z@@ e@@ al i congratulate these students and i also fel@@ ic@@ it@@ ate their par@@ ents , their k@@ ith and k@@ in , their teachers and all the gram prad@@ h@@ ans as well from the dep@@ ths of my heart .\n",
            "during one ud@@ a@@ asi , gur@@ un@@ an@@ ak de@@ v ji travel@@ led north to kashmir and neighb@@ oring areas .\n",
            "my dear countrymen , the government , society , institutions , organizations , in fact everyone , is making some or the other effort towards greater cleanliness .\n",
            "there is a hol@@ y place in kash@@ i nam@@ ed gur@@ u@@ b@@ ag@@ h gur@@ ud@@ war@@ a . it is beli@@ ev@@ ed that guru nan@@ ak de@@ v ji had hal@@ ted there .\n",
            "u@@ su@@ ally such talks do not go beyond discussion on how much money was gr@@ an@@ ted by the central government state - w@@ ise , and how much money was ac@@ tu@@ ally sp@@ ent by each state .\n",
            "the government has also taken many important decisions , wher@@ e@@ in priority has been accord@@ ed to our farmers and villages .\n",
            "why d@@ on@@ t we dec@@ ide to bre@@ ak our own previous record\n",
            "the first state to achieve this was si@@ k@@ k@@ im , then him@@ achal pradesh ac@@ comp@@ lish@@ ed it and kerala is going to be of@@ d on november 1st .\n",
            "last time on 15@@ th august i had proposed the san@@ s@@ ad ad@@ ar@@ sh gram yojana .\n",
            "let us turn this v@@ ac@@ ation into a lear@@ ning opportunity , for a to@@ t@@ ally new experience . my dear countrymen\n",
            "through self lear@@ ning , m@@ ach@@ ines today can enhance their intelligence to a smar@@ ter level .\n",
            "we are experi@@ enc@@ ing a new sense of con@@ sci@@ ous@@ ness .\n",
            "but the m@@ p from that place , a senior member who has also been our dep@@ uty s pe@@ ak@@ er , shri k@@ ari@@ a m@@ und@@ a has dedicated his life for the trib@@ als .\n",
            "but this is not the tru@@ th .\n",
            "this year the ra@@ ins have started on a good note .\n",
            "vik@@ as krish@@ na y@@ ad@@ a@@ v reached the qu@@ ar@@ ter fin@@ als in bo@@ x@@ ing but could not win br@@ on@@ z@@ e .\n",
            "and therefore my op@@ in@@ i@@ on is that ex@@ ams are like a festival and , h@@ ence , must be celebrated .\n",
            "farmers also feel that their ri@@ p@@ ened cr@@ ops and fru@@ its should reach markets across the country .\n",
            "you have brought gl@@ ory to the nation .\n",
            "my mother remembered . she was t@@ ell@@ ing me that .\n",
            "you may be aware of the fact that in@@ side red fort , many cham@@ bers and struc@@ tures lay loc@@ ked un@@ used for several decades after independence .\n",
            "whole - hear@@ ted efforts have been made and a con@@ vic@@ tion has now set in that we shall indeed succ@@ e@@ ed .\n",
            "y@@ est@@ er@@ day was the festival of on@@ am . we pr@@ ay for on@@ am to provide strength to the country , especially kerala so that it retur@@ n@@ s to nor@@ mal@@ c@@ y on a new@@ er journey of development .\n",
            "her family is very poor . i was sur@@ prised that the gir@@ l had memor@@ ized the bhag@@ v@@ ada ge@@ et@@ a comple@@ t@@ ely but the most inter@@ esting thing was that she k@@ ept rec@@ iting the sh@@ lo@@ k@@ as along with its eng@@ lish inter@@ pre@@ tation and its defin@@ ition in eng@@ lish as well as hin@@ di . when i asked , her fa@@ ther told me that she started rec@@ iting these sh@@ lo@@ k@@ as since she was only five years old .\n",
            "some@@ time back , i had the opportunity to meet the schol@@ ars of the suf@@ i tradition .\n",
            "ac@@ tu@@ ally , this month , the world chil@@ dr@@ ens w@@ inn@@ ers games were held in mo@@ s@@ co@@ w .\n",
            "and on the other hand , ex@@ ac@@ tly one hundred years ago on the 10@@ th of april , 19@@ 17 , mahatma gandhi had launched the cham@@ par@@ an sat@@ y@@ ag@@ ra@@ ha .\n",
            "and there are many families who have written to me these very fe@@ el@@ ings .\n",
            "it is th@@ or@@ ou@@ gh@@ ly indian in heart spirit .\n",
            "it provided much ne@@ eded succ@@ or to the sc@@ or@@ ched tre@@ es sh@@ ru@@ bs , and to par@@ ched water bo@@ dies on the other hand exc@@ es@@ sive ra@@ inf@@ all ca@@ uses de@@ v@@ ast@@ ating flo@@ ods .\n",
            "it is one of the sim@@ pl@@ est ke@@ ys\n",
            "the organiz@@ ing of gram sab@@ has in these villages , wher@@ e@@ in the people particip@@ ated in large num@@ bers and ch@@ al@@ ked out schemes for themselves is something that is an extremely pleas@@ ant experience .\n",
            "my dear young friends , i d@@ on@@ t like it when hol@@ id@@ ays are just fr@@ it@@ tered away .\n",
            "this campaign made people aware of social mal@@ a@@ dies such as child - mar@@ ri@@ age and the do@@ wr@@ y system .\n",
            "when we go to a temple , we are given an offer@@ ing of pr@@ as@@ ad and even if a small b@@ it of that sp@@ ill@@ s , we feel b@@ ad in our hear@@ ts .\n",
            "you must have noti@@ ced that this time it has been decided to construc@@ t five lakh p@@ on@@ ds and far@@ m water reser@@ vo@@ ir@@ s .\n",
            "about 80 - 8@@ 5 programmes have already been conduc@@ ted .\n",
            "the world is wor@@ ri@@ ed about di@@ ab@@ et@@ es and so it has been cho@@ s@@ en as the theme for world health day on 7@@ th april .\n",
            "there is always some effort made in this direction , but still the ap@@ ath@@ y on this issue is a cr@@ ime against society .\n",
            "they prov@@ ed that through pers@@ ever@@ ance , gr@@ it and a firm resolve , all k@@ in@@ ds of ob@@ st@@ ac@@ les and bar@@ ri@@ ers can be bro@@ k@@ en cro@@ s@@ sed , to char@@ t out an all new pa@@ th@@ .@@ . .@@ .@@ .\n",
            "my dear fel@@ low citizens , there is a lot of work being done in this direction .\n",
            "f@@ it india does not me@@ an that if we go to the gy@@ m for two hours every mor@@ ning and ev@@ ening , it will suf@@ fic@@ e\n",
            "the festivals of v@@ as@@ ant pancha@@ m@@ i , maha@@ shi@@ v@@ r@@ at@@ ri and hol@@ i , im@@ part hu@@ es of happiness to a persons life .\n",
            "than@@ ga@@ vel@@ u had lost his right le@@ g when he was just 5 .\n",
            "once , an inter@@ esting fact was brought to my noti@@ ce .\n",
            "if you have a p@@ ho@@ to@@ grap@@ h with your teach@@ er or an in@@ cident connected with your teach@@ er or an inspir@@ ational thing associated with your teach@@ er , ple@@ ase do share the same on nar@@ end@@ ram@@ odi@@ app .\n",
            "and one after the other , anc@@ ill@@ ary services like transport , l@@ od@@ ging , guid@@ es and ec@@ o - friendly services are getting in@@ duc@@ ted by themselves .\n",
            "we wat@@ ch the ad@@ ver@@ ti@@ se@@ ment on t@@ v but do not pay attention .\n",
            "government of india had never thought that in terms of tourism you all could contribute in such a mas@@ sive way .\n",
            "this cha@@ t was ak@@ in to a youn@@ ger bro@@ ther speaking to his el@@ der sist@@ er affec@@ tion@@ ately .\n",
            "and , we all are of the view , and this is the view of all 125 crore indians , right from the village pradhan to the prime minister that every loss of life in kashmir whether it is of a young man or of a security personnel it is our own loss , a loss of our people , a loss for the nation\n",
            "sha@@ k@@ es@@ pe@@ are in his play , the mer@@ ch@@ ant of ven@@ ice , while expl@@ aining the importance of for@@ giv@@ eness , has written , mer@@ c@@ y is tw@@ ice bl@@ est , it bl@@ es@@ se@@ th him that gi@@ ves and him that tak@@ es , me@@ an@@ ing , the for@@ giv@@ er and the for@@ given both stand to recei@@ ve div@@ ine bl@@ ess@@ ing .\n",
            "i extend my warm@@ est greetings to every@@ body on the occasion of e@@ aster .\n",
            "in this mission of his , s@@ ai@@ du@@ l sol@@ d off family je@@ well@@ ery and ra@@ ised funds through char@@ ity .\n",
            "because when i was in gujarat , i had set this tradition of welcom@@ ing , by not giving b@@ ou@@ qu@@ ets , but boo@@ ks or hand@@ ker@@ ch@@ i@@ ef@@ s in@@ ste@@ ad .\n",
            "it is said that the re@@ ver@@ ed ass@@ am@@ ese sain@@ t shan@@ kar de@@ v too was inspired by him .\n",
            "equality and equ@@ al opportunity are the two biggest mantr@@ as for any society and government .\n",
            "indian doctors have car@@ ved a n@@ ic@@ he for themselves in the entire world through their capabilities and skills .\n",
            "we have around 500 small cities .\n",
            "today , through the ' mann ki baat ' programme , i am ent@@ re@@ ating 3 requ@@ ests to the fel@@ low countrymen .\n",
            "si@@ r , i have a question . recently in kerala , we hear@@ d you speak about re@@ plac@@ ing b@@ ou@@ qu@@ ets that we give as gi@@ f@@ ts , with good boo@@ ks as me@@ men@@ to@@ s .\n",
            "i say this , thin@@ king of the histor@@ ical value of such gi@@ f@@ ts .\n",
            "in a way , in our country the months of august and september are the months for celebr@@ ation of festivals .\n",
            "the en@@ su@@ ing wee@@ ks shall be wit@@ ness to the spar@@ k@@ le of festivals throu@@ gh@@ out the country .\n",
            "the way , the years from 194@@ 2 to 194@@ 7 were the deci@@ sive years for att@@ ain@@ ment through resolve .\n",
            "but when i read their le@@ tter , i re@@ alized how be@@ au@@ ti@@ fully a sym@@ b@@ ol@@ ic message was given .\n",
            "our par@@ ents , even today , in@@ si@@ st on foc@@ using on stu@@ dies and academ@@ ics .\n",
            "my greetings to all of you for the same .\n",
            "state governments , union government , local self - government bo@@ dies , social organ@@ is@@ ations and the citizens did wh@@ at@@ ever best could be done .\n",
            "our scientists launched the a - s@@ at in march . after that , am@@ id@@ st the h@@ ec@@ tic eng@@ ag@@ ements during elections , an important development such as the a - s@@ at could not be a prom@@ in@@ ent part of the disc@@ our@@ se .\n",
            "during these last days , many great aspects of at@@ al@@ ji came up to the fore .\n",
            "my dear countrymen , im re@@ fer@@ ring to the 15@@ 0th birth anniversary of mahatma gandhi .\n",
            "and today when two y@@ ou@@ ths of my country made india proud in the us , i thought of it worth men@@ tioning .\n",
            "i had the opportunity to interact for hours with the distingu@@ ished scientists of the country .\n",
            "and as a result , one over@@ ri@@ ding priority for every government has been to ensure that there is no bur@@ den on the pl@@ ate of the poor on account of gst .\n",
            "chhattisgarh has started the lok - sur@@ aj , j@@ al - sur@@ aj campaign .\n",
            "this is the nations strength .\n",
            "people are not only attr@@ ac@@ ted to yo@@ g the world over , they have im@@ plic@@ it faith in it and the whole world has em@@ br@@ ac@@ ed it .\n",
            "but when you present a book , it becom@@ es a part of the house@@ hold , a part of the family .\n",
            "my dear countrymen , i must express my grati@@ tude to you and to the people of the world for one more thing\n",
            "yo@@ g@@ esh d@@ an@@ de@@ kar from nag@@ pur , har@@ sh@@ v@@ ar@@ dhan ji from my@@ s@@ ore , pra@@ ve@@ en nad@@ kar@@ n@@ i ji , div@@ yan@@ sh@@ u gu@@ pt@@ a ji have all asked me to say something about the mon@@ soon in this session of mann ki baat .\n",
            "l@@ ata di@@ di : there are many who are el@@ der just by age , but greater are the bl@@ ess@@ ings of those who do great work , and h@@ ence are greater .\n",
            "if you know a poor person who is un@@ able to proc@@ ure treatment due to l@@ ack of money , do in@@ form him about this scheme .\n",
            "all of us have to take our nation to new@@ er heights .\n",
            "of the num@@ er@@ ous col@@ our@@ ful festivals of india , on@@ am is a prime festival of ker@@ el@@ a .\n",
            "o@@ h dear , i k@@ new the an@@ sw@@ er to that question but i c@@ oul@@ d@@ n@@ t remember it th@@ en@@ .@@ . .@@ .@@ .\n",
            "i would like to tel@@ l them with great pleasure that we have a separ@@ ate ministry for north - eastern region .\n",
            "that day too i had said that cleanliness should become our nature , a d@@ uty for every citizen and there should be an atmo@@ sphere har@@ b@@ our@@ ing a sense of re@@ v@@ ul@@ sion against fil@@ th .\n",
            "just two days back , i have written le@@ t@@ ters to all governments and all leaders of the world about the yoga day .\n",
            "at this summit , a resolution was taken expres@@ sing concern about the d@@ w@@ ind@@ ling ti@@ ger population in the world .\n",
            "by mis@@ guid@@ ing the poor , enti@@ c@@ ing them through the vic@@ es of av@@ ar@@ ice and temp@@ tation , wr@@ ong@@ fully pu@@ m@@ ping money into their bank accounts , or getting them to under@@ take some wr@@ ong activities , some people are tr@@ ying to sa@@ ve their black money .\n",
            "this waste can be utiliz@@ ed in fields , and if you will remember that fields are gre@@ en then you will remember what to place in the gre@@ en l@@ it@@ ter b@@ ins .\n",
            "if the ra@@ in water does not f@@ low away , it go@@ es into the earth , then the a@@ qu@@ i@@ f@@ ers get re@@ char@@ ged and the year long water w@@ o@@ es get resol@@ ved .\n",
            "twenty - four te@@ ams from all over the world are coming to play in our country .\n",
            "it appe@@ ars as if the subject of cleanliness has become so dear to him that even on his fa@@ thers birth anniversary he remembered the cleanliness mission .\n",
            "his life is an inspiration to us , our society and the whole country .\n",
            "there is a wond@@ erful confl@@ u@@ ence of a sense of service and satisfaction .\n",
            "now , this surve@@ y will be conduc@@ ted in about 500 cities with a population of more than 1 lakh . each city will im@@ bi@@ be a sense of confidence that , well , we have l@@ ag@@ ged be@@ hind but we will su@@ re@@ ly per@@ form better next time .\n",
            "there can be no greater d@@ on@@ ation than this .\n",
            "in the back@@ dro@@ p of the dist@@ ur@@ b@@ ing in@@ cid@@ ents taking place during the past few months , the soldiers of our defence forces have been sacrific@@ ing their all for our peace and happiness .\n",
            "therefore , this is an example that if there is an aim then even a small department can do a su@@ pre@@ me job .\n",
            "work is prog@@ res@@ sing at a very rapid pace in all of these states .\n",
            "they have s@@ ent me the im@@ age of mother india .\n",
            "besides dar@@ sha@@ n , all these amb@@ ass@@ ad@@ ors had the opportunity to know more about si@@ kh traditions and culture .\n",
            "the talent of bha@@ j@@ j@@ u sh@@ y@@ am ji , who made india proud in many nations ab@@ road , was also recogn@@ ized and he was awar@@ ded the p@@ ad@@ ma shri .\n",
            "it may give you t@@ em@@ por@@ ary succ@@ or , but we should comple@@ t@@ ely st@@ op taking anti@@ bi@@ o@@ tics without the adv@@ ice of a doc@@ tor .\n",
            "an in@@ j@@ ur@@ y can also oc@@ cu@@ r .\n",
            "there mi@@ ght be lakhs and lakhs of people around us .\n",
            "equality and equ@@ al opportunity , comp@@ as@@ sion and fr@@ at@@ ern@@ ity , these are the pa@@ ths leading us to a bri@@ ght future .\n",
            "and i did not have it in my mind that i would say something on this t@@ op@@ ic , but ab@@ hi has re@@ min@@ ded me that last year i had asked you to put an ear@@ then bo@@ w@@ l fill@@ ed with water for the bir@@ ds .\n",
            "i would like to tel@@ l you something more and that too from my heart .\n",
            "it was mo@@ st@@ ly played at the thre@@ sh@@ old of the cour@@ ts of em@@ per@@ ors and k@@ ings .\n",
            "she is utiliz@@ ing the solar energy of the su@@ n to ill@@ u@@ min@@ ate the houses of the poor .\n",
            "economic growth will be in@@ complete without a social transformation .\n",
            "i am sure you dist@@ inc@@ tly remember that for the last many months , a nav@@ al team com@@ pr@@ ising six women com@@ man@@ ders was on a vo@@ y@@ age .\n",
            "all around , there is a del@@ u@@ ge of strong fe@@ el@@ ings of sym@@ pa@@ th@@ y for the marty@@ rs their family members .\n",
            "i feel very g@@ la@@ d to tel@@ l you that we are also introduc@@ ing gold co@@ in , a gold co@@ in with ash@@ ok cha@@ k@@ ra .\n",
            "and this happ@@ ens every@@ where in life .\n",
            "this sc@@ our@@ ge and this ob@@ no@@ x@@ i@@ ous hab@@ it should not become roo@@ ted in our society .\n",
            "and from my personal experience i will say that the governments too needs to change their min@@ d@@ sets .\n",
            "modi ji : pr@@ an@@ a@@ am di@@ di .\n",
            "the tradition of public celebr@@ ation of gan@@ esh ut@@ sa@@ v is lo@@ k@@ man@@ ya ti@@ lak@@ s gi@@ ft to us .\n",
            "thus , you may find that desp@@ ite becoming br@@ il@@ li@@ ant in passing the ex@@ ams , you have some@@ times f@@ ail@@ ed in life .\n",
            "therefore it is appropriate that par@@ ents desi@@ ring their well - being should plant t@@ ree and re@@ ar them like their own children .\n",
            "what can be more satis@@ f@@ ying than this for any one and therefore , i thank everyone and express my grati@@ tude to all who contribu@@ ted in making it a successful programme .\n",
            "some people take mann ki baat as a mon@@ olo@@ gue and some cri@@ ti@@ ci@@ z@@ e it from a political an@@ g@@ le but after a long experience , i now feel , that when i had first started mann ki baat , i had not given thought to the fact that mann ki baat would make me a member of every family in india .\n",
            "my dear countrymen , 5@@ th september is teachers day .\n",
            "un@@ fortun@@ ately , over the decades we have s@@ li@@ pp@@ ed from there to the low@@ est r@@ un@@ gs .\n",
            "my dear countrymen , no men@@ tion of india can be complete without c@@ iting its festivals .\n",
            "its a matter of s@@ orrow for all of us that we lost kal@@ pan@@ a cha@@ w@@ la at that early age , but her life , her work is a message to young women across the world , especially to those in india , that there are no up@@ per lim@@ its for nar@@ i sha@@ kti .@@ .@@ .\n",
            "but the case of e - ci@@ g@@ are@@ t@@ tes is qu@@ ite different .\n",
            "almost the entire country is re@@ el@@ ing under the sc@@ or@@ ching impact of sev@@ ere he@@ at .\n",
            "he has written that on the 3rd of march , 19@@ 00 , the bri@@ tis@@ h ar@@ re@@ sted bir@@ s@@ a m@@ und@@ a , when he was just 25 years old . co@@ in@@ cid@@ ent@@ ally , the 3rd of march is also the birth anniversary of jam@@ set@@ ji t@@ ata .\n",
            "the country needs more and more scientists .\n",
            "there are many things that our fore@@ fa@@ thers have given to us which are the best and the entire world also has the right on those things .\n",
            "there the people are poor and the area is mal@@ ari@@ a inf@@ ested .\n",
            "i have been told about m@@ am@@ ta sh@@ arm@@ a from bh@@ op@@ al . she received rs 4@@ 0,000 from the mudra yojana .\n",
            "we have received qu@@ ite a few suggestions from su@@ ch@@ it@@ ra r@@ ag@@ ha@@ v@@ ac@@ hari from ch@@ en@@ na@@ i in this regard .\n",
            "the government of india w@@ ants to speci@@ ally re@@ ward technology developed to find solutions to our problems .\n",
            "these be@@ ti@@ s have once again prov@@ ed that they are second to n@@ one in any respect .\n",
            "ple@@ ase cha@@ t about o@@ ther@@ things with your family , talk of other t@@ op@@ ics , re@@ f@@ res@@ h your mind with some light ban@@ ter .\n",
            "all such poin@@ ts need to be p@@ ond@@ ered upon when we ob@@ serve engine@@ ers day .\n",
            "it is our responsibility that we can see the entire tradition of yoga on one platform from the uni@@ verse .\n",
            "if we plant on the b@@ ound@@ aries of our fields tim@@ ber tre@@ es that can be u@@ til@@ ised for construction of hom@@ es and fur@@ n@@ iture and which can also be cu@@ t and sol@@ d after 15 to 20 years with the per@@ mission of the government . this way , not only can these become a new source of your income , but can also sa@@ ve india from import@@ ing tim@@ ber .\n",
            "people have shared information on traditional me@@ th@@ ods .\n",
            "it is also natural that we remember our jaw@@ ans who sacrific@@ ed their lives to maint@@ ain the pride and honour of our tr@@ ic@@ ol@@ or f@@ la@@ g .\n",
            "the dis@@ satisfaction ar@@ ising out of success never becom@@ es a l@@ add@@ er to success it gu@@ aran@@ te@@ es f@@ ail@@ ure .\n",
            "if we try to take a bir@@ ds e@@ y@@ e view of thousands of years of the on@@ ward march of human life and progress , we can eas@@ ily say that this great journey has no point of cul@@ min@@ ation .\n",
            "he sho@@ wed the ri@@ gh@@ te@@ ous path of life to millions , inspir@@ ing them for centur@@ ies .\n",
            "along with that , he was also a@@ dep@@ t at organ@@ ising skills .\n",
            "pil@@ gr@@ ims go to have a dar@@ sha@@ n of v@@ it@@ th@@ al who is also known as vi@@ th@@ ob@@ a or p@@ and@@ ur@@ an@@ g .\n",
            "not only you and i , june 21st remains a part of the entire wor@@ l@@ ds collective con@@ sci@@ ous@@ ness .\n",
            "my hear@@ ti@@ est congratulations to all these play@@ ers .\n",
            "in france , yoga was per@@ formed in the vic@@ in@@ ity of the ei@@ f@@ fe@@ l to@@ w@@ er .\n",
            "my dear fel@@ low citizens , my hear@@ ti@@ est best wishes to you all .\n",
            "the occasion was the completion of 75 years of the formation of the az@@ ad hind government .\n",
            "gu@@ ests from the bri@@ de@@ gro@@ om@@ s side also gr@@ ac@@ ef@@ ul@@ ly accep@@ ted it as their cer@@ em@@ on@@ ial welcome .\n",
            "so many years have pas@@ sed and yet , u@@ een el@@ iz@@ ab@@ e@@ th has tre@@ as@@ u@@ red the hand@@ ker@@ chief gi@@ f@@ ted by mahatma gandhi .\n",
            "see , how the village life is also getting trans@@ formed .\n",
            "this disc@@ overy not only prov@@ es the the@@ ory of our gre@@ atest scienti@@ st of the previous century , al@@ ber@@ t e@@ in@@ ste@@ in , but is also consid@@ ered a great disc@@ overy for the world of phy@@ si@@ cs .\n",
            "i call upon all of you to de@@ fe@@ at di@@ ab@@ et@@ es which pl@@ ays ho@@ st to so many dise@@ ases .\n",
            "in re@@ ality they are true k@@ arm@@ yo@@ g@@ is , who are sel@@ fl@@ ess@@ ly eng@@ aged in public service , social service and , above all , in the service to the nation .\n",
            "therefore , we need to not only con@@ serve our fore@@ sts , fl@@ or@@ a and fa@@ un@@ a , but also create an environment wher@@ e@@ in they can fl@@ our@@ ish proper@@ ly .\n",
            "but when i see your support , when i look at your co - operation\n",
            "on 2nd march the entire country im@@ mer@@ sed in joy will celebrate the festival of hol@@ i .\n",
            "raj@@ at ag@@ ar@@ w@@ al has told us something good .\n",
            "the hear@@ t@@ ening effect of the nov@@ el way in which our 1 . 25 billion countrymen dedicated this di@@ wal@@ i to soldiers of our army , to our defence forces , was cle@@ arly e@@ vid@@ ent on their fac@@ es .\n",
            "it mak@@ es me so happy and it gi@@ ves me a new hope and i do express my happiness .\n",
            "we see problems in d@@ ail@@ y life .\n",
            "and h@@ ence all of them are a source of inspiration to us .\n",
            "well , i had never given a thought to a possible connec@@ tion between my s@@ le@@ ep , my spe@@ ec@@ h@@ es and my voice .\n",
            "my dear countrymen , in the current budget , emphasis has been laid on tur@@ ning waste to wealth and waste to energy through bi@@ o gas , under the swach@@ ch bharat campaign .\n",
            "and , he was not bor@@ n a div@@ yan@@ g .\n",
            "this mon@@ soon se@@ ason , there should be mass plant so@@ wing campai@@ gn@@ s conduc@@ ted by youth and social organizations .\n",
            "in the present 4@@ 7@@ 9 medical colle@@ ges , m@@ b@@ bs se@@ ats have been increased to about 6@@ 8 thousand .\n",
            "before the on@@ set of gst scheme , there were 17 different ty@@ p@@ es of tax@@ es prev@@ ail@@ ing in the country , but now only one tax is applic@@ able in the entire country .\n",
            "let us come together and make this a festival of resolve .\n",
            "we can understand this from b@@ ap@@ us life .\n",
            "gur@@ un@@ an@@ ak j@@ hi@@ ra sa@@ hi@@ b is a fam@@ ous place in b@@ id@@ ar dedicated to him , ch@@ er@@ i@@ sh@@ ing re@@ min@@ is@@ c@@ ences of him .\n",
            "the good thing is that g@@ ay@@ at@@ ri is expres@@ sing her own an@@ ger , and giving suggestions to me , but she also go@@ es on to say , that she has made several efforts but could not succ@@ e@@ ed .\n",
            "when some@@ body talks about the environment , one can eas@@ ily sense his or her cru@@ cial concerns on the matter at the same time , experi@@ ments that the person has att@@ emp@@ ted or seen also come to the sur@@ face , with vi@@ vi@@ d descri@@ p@@ tions of new ideas on the issue .\n",
            "this change brought about two important changes in indi@@ as politi@@ cs .\n",
            "the f@@ low@@ er head@@ s of the summ@@ er fru@@ it man@@ go mani@@ f@@ est themselves in the s@@ pr@@ ing se@@ ason itself .\n",
            ". the people of kashmir open hear@@ t@@ ed@@ ly bec@@ ame equ@@ al stakeholders in this festival .\n",
            "i hear@@ d it and i think , i will share it with you too .\n",
            "comp@@ as@@ sion should be the univers@@ al way of life .\n",
            "we shall now have to look for technological solutions for these .\n",
            "but the concern you have expressed is gen@@ u@@ ine .\n",
            "bi@@ o@@ gas generation will increase self - reli@@ ance in energy utiliz@@ ed for co@@ o@@ king and li@@ gh@@ ting .\n",
            "it is a matter of joy .\n",
            "last month i had the privile@@ ge of rele@@ as@@ ing the ti@@ ger c@@ en@@ sus in the country .\n",
            "on this day , women are also fel@@ ic@@ it@@ ated with ' nar@@ i sha@@ kti pur@@ as@@ kar ' who have per@@ formed ex@@ em@@ pl@@ ary t@@ as@@ ks in different sectors in the past .\n",
            "i can see that 2017 to 2022 pres@@ ents itself as a new time se@@ g@@ ment of five years for att@@ ain@@ ment through resolve .\n",
            "i hope that my farmer brothers and sisters will do@@ wn@@ lo@@ ad the k@@ is@@ an su@@ vid@@ ha app on their mobile p@@ hon@@ es .\n",
            "in ahmedabad in gujarat , around 55 thousand people per@@ formed yoga together and created a new world record .\n",
            "both te@@ ams per@@ formed p@@ ar excell@@ ence . besides , bo@@ w@@ l@@ er ra@@ shi@@ d k@@ han had per@@ formed ex@@ ce@@ ed@@ ingly well in the i@@ p@@ l earlier this year .\n",
            "and wh@@ en@@ ever i do that , i get happy that the dign@@ ity of the judi@@ ci@@ ary was very prou@@ d@@ ly hon@@ ou@@ red and no allow@@ ance for any he@@ ated disc@@ our@@ se or ten@@ sion was made any@@ where .\n",
            "if every citizen of my country is healthy , then my country will be healthy .\n",
            "today when india speak@@ s of climate justice or pl@@ ays a major role in the c@@ op@@ 21 and paris agreements or when we un@@ ite the whole world through the medium of international solar alliance , they all are roo@@ ted in fulfill@@ ing that very dream of mahatma gandhi .\n",
            "friends , you hear@@ d what t@@ end@@ ul@@ kar - ji had to say .\n",
            "the practi@@ ce of yoga lead@@ s to ger@@ min@@ ation of a sense of for@@ giv@@ eness in the same manner as a mother has for her children and mental peace becom@@ es our permanent friend .\n",
            "the su@@ dar@@ sha@@ n cha@@ k@@ ra be@@ aring mo@@ han left the banks of the y@@ am@@ un@@ a for the sea be@@ ach of gujarat , establishing him@@ self in the city of d@@ war@@ i@@ ka , while the mo@@ han bor@@ n on the sea be@@ ach reached the banks of the y@@ am@@ un@@ a , bre@@ a@@ thing his last in delhi .\n",
            "my dear countrymen , you are connected with us through mann ki baat .\n",
            "some times when i think of it , i remember so many in@@ cid@@ ents related with my teachers because they were our her@@ o@@ es in our small village .\n",
            "the kind of langu@@ age that was spo@@ k@@ en in order to generate ten@@ sion in the atmo@@ sphere\n",
            "l@@ ata di@@ di : all this is just the fru@@ it of the bl@@ ess@@ ings of my par@@ ents the bl@@ ess@@ ings of my l@@ ist@@ en@@ ers .\n",
            "but some@@ times , when the ra@@ in un@@ le@@ ash@@ es full force of its fur@@ y , we come to real@@ ise the ext@@ ent of the de@@ struc@@ tive mi@@ ght of water .\n",
            "but i came to know about two things from my people in the government .\n",
            "if teachers , par@@ ents and even senior students all comb@@ ine to form a team , a un@@ it with a common thin@@ king , and in a plann@@ ed way move forward , then ex@@ ams become a lot e@@ asi@@ er .\n",
            "the last five years have witn@@ essed - not only in delhi but in hund@@ re@@ ds of cities of india , union territ@@ ories , state cap@@ it@@ als , district centres , even in small cities bel@@ ong@@ ing to ti@@ er two or ti@@ er three categ@@ ories , inn@@ um@@ er@@ able men , women , be they the city fol@@ k , village fol@@ k , children , the youth , the el@@ der@@ ly , div@@ yan@@ g , all are particip@@ ating in@@ run for un@@ it@@ y@@ in large num@@ bers .\n",
            "keeping all this in mind , we have launched three major schemes .\n",
            "i do believe that for our countrymen a visit to the national war memorial will be ak@@ in to a pil@@ gr@@ im@@ age to a hol@@ y place .\n",
            "when the med@@ al was awar@@ ded to her , she said through this med@@ al i have ac@@ tu@@ ally de@@ fe@@ ated the dis@@ ability itself .\n",
            "one must start from some@@ where and it would su@@ re@@ ly turn out to be a great power .\n",
            "three world rec@@ or@@ ds in a single day by div@@ yan@@ g people is a matter of great pride for our countrymen .\n",
            "peop@@ les an@@ ger was at its pe@@ ak .\n",
            "sardar patel used to ob@@ serve even the min@@ ut@@ est of things in - dep@@ th\n",
            "i keep on ur@@ ging everyone to bu@@ y some khadi it@@ ems between gandhi jayanti to di@@ wal@@ i .\n",
            "inn@@ um@@ er@@ able such in@@ cid@@ ents come to light every@@ day through tel@@ e@@ vision , media , new@@ sp@@ ap@@ ers and mutual con@@ vers@@ ations .\n",
            "you c@@ ant im@@ ag@@ ine how you can serve the country in this way and become a brave soldi@@ er in the fight against black money and corruption .\n",
            "i am aware that this country is qu@@ ite vast and we have to reach the villages and far off cor@@ n@@ ers of the country .\n",
            "he has written i was very much dist@@ ressed after the ur@@ i attack and strongly felt like doing something .\n",
            "and we should express this concern on every public platform .\n",
            "to celebrate this festival in the entire indian society and around the world i also ur@@ ge you to sugg@@ est new ideas , new conc@@ ep@@ ts , and innov@@ ations on which we can p@@ ond@@ er and make prepar@@ ations and thus we can also celebrate the pr@@ ak@@ ash par@@ v as pr@@ ern@@ a par@@ v with great pride .\n",
            "it is a big challenge but you cannot run away from your responsi@@ bilities .\n",
            "he had tal@@ ked about the importance of feder@@ alism , feder@@ al system and stressed on cent@@ er and states working together for the up@@ lif@@ t@@ ment of the country .\n",
            "he has prov@@ en his capabilities as the ir@@ on - man of india .\n",
            "our air war@@ ri@@ ors ast@@ ound@@ ed everyone by per@@ forming yo@@ g@@ as@@ ans in mi@@ d sk@@ y , some 15 thousand fe@@ et above the earth .\n",
            "but for me the visit to south africa was more like a pil@@ gr@@ im@@ age .\n",
            "i am sure , they would be in@@ tri@@ gu@@ ed .\n",
            "he has asked me to discus@@ s this t@@ op@@ ic in mann ki baat .\n",
            "the gra@@ ve in@@ j@@ ur@@ y ca@@ used by terrorism could not de@@ ter him .\n",
            "once we start looking at gar@@ b@@ age and waste as a wealth , we shall also find new@@ er , techn@@ i@@ qu@@ es of waste management .\n",
            "i will try and interact with you again next month .\n",
            "my compli@@ ments to those who vis@@ u@@ alized this idea , compli@@ ments to these students and special compli@@ ments to those par@@ ents who took their chil@@ dr@@ ens le@@ t@@ ters so seri@@ ously and also took a decision to construc@@ t a to@@ il@@ et .\n",
            "once again , i thank all of you from the c@@ ore of my heart .\n",
            "in india , over the him@@ al@@ ay@@ as , across the indian ocean , from the lo@@ f@@ ty heights of si@@ ach@@ en to the dep@@ ths of a sub@@ mar@@ ine , from air - force to air@@ c@@ ra@@ ft carri@@ ers , from air - con@@ di@@ tioned gy@@ ms to ho@@ t des@@ er@@ t and from villages to cities - wher@@ ever possible , yoga was not just practi@@ ced every@@ where , but was also celebrated collec@@ tively .\n",
            "wh@@ en@@ ever we talk about nature and w@@ il@@ d@@ life , we only talk about conservation .\n",
            "i have even been t@@ ell@@ ing the farmers to plant tre@@ es on the b@@ ound@@ aries of their fields in@@ ste@@ ad of p@@ utting bar@@ ric@@ ades .\n",
            "per@@ ha@@ ps there is@@ n@@ t a district in india which i have not visited .\n",
            "you have prob@@ ab@@ ly known gur@@ u@@ de@@ v r@@ abin@@ d@@ ran@@ ath t@@ ag@@ ore till date as a writ@@ er and a mu@@ si@@ ci@@ an .\n",
            "these small trad@@ ers were provided loans worth lakhs and crores of rupees under mudra yojana because there are crores of such small trad@@ ers and they im@@ part momentum to the business activities worth bil@@ lions of rupees .\n",
            "the first necess@@ ity for a healthy life is cleanliness .\n",
            "i fel@@ ic@@ it@@ ate all such citizens involved in these k@@ in@@ ds of activities and especially congratulate friends from ali@@ garh .\n",
            "wh@@ en@@ ever you are there , do cap@@ ture im@@ ages and share them on social media so that others get inspired to come visit this sac@@ red s@@ ite with e@@ ag@@ ern@@ ess .\n",
            "but there is another inter@@ esting thing , some people ask me one thing al@@ be@@ it with some h@@ es@@ itation - modi ji , you were speaking in hin@@ di and be@@ ar gr@@ y@@ l@@ ls does not know hin@@ di , so how did you car@@ ry on such a fast con@@ vers@@ ation between the two of you\n",
            "pradhan mantri mudra yo@@ j@@ na , start up yo@@ j@@ na , stand up yo@@ j@@ na for d@@ al@@ its , adi@@ v@@ as@@ is , women , educ@@ ated youth , youth who want to stand on their own fe@@ et - for millions and millions through pradhan mantri mudra yo@@ j@@ na , they have been able to get loans from banks without any gu@@ aran@@ tee . they have been able to stand on their own fe@@ et and have succ@@ e@@ eded in giving employment to one or two other people as well .\n",
            "we all should come to know about all the yoga organizations , yoga teachers and all the necessary information about the yoga from this online program .\n",
            "so i just want to ask you this , c@@ ant this be trans@@ formed into a pleas@@ ant atmo@@ sphere\n",
            "just one message can enhance their cap@@ ability and our country has shown it .\n",
            "i am g@@ la@@ d that be it whe@@ at , be it pul@@ ses , be it o@@ il@@ se@@ eds , the data received till 20@@ th november indic@@ ates that so@@ wing has increased substanti@@ ally as comp@@ ared to so@@ wing during the last year .\n",
            "you too do@@ wn@@ lo@@ ad ap@@ ps of banks on your mobile p@@ hon@@ es .\n",
            "some excell@@ ent cre@@ ations were received in this competition .\n",
            "during run for un@@ it@@ y@@ not only do we have to run , but in doing so the spirit of f@@ it india is also refl@@ ected .\n",
            "there is no de@@ ar@@ th of talent in our country .\n",
            "there is generation of renew@@ ed energy all around us .\n",
            "gener@@ ally , very few people associated with the agricultural world or those very few who can be lab@@ el@@ ed as real farmers have ever f@@ ound their name in the l@@ ist of p@@ ad@@ ma@@ sh@@ ree awards .\n",
            "he is working on those issues .\n",
            "many sec@@ ts , communities , langu@@ ages , c@@ ast@@ es , at@@ ti@@ res , and it is this diversity that ad@@ ds to its char@@ m .\n",
            "see everyone is doing their b@@ it for taking the country forward .\n",
            "p@@ aw@@ an ga@@ ur@@ ai of bel@@ ag@@ av@@ i , sit@@ an@@ sh@@ u mo@@ han par@@ id@@ a of b@@ hu@@ ban@@ es@@ war , y@@ ash sh@@ arm@@ a , sha@@ ha@@ b al@@ ta@@ f and many others have written about the challenges related with water .\n",
            "some students mi@@ ght have jo@@ ined their respective colle@@ ges and some must be about to join .\n",
            "she had to face many difficul@@ ties , many problems .\n",
            "never rest till every why , what how are an@@ sw@@ ered .\n",
            "during a mat@@ ch in k@@ ol@@ k@@ ata one play@@ er from the op@@ pos@@ ite team h@@ it him on the he@@ ad with the h@@ oc@@ key sti@@ c@@ k .\n",
            "on this yoga day , since this was the third international day of yoga , i had asked you to share p@@ ho@@ to@@ s of three generations of the family doing yoga together .\n",
            "it is very important to understand the qu@@ estions .\n",
            "now the poor will not have to run for recommend@@ ation .\n",
            "i am sure that all of you will join me . this will su@@ re@@ ly benefit our poor ar@@ tis@@ ans , and arti@@ sts , and provide employment to those who make id@@ ols .\n",
            "under this scheme , you can de@@ posi@@ t your gold in a bank and the bank will give you interest , same as you de@@ posi@@ t your money and get interest from the bank .\n",
            "ambedkar had laid the foundation of team india ' s spirit in a way .\n",
            "he used to wr@@ ite po@@ ems on the w@@ alls of his pr@@ is@@ on cel@@ l .\n",
            "these are but living examples before your e@@ y@@ es@@ .@@ . .@@ .@@ . these very examples are a source of inspiration for the future of a rising glo@@ wing india .\n",
            "and , as we all know , what ter@@ ri@@ ble events took place after her ass@@ ass@@ ination .\n",
            "h@@ ence , the indian dias@@ pora has a significant role to play in promoting festival tourism in india .\n",
            "not just that , our entire rou@@ t@@ ine coll@@ ap@@ ses .\n",
            "take it further as it has a very major and prom@@ in@@ ent role in the fight against corruption and black money .\n",
            "that fil@@ es associated with netaji be dec@@ l@@ assi@@ fied has been a long standing demand for years . and i am happy that we could man@@ age to do the same .\n",
            "i did this and en@@ jo@@ yed it and in this way i made friends with a lot of bir@@ ds .\n",
            "i ur@@ ge all of you to share these traditional me@@ th@@ ods of water conservation .\n",
            "after the de@@ par@@ ture of pat@@ j@@ had , when old and wi@@ ther@@ ed le@@ a@@ ves f@@ all , new le@@ a@@ ves beg@@ in to s@@ pr@@ out on the tre@@ es .\n",
            "this no@@ tion is ba@@ si@@ c@@ ally un@@ found@@ ed . it is the roo@@ t cause of this proble@@ m .\n",
            "my dear countrymen , 2@@ 6th july is mar@@ ked as kar@@ gi@@ l vi@@ j@@ ay di@@ was in the history of our country .\n",
            "but have you hear@@ d about the sh@@ ining to@@ il@@ et cont@@ est\n",
            "i am of@@ ten wor@@ ri@@ ed by this thought that we speak to the youth of our nation and ask about their dream job , only one out of 100 would express their interest in becoming a scienti@@ st .\n",
            "and it was the cham@@ par@@ an sat@@ y@@ ag@@ ra@@ ha that brought to the fore , mahatma gand@@ his organ@@ is@@ ational skills , and his strong ability to g@@ au@@ ge the pul@@ se of indian society . mahatma gandhi , through his de@@ me@@ an@@ our and de@@ eds , could inspire the po@@ o@@ rest of the poor , the most ill@@ iter@@ ate , to un@@ ite and come together out into the open for the struggle against the bri@@ tis@@ h rule\n",
            "on the 2@@ 8@@ th of august 2014 , we had launched this campaign with a dream in our hear@@ ts .\n",
            "i would defin@@ it@@ ely want to tel@@ l our youn@@ ger generation that to even per@@ c@@ ei@@ ve what kind of tor@@ ture and tr@@ ou@@ ble these freedom figh@@ ters had suf@@ fer@@ ed to achieve freedom for us one must visit this cell@@ ul@@ ar j@@ ail .\n",
            "initi@@ ally i was sur@@ prised to see why they created a map of india using th@@ um@@ b prin@@ ts .\n",
            "but the p@@ ain remains that sardar patel , who li@@ ved for indi@@ as unity , st@@ ro@@ ve ti@@ rel@@ ess@@ ly for it , had to even end@@ ure some un@@ pleas@@ ant@@ ness on account of it , but never ever ab@@ and@@ on@@ ed the path of unity\n",
            "people who con@@ su@@ me to@@ b@@ acc@@ o are v@@ ul@@ ner@@ able to high ris@@ k dise@@ ases like canc@@ er , di@@ ab@@ et@@ es , b@@ lo@@ od pres@@ sure etc .\n",
            "this is 8 percent more than the last record set up by our farmers .\n",
            "come , le@@ ts march together as we do our b@@ it .\n",
            "and today i feel a great sense of ful@@ fil@@ ment , that within three years , the last man on the fr@@ ing@@ es of society has become a part of the ma@@ inst@@ re@@ am economy of the country . his ways have changed , he has now started going to the bank .\n",
            "do not keep your gold in your hom@@ es .\n",
            "just as di@@ ab@@ et@@ es can be the roo@@ t cause of all s@@ or@@ ts of dise@@ ases , de@@ pres@@ sion too , dest@@ ro@@ ys all our abilities to sust@@ ain , to fight , to disp@@ lay courage and to take a decision .\n",
            "but on the sugg@@ estion of our bro@@ ther akh@@ il@@ esh vajpay@@ ee , our government loo@@ ked into this sugg@@ estion seri@@ ously and we have implemented this facility in our system for our dis@@ ab@@ led brothers and sisters .\n",
            "you will be sur@@ prised and also happy to know that during this one week , these institutions received 13 thousand crore rupees .\n",
            "i am very grat@@ eful to you for your contribution .\n",
            "training and awareness have a very important role to play .\n",
            "my focus would be on the b@@ all , and s@@ low@@ ly - s@@ low@@ ly targets were achieved on their own .\n",
            "nc@@ c ca@@ de@@ ts , n@@ ss vol@@ un@@ te@@ ers , engineering students all together decided that the to@@ ile@@ ts will be construc@@ ted by them .\n",
            "this is the time which is pe@@ ak se@@ ason for colle@@ ges .\n",
            "and it is said that lakhs of years ago it was all one land@@ mass .\n",
            "san@@ to@@ sh gi@@ ri go@@ swami has written to me from j@@ od@@ h@@ pur something similar , almost along the same lines .\n",
            "my dear countrymen , i had the opportunity to visit south africa for the first time some time back .\n",
            "bhar@@ ta@@ hari has said that with regular yo@@ gic exerc@@ ise , tru@@ th becom@@ es our child , mer@@ c@@ y becom@@ es our sist@@ er , self re@@ str@@ ain@@ t our bro@@ ther , earth tur@@ n@@ s in to our bed and knowledge s@@ ati@@ ates our h@@ un@@ ger .\n",
            "what should the structure of the programme be like\n",
            "some@@ one just told me that there is one sha@@ il@@ esh b@@ ho@@ s@@ le in mumbai .\n",
            "an in@@ cident regarding a small gir@@ l from a remote village too can inspire the hundred and twenty five crore people .\n",
            "i know that the 10@@ th board ex@@ ams are going to commen@@ ce in a few days from now .\n",
            "they are wor@@ ri@@ ed about environment .\n",
            "you will noti@@ ce the be@@ aut@@ y of wor@@ d , s@@ enti@@ ment and thought in this po@@ e@@ m in k@@ ann@@ ada : ( pen@@ n@@ ind@@ a per@@ me@@ g@@ ond@@ an@@ u him@@ av@@ an@@ tan@@ u .\n",
            "speaking about festivals today , i would first like to wish you all mic@@ h@@ ha@@ mid@@ u@@ k@@ k@@ ad@@ am .\n",
            "i am sure you know about k@@ ab@@ ir@@ w@@ ad in gujarat when i used to work there , i had organized a national session of people bel@@ ong@@ ing to the tradition of sain@@ t k@@ ab@@ ir .\n",
            "and all these things are not necess@@ ar@@ ily achieved through the budget , government projects , or government money .\n",
            "friends , this programme was not a m@@ ere government form@@ ality that officials ro@@ am around the village and retur@@ n to b@@ ase , in@@ ste@@ ad of this the officials sp@@ ent two days and a ni@@ ght at the panchay@@ at , enab@@ ling them to sp@@ end time in the village and meet almost every d@@ en@@ iz@@ en of the village and to reach across to every establishment in the village .\n",
            "the establishment of a k@@ it@@ ch@@ en free of social dis@@ cr@@ im@@ ination where a person of any c@@ aste , sec@@ t , religi@@ on or community could e@@ at or what we know as the lan@@ gar@@ system was initi@@ ated sol@@ ely by guru nan@@ ak de@@ v ji .\n",
            "the river f@@ low@@ s under the ris@@ pan@@ a bridge before passing by my home .\n",
            "local people are not only getting innovative opportunities of employment but for the conven@@ i@@ ence of visiting tour@@ ists , many vill@@ ag@@ ers are providing facilities like home st@@ ays .\n",
            "some of them must have go@@ t@@ ten over with their examin@@ ations by now . for those of you who still have examin@@ ations , it must be a t@@ esting time with ex@@ ams on one hand and t - 20 cr@@ ick@@ et world c@@ up on the other .\n",
            "dear countrymen , as the time of mann ki baat dra@@ ws near , a very large number of suggestions are received on my@@ go@@ v or nar@@ end@@ ram@@ odi@@ app .\n",
            "this app is not only connec@@ ting the vill@@ ag@@ ers with the whole world but now they can ob@@ tain any information on their own mobile p@@ hon@@ es .\n",
            "jharkhand , al@@ thou@@ gh being a pre@@ dom@@ in@@ antly fo@@ rest area , still has some parts which face water proble@@ m .\n",
            "am@@ ir k@@ h@@ us@@ ro has por@@ tr@@ ay@@ ed these mo@@ ments of change of se@@ ason in a very inter@@ esting manner .\n",
            "rash@@ tr@@ am j@@ ag@@ r@@ y@@ am v@@ y@@ ay@@ am : means intern@@ al vig@@ il@@ ance is the pr@@ ize of liber@@ ty .\n",
            "if you do yoga , keep doing it .\n",
            "on one hand , farmers have a major role in the entire cha@@ in from cul@@ tiv@@ ation till the marketing of cr@@ ops , wher@@ e@@ as on the other hand , to make certain the farmers ' participation in re@@ ap@@ ing prof@@ its is an att@@ emp@@ t to gu@@ aran@@ tee their right .\n",
            "we could look at this as prepar@@ ations for di@@ wal@@ i , prepar@@ ations for nav@@ ar@@ at@@ ri , prepar@@ ations for d@@ ur@@ ga pu@@ j@@ a .\n",
            "i wish to be a part of this journey along with you .\n",
            "ro@@ om@@ a de@@ v@@ nath writ@@ es , i tru@@ ly feel happy healthy through my mor@@ ning wal@@ ks . she further ad@@ ds , for me , f@@ it@@ ness comes with a s@@ mil@@ e and we should s@@ mil@@ e , when we are happy .\n",
            "i was very happy to he@@ ar that people not only l@@ ist@@ en to mann ki baat but also remember it on many occa@@ sions .\n",
            "sardar patel was entr@@ u@@ sted with over@@ looking all the arran@@ ge@@ ments .\n",
            "si@@ r , as you know , 3@@ 1st october is sardar patel j@@ is bir@@ th@@ day .\n",
            "buddha pur@@ n@@ im@@ a is a special day for every indian . we must be proud of the fact that india is the bir@@ th@@ place of lord buddha , the very sy@@ n@@ on@@ y@@ m of power of comp@@ as@@ sion , service and sacrific@@ e , who gu@@ ided millions of people around the world . this buddha pur@@ n@@ im@@ a re@@ min@@ ds us of all of our o@@ bli@@ gation to em@@ ul@@ ate lord budd@@ has teach@@ ing as well as , p@@ ledge our@@ selves to follow his foo@@ t@@ steps .\n",
            "in a way , there are many events in the month of august that are closely associated with the history of our freedom movement .\n",
            "sh@@ r@@ im@@ an s . s@@ under ji has expressed his fe@@ el@@ ings on the role of par@@ ents .\n",
            "the 5@@ 5@@ 0th pr@@ ak@@ ash par@@ v of guru nan@@ ak de@@ v ji will be celebrated in 2019 .\n",
            "we must make 2017 our year of resolve .\n",
            "my dear countrymen , a few days ago , i read a very inter@@ esting com@@ ment on my go@@ v .\n",
            "i once again pay heart felt tributes to sist@@ er mar@@ i@@ am@@ thre@@ sia and congratulate the citizens of india , and especially our ch@@ ris@@ ti@@ an brothers and sisters , for this achiev@@ ement .\n",
            "right from si@@ r j@@ ag@@ dis@@ h chand@@ ra bo@@ se and har@@ go@@ b@@ ind k@@ h@@ ur@@ ana to sat@@ y@@ end@@ ran@@ ath bo@@ se have brought la@@ u@@ rel@@ s to india .\n",
            "recently , i had the opportunity to read a le@@ tter , which i feel , i should share with you .\n",
            "the process of lin@@ king with your thoughts through mann ki baat as a medium is a very important and personal journey for my@@ self .\n",
            "hel@@ l@@ o , this is vis@@ wan@@ a@@ than an@@ and .\n",
            "i will be sharing my hear@@ t@@ felt thoughts with one and all .\n",
            "and i have been told that almost half the beneficiaries are women , the mo@@ thers and the daugh@@ ters .\n",
            "the indian team did something that is ex@@ em@@ pl@@ ary to the whole world .\n",
            "in any cor@@ ner of the world , yoga enthusi@@ ast welcom@@ es the su@@ n as soon it r@@ ises and then there is the complete journey ending with s@@ un@@ set .\n",
            "let us also make our contribution in enhancing this campaign .\n",
            "our mother india , our country is a b@@ oun@@ ti@@ ful land\n",
            "ri@@ p@@ ud@@ am@@ an ji , once again , many than@@ ks to you .\n",
            "my dear brothers and sisters , i am sure all of you remember the signific@@ ance of the 3@@ 1st of october .\n",
            "la@@ x@@ mi@@ k@@ ut@@ ty is a teach@@ er in k@@ all@@ ar and still re@@ sides in a hu@@ t made of pal@@ m le@@ a@@ ves in a tribal tr@@ act am@@ id@@ st d@@ en@@ se fore@@ sts .\n",
            "i pay my tribu@@ te to the brave soldiers .\n",
            "about 15 - 20 days before as@@ ha@@ d@@ hi e@@ k@@ ad@@ ash@@ i war@@ kar@@ i or pil@@ gr@@ ims start the p@@ and@@ har@@ pur y@@ at@@ ra on foo@@ t .\n",
            "the ordin@@ ance is coming to an end but those 13 poin@@ ts which were to benefit farmers which is directly connected to the mon@@ et@@ ary issues of the farmers will be made into a law and will be implemented today itself so that the farmers do not suf@@ fer any lo@@ s@@ ses , especially mon@@ et@@ ary lo@@ s@@ ses . and h@@ ence the task of implementing the 13 poin@@ ts which was left in@@ complete in the earlier act is being ac@@ comp@@ lish@@ ed today .\n",
            "lak@@ h@@ w@@ ind@@ er singh ji , i am happy to he@@ ar your message .\n",
            "i get a vari@@ ety of le@@ t@@ ters , written by all s@@ or@@ ts of people .\n",
            "every citizen has a right to know about the stat@@ us of the cleanliness mission in his city and the government of india has provided a dedicated tel@@ ep@@ h@@ one number 196@@ 9 for this purpose .\n",
            "fir@@ st@@ ly , these gir@@ ls have created an im@@ age of mother india by th@@ um@@ b prin@@ ts on a huge pi@@ ec@@ e of clo@@ th .\n",
            "and friends , does our life come to a stand@@ still if we meet with f@@ ail@@ ure some@@ times .\n",
            "i received a le@@ tter recently which tou@@ ched my heart .\n",
            "the same s@@ enti@@ ments were expressed to me by many mo@@ thers and sisters when i was not even the prime minister .\n",
            "but those who are well acqu@@ ain@@ ted with san@@ sk@@ r@@ it sub@@ ha@@ sh@@ it@@ as ep@@ i@@ gram@@ m@@ atic vers@@ es , know very well that it is possible to make a cr@@ is@@ p , pre@@ c@@ ise stat@@ ement , using very few words through the us@@ age of sub@@ ha@@ sh@@ it@@ as . and since there is a sense of ge@@ o@@ grap@@ h@@ ical cultural bel@@ ong@@ ing , they are eas@@ y to understand ass@@ im@@ il@@ ate .\n",
            "things that sp@@ a@@ wn and spread a lot are very difficul@@ t to st@@ op at later st@@ ages .\n",
            "today , due to this ho@@ b@@ by , he gar@@ n@@ ered respect not only in india but the entire world .\n",
            "these are your banks and now you should never le@@ ave them .\n",
            "but we must all come together .\n",
            "wh@@ at@@ ever the old do@@ g@@ m@@ atic thin@@ king mi@@ ght have been , but the society now must be fre@@ ed from dis@@ cr@@ im@@ ination between our s@@ ons and daugh@@ ters .\n",
            "life does not get st@@ uc@@ k if we do not get results accord@@ ing to our expec@@ tations .\n",
            "if you were to do that , n@@ one would be able to st@@ op you from moving ahead . when you bre@@ ak your own record , you will not have to dep@@ end on others for happiness and satisfaction .\n",
            "it was on the 9@@ th of january , when our re@@ ver@@ ed mahatma gandhi retur@@ ned to india from south africa .\n",
            "vis@@ u@@ als of chand@@ ra@@ y@@ a@@ an ii lif@@ ting off fill@@ ed our country@@ men@@ s hear@@ ts with gl@@ ory , z@@ est and joy .\n",
            "some@@ body wat@@ ching from a dist@@ ance would gen@@ u@@ in@@ ely feel that fes@@ tivity is a sy@@ n@@ on@@ y@@ m for the indian way of life , and this is natural too .\n",
            "this has been prov@@ ed too .\n",
            "you must also visit sit@@ es associated with nature and w@@ il@@ d life and anim@@ als .\n",
            "sav@@ ar@@ kar j@@ is person@@ ality was full of special qu@@ al@@ ities\n",
            "some states , especially gujarat and andhra pradesh have made full use of technology .\n",
            "and this is very ob@@ vi@@ ous in a democratic country , but on the basis of as much i understand and as much i know , i ass@@ ure you that we are on the right tr@@ ack .\n",
            "our cha@@ in of mann ki baat has l@@ ent pace to the cleanliness campaign from time to time . similarly , efforts towards ensuring cleanliness have always inspired mann ki baat .\n",
            "if you have some experience , any information in this regard , if you have ever res@@ cu@@ ed any child from this dru@@ g ad@@ dic@@ tion , if you know of any ways and means to help , if any government official has played a good role , if you give me any such information , i will conve@@ y such efforts to the public and together we will try to create an environment in each family that no child ever thin@@ ks of cho@@ o@@ sing this vice out of sh@@ e@@ er fru@@ str@@ ation .\n",
            "you must have seen many a child , sh@@ y by nature , le@@ a@@ p up with z@@ est , the moment a g@@ ame beg@@ ins .\n",
            "people have been as@@ c@@ ending the ever@@ est for years many have man@@ aged to reach the pe@@ ak successfully .\n",
            "this will per@@ ha@@ ps become a big thing among friends .\n",
            "i would again like to congratulate all the people associated with this initiative and i would like to requ@@ est the poor bre@@ thre@@ n who have op@@ ened an account to never sev@@ ere this relationship .\n",
            "one of the suggestions given was to keep a du@@ st@@ b@@ in at every 100 me@@ ters and a p@@ utting in place a clean@@ ing system .\n",
            "we get the mantr@@ a to enable the society to fight against b@@ lin@@ d - faith through devo@@ tion and bha@@ kti .\n",
            "our space program has been possible due to inn@@ um@@ er@@ able young scientists of the country .\n",
            "\" housing for all \" or the \" house for everyone \" scheme invol@@ ves ge@@ o - t@@ ag@@ ging of about 40 lakh hom@@ es spread over 23 states .\n",
            "like yoga connec@@ ts body , mind , heart and sou@@ l : similarly it is connec@@ ting the world now .\n",
            "can we not re@@ move the bloc@@ k@@ age and clean the in@@ le@@ ts so that more water gets coll@@ ected\n",
            "for the s@@ ake of our nation , i believe all of us should come together and take a vo@@ w in getting ri@@ d of the di@@ r@@ t and fil@@ th from our country . on the occasion of vi@@ j@@ aya d@@ as@@ ham@@ i , we must take a vo@@ w to elimin@@ ate di@@ r@@ t and fil@@ th and we can do so .\n",
            "india tak@@ es great pride in the fact that tamil is the most ancient of world langu@@ ages . we indians also feel proud that from ve@@ di@@ c times to the modern day , san@@ sk@@ r@@ it langu@@ age has played a st@@ ell@@ ar role in the univers@@ al spread of knowledge .\n",
            "they do not labour for any hon@@ or , but their work insp@@ i@@ res us .\n",
            "after the completion of this campaign , the entire village celebrated this ac@@ comp@@ lish@@ ment just like a me@@ ga festival .\n",
            "ch@@ al@@ king out plans and de@@ vis@@ ing strate@@ g@@ ies was his c@@ ore for@@ te ' .\n",
            "in india , our soldiers too were doing yoga in si@@ ach@@ en on wh@@ ite sh@@ e@@ et of s@@ now and on sea too , wher@@ ever our nav@@ al ships are po@@ sted , the yoga program was being carried out by indian navy .\n",
            "i conve@@ y my hear@@ ty greetings to all fel@@ low indians and everyone across the world , especially the mus@@ li@@ m bre@@ thre@@ n at the adv@@ ent of this au@@ sp@@ ici@@ ous month of ram@@ az@@ an .\n",
            "there are a lot of simil@@ ar@@ ities am@@ ong@@ st us .\n",
            "this time too , we need to ensure that we do yoga our@@ selves and mo@@ tiv@@ ate our family , friends and all others from now itself to do yoga .\n",
            "the national soldiers memorial is a sym@@ bo@@ l of the nations grati@@ tude to those men who made the su@@ pre@@ me sacrific@@ e after we g@@ ained independence .\n",
            "ve@@ d@@ as , descri@@ be the earth and the environment as the basic sour@@ ces of energy .\n",
            "thousands of people under the jan dhan yojana are now eli@@ gible to take an over@@ dra@@ ft and they have avail@@ ed it too .\n",
            "our farmers in me@@ gh@@ al@@ aya , in the year 2015 - 16 , achieved record production as comp@@ ared to the last five years .\n",
            "she has also sec@@ u@@ red a good ran@@ k in the cl@@ at ex@@ am .\n",
            "if you d@@ on@@ t speak to your friends , wal@@ k al@@ one , and your face dro@@ ops f@@ li@@ pp@@ ing through the p@@ ages of so many boo@@ ks at the last moment , then you cannot have peace of mind .\n",
            "the good news for our young friends is that the fif@@ a under 17 world c@@ up is being organized in india , from the 6th to the 2@@ 8@@ th of october .\n",
            "i fel@@ ic@@ it@@ ate all the teachers in the country on this occasion . i also salute your sense of commitment towards science , education and students .\n",
            "there is a direct connect with nature .\n",
            "lak@@ sh@@ m@@ i ji is continu@@ ously serv@@ ing society with her knowledge of her@@ bal medic@@ ines .\n",
            "our rish@@ is , s@@ ages and sain@@ ts have laid emphasis on certain t@@ ene@@ ts yoga has prov@@ ed them in a con@@ cre@@ te manner .\n",
            "this dec@@ om@@ po@@ sed waste can be saf@@ ely hand@@ led qu@@ ite conven@@ i@@ ently and can be used as n@@ p@@ k which is a very us@@ eful fertiliz@@ er .\n",
            "from the far south , in mad@@ ur@@ ai , tamil nadu , ar@@ ul@@ mo@@ z@@ hi sar@@ v@@ an@@ an , a house@@ wi@@ f@@ e , s@@ ent me a le@@ tter . and what was in that le@@ tter\n",
            "do read them , get inspired and share similar inst@@ ances . i respec@@ t@@ fully salute all the lak@@ sh@@ m@@ is of india .\n",
            "may your bl@@ ess@@ ings remain there for me\n",
            "you will see that with our joint efforts , this movement will get a f@@ res@@ h boost , a new d@@ y@@ nam@@ ism .\n",
            "guru ra@@ vid@@ as ji was bor@@ n in the hol@@ y city of varanasi .\n",
            "expec@@ tations make the path difficul@@ t .\n",
            "come on , i inv@@ ite you all .\n",
            "my dear countrymen , for mann ki baat , i keep getting thousands of le@@ t@@ ters and com@@ ments from your side , on various plat@@ forms .\n",
            "i do not know if everyone li@@ k@@ es this or not , but i am s@@ aying it out of personal experience .\n",
            "the total number of vo@@ ters in india ex@@ ce@@ eds the entire population of eur@@ ope .\n",
            "for example , in order to ill@@ u@@ str@@ ate the signific@@ ance of the guru in ones life , it has been said : r - , u .\n",
            "i once again congratulate all play@@ ers of our team .\n",
            "they believe in living in the manner as pre@@ ach@@ ed by the sh@@ lo@@ ka of s@@ r@@ im@@ ad@@ bhag@@ v@@ ad g@@ ita ' yoga : k@@ arm@@ as@@ u k@@ au@@ sh@@ al@@ am ' , or excell@@ ence in action is yoga .\n",
            "m@@ ag@@ har was consid@@ ered un@@ hol@@ y but s@@ ant k@@ ab@@ ir@@ d@@ as never sub@@ sc@@ ri@@ bed to that view .\n",
            "this time many people have sugg@@ ested that i say something about the mon@@ so@@ ons .\n",
            "he started the se@@ w@@ age d@@ ress and cleanliness business .\n",
            "ac@@ tu@@ ally , we should understand the importance of these things , it will su@@ re@@ ly benefit you .\n",
            "as a society , as a nation , the 2 min@@ ute tribu@@ te of sil@@ ence on 3@@ 0th january at 11 am , should become our inst@@ inc@@ tive nature .\n",
            "my dear countrymen , sardar patel integrated the nation with the th@@ read of un@@ is@@ on .\n",
            "through mann ki baat today , i wish to share with students , my young friends , information on a very inter@@ esting competition .@@ .@@ . i inv@@ ite young bo@@ ys gir@@ ls to a qu@@ i@@ z competition .\n",
            "no civil society can t@@ ol@@ er@@ ate any kind of in@@ justice towards the wom@@ an - power of the country .\n",
            "we have bangladesh as our neighb@@ our .\n",
            "they should only mo@@ tiv@@ ate them to prep@@ are well for their ex@@ ams .\n",
            "now , if i start including all those ph@@ one c@@ alls , com@@ ments , wh@@ at@@ ever i could read or he@@ ar , many of which tou@@ ched my heart if i ex@@ cl@@ usi@@ vely speak about them , per@@ ha@@ ps ill requ@@ ir@@ e a few months to respon@@ d to them in some way or the other .\n",
            "not just this , when we go to a show@@ ro@@ om to bu@@ y a s@@ are@@ e , we d@@ on@@ t bar@@ ga@@ in , but when it comes to some@@ one poor , we just cannot re@@ si@@ st bar@@ g@@ aining .\n",
            "due to these road ac@@ cid@@ ents we are witn@@ ess@@ ing one de@@ ath every four min@@ utes in our country .\n",
            "my dear countrymen , i want to speci@@ ally express my inde@@ b@@ t@@ ed@@ ness to you .\n",
            "and when there is a fes@@ tive mo@@ od of celebr@@ ation , the best within us comes out .\n",
            "it is indeed an extremely jo@@ y@@ ous experience to im@@ bu@@ e the ess@@ ence of bhag@@ w@@ an krish@@ na re@@ vel bl@@ is@@ s@@ fully .\n",
            "mo@@ re@@ over , when i tri@@ ed to use un@@ f@@ air means to p@@ ass the ex@@ am , i got c@@ au@@ ght and a lot of my friends around me also had to under@@ go dist@@ ress because of me .\n",
            "it is played with co@@ w@@ ries or tam@@ ar@@ ind se@@ eds or di@@ ce on an eight by eight s@@ qu@@ are board .\n",
            "once again i pay my homage to a great leader like him .\n",
            "the building mater@@ ial ne@@ eded to construc@@ t the to@@ ile@@ ts whether it was br@@ ic@@ ks or cement , the entire construction mater@@ ial was carried by the young men on their shoul@@ ders , sp@@ ending an entire day wal@@ king in those fore@@ sts .\n",
            "all i would like to say to you is be cal@@ m , en@@ joy life , seek inn@@ er happiness in life .\n",
            "various interest groups were play@@ ing games to take adv@@ ant@@ age of that situation in their own way\n",
            "they , by their di@@ li@@ gence , are pow@@ ering positive changes in the lives of others in my@@ ri@@ ad ways .\n",
            "he has written in g@@ it@@ an@@ j@@ al@@ i he , who has the knowledge has the responsibility to im@@ part it to the students . i do not know bangl@@ a but had the hab@@ it of rising early since my chil@@ d@@ hood and in eastern india , radio broad@@ c@@ ast@@ s start early while in w@@ est@@ ern india these start a b@@ it l@@ ate .\n",
            "' e@@ k@@ ta , we all are proud of you .\n",
            "this is what pr@@ ab@@ ha@@ kar re@@ d@@ d@@ y has said , something which i would have per@@ ha@@ ps da@@ red not to say because i am not very particular when it comes to s@@ le@@ ep@@ ing . my friends com@@ pl@@ ain to me of@@ ten that i s@@ le@@ ep very l@@ it@@ t@@ le .\n",
            "my dear countrymen , once again let us dedic@@ ate this di@@ wal@@ i to our jaw@@ ans . my best wishes and greetings on di@@ wal@@ i to all of you .\n",
            "today the fif@@ th one day cr@@ ick@@ et mat@@ ch is being played between india and south africa in mumbai .\n",
            "it is a fe@@ ature in the life of most of the successful play@@ ers that they compe@@ te with themselves .\n",
            "you have to vol@@ un@@ tar@@ ily l@@ end your leadership to this great campaign , this ma@@ ha abhiyan , to create a ca@@ sh@@ less society , to er@@ a@@ dic@@ ate corruption from our country , to ab@@ ol@@ ish the sc@@ our@@ ge of black money and to help people in over@@ coming their difficul@@ ties and problems .\n",
            "an air india bo@@ e@@ ing j@@ et with an all wom@@ an cre@@ w led by k@@ sha@@ mat@@ a vajpay@@ ee f@@ le@@ w from delhi to s@@ an fr@@ anc@@ is@@ co , us@@ a and back .\n",
            "raj@@ k@@ u@@ mar@@ i de@@ vi added 300 women of her area to a ' self help group ' and mo@@ tiv@@ ated the entire lot become finan@@ ci@@ ally self - reli@@ ant .\n",
            "on the 2nd of october , p@@ lo@@ gg@@ ing for two k@@ il@@ om@@ et@@ ers and it will be held across the country .\n",
            "today , we have adop@@ ted in all aspects of governance the mantr@@ a of co - oper@@ ative feder@@ alism and going a step further , we have adop@@ ted competitive cooperative feder@@ alism but most import@@ antly , dr . bab@@ a sa@@ he@@ b ambedkar is an inspiration for millions of people like me , who bel@@ ong to back@@ ward cl@@ ass@@ es .\n",
            "all of us cr@@ ick@@ et l@@ ov@@ ers know br@@ ad@@ man@@ s name very well .\n",
            "we should keep away from the idea of separ@@ atis@@ m or the at@@ ti@@ tude of separ@@ atis@@ m and also gu@@ ard the country from any such ide@@ olo@@ gy .\n",
            "i speci@@ ally congratulate the haryana government for establishing a connect with the farmers to we@@ an them away from conv@@ enti@@ onal mod@@ es of farm@@ ing towards cr@@ ops that do not requ@@ ir@@ e much water .\n",
            "i see that in the experiences that people have shared with me , they have spo@@ k@@ en a lot about yoga .\n",
            "under the a@@ e@@ g@@ is of ' go@@ bar@@ d@@ han@@ yojana ' , many benefits will ac@@ cr@@ ue to rural areas .\n",
            "i extend my good wishes to you on this pi@@ ous festival .\n",
            "many individu@@ als taking initiative on their own be@@ half shared innovative ideas , held discussions , conduc@@ ted po@@ e@@ try rec@@ it@@ als .\n",
            "every place in india is re@@ ple@@ te with sign@@ posts of heritage .\n",
            "indi@@ as art culture has been por@@ tr@@ ay@@ ed here in a very attr@@ active way .\n",
            "once there is an@@ ger , dis@@ satisfaction , r@@ age , we will be comp@@ el@@ led to act against this sc@@ our@@ ge .\n",
            "swach@@ ch bharat is already our resolve besides this , how can our 125 crore countrymen pay the best tribu@@ te to gand@@ hi@@ ji by working in close harmony\n",
            "if you look into this in@@ cident min@@ ut@@ ely , one thing will stri@@ ke you and that is the young man living in americ@@ a ad@@ her@@ ing to that country@@ s life - st@@ y@@ le and ide@@ olo@@ gy , who left india many years ago but , kno@@ ws the fin@@ er deta@@ ils about his village , kno@@ ws about the challenges and still is em@@ o@@ tion@@ ally att@@ ach@@ ed to his village .\n",
            "and these are the people who have sp@@ ent themselves in service of others .\n",
            "and besides india , these satell@@ ites are of france , germany , it@@ al@@ y , japan , br@@ it@@ ain and americ@@ a , nearly 14 such countries .\n",
            "i am happy that in a very short sp@@ an of time all these schemes have been accep@@ ted in large num@@ bers .\n",
            "let me tel@@ l an@@ ag@@ ha , jay@@ esh other children that these le@@ t@@ ters en@@ li@@ ven me up after a hard days work .\n",
            "a lot of public education programmes are being conduc@@ ted but we do not pay attention . we feel that we live in very good houses with prop@@ er arran@@ ge@@ ments being to@@ t@@ ally un@@ aware that water is collec@@ ting at some loc@@ ation in the house and that we are inv@@ iting d@@ eng@@ ue .\n",
            "in 196@@ 9 , we celebrated his birth cent@@ en@@ ary and in 2019 we are going to celebrate the 15@@ 0th birth anniversary of mahatma gandhi .\n",
            "ir@@ respective of age group or area of work , people are contribu@@ ting .\n",
            "it was such a stri@@ king p@@ ho@@ to that my attention was m@@ ag@@ ne@@ tic@@ ally dra@@ wn towards it .\n",
            "let us distribu@@ te joy , where there is a pa@@ uc@@ ity of it@@ .@@ . .@@ .@@ .\n",
            "and as i mentioned , our country is bl@@ essed with millions and millions of the bri@@ ght@@ est of bra@@ ins .\n",
            "thank you for read@@ ing ex@@ am war@@ ri@@ ors more than tw@@ ice and special than@@ ks for po@@ int@@ ing out shor@@ t@@ com@@ ings in the book .\n",
            "our society must recogn@@ ize such daugh@@ ters , hon@@ or them and feel proud of them .\n",
            "it is not like that dear friends .\n",
            "on the 2@@ 3rd of january , the country celebrated the birth anniversary of netaji in an un@@ common , special manner .\n",
            "on one hand , where we have professionals and engine@@ ers , on the other hand , there ex@@ ist farmers til@@ ling the fields , our bre@@ thre@@ n - sisters associated with agriculture .\n",
            "there is a lot within you , the tre@@ as@@ ure as a result of the hard work of a whole year is fill@@ ed in@@ side you , but if your mind is tr@@ ou@@ b@@ led , you will not be able to disc@@ over that wealth .\n",
            "and , she has said that a le@@ tter from the prime minister was no less than a p@@ ad@@ ma sh@@ ree honour for her .\n",
            "there id inaugurated a state of the art museum dedicated to mahatma gandhi .\n",
            "cleanliness is one of the strong@@ est protec@@ tions from dise@@ ase .\n",
            "these are discussed collec@@ tively in the country and across the world .\n",
            "this time when i had come to mumbai , i felt like meeting you in person .\n",
            "gold can become the financial property of the nation and every indian must contribute to this .\n",
            "this is the birth anniversary of the great h@@ oc@@ key play@@ er , h@@ oc@@ key w@@ iz@@ ard , major d@@ hy@@ an ch@@ and ji .\n",
            "it will be e@@ asi@@ er to keep the village clean and san@@ iti@@ z@@ ed , li@@ vest@@ ock health will improve and far@@ m y@@ i@@ el@@ ds will increase .\n",
            "these shor@@ t@@ cu@@ t ways become the re@@ ason for using un@@ f@@ air means .\n",
            "in the health sector the nation has now moved ahead from the conv@@ enti@@ onal approach .\n",
            "you just need to f@@ ill it once or tw@@ ice in a month .\n",
            "have we ever thought why water acqu@@ i@@ res col@@ our in ri@@ vers and seas\n",
            "wher@@ e@@ as me@@ gh@@ a j@@ ain has mentioned that a 9@@ 2 year old wom@@ an has been offer@@ ing free dr@@ in@@ king water to pass@@ eng@@ ers at g@@ w@@ ali@@ or railway station .\n",
            "this means that now same situation exist@@ s as it was pri@@ or to the formation of my government .\n",
            "this is why diversity is the mantr@@ a of unity .\n",
            "from over bri@@ d@@ ges to school w@@ alls to h@@ ut@@ ments he gave a free hand to his talent , gar@@ n@@ ering huge public support on the way , on the lines of an effective campaign .\n",
            "the government is continu@@ ously making efforts in this regard .\n",
            "i thank all of you once again .\n",
            "but it happ@@ ened for the first time that when our daugh@@ ters did not succ@@ e@@ ed in w@@ inning the world c@@ up , the hundred and twenty - five million people took this de@@ fe@@ at on their own shoul@@ ders , never le@@ t@@ ting the bur@@ den w@@ ei@@ gh down these daugh@@ ters of ours .\n",
            "i am not s@@ aying that you should keep on s@@ le@@ ep@@ ing .\n",
            "public cleanliness must be in@@ sist@@ ed upon not just in our hom@@ es but in our villages , town@@ s , cities , states and in our entire country cleanliness has to be in@@ ex@@ tr@@ ic@@ ab@@ ly linked to our festivals .\n",
            "the festival of r@@ ak@@ sha@@ band@@ han sym@@ b@@ ol@@ iz@@ es the b@@ ond of lo@@ ve trust between a bro@@ ther a sist@@ er .\n",
            "there are ad@@ ver@@ ti@@ se@@ ments in the new@@ sp@@ ap@@ ers but it es@@ cap@@ es our attention .\n",
            "this tre@@ mend@@ ous participation of youth and women is a major gl@@ ori@@ ous di@@ men@@ sion in is@@ ro@@ s success .\n",
            "the presid@@ ents , prime ministers , cel@@ e@@ bri@@ ties and ordinary citizens of many countries of the world sho@@ wed me on the t@@ wit@@ ter how they celebrated yoga in their respective nations .\n",
            "you must focus on how to improve upon your past performance and how to per@@ form even better .\n",
            "this week i had the opportunity to meet our daugh@@ ters , the members of our w@@ o@@ men@@ s cr@@ ick@@ et team .\n",
            "once again , i congratulate these daugh@@ ters and their spirit of adv@@ ent@@ ure for bringing la@@ u@@ rel@@ s to the country , for ra@@ ising the gl@@ ory of the navy and signific@@ antly so , for conve@@ ying to the world that indi@@ as daugh@@ ters are no less .\n",
            "just a few days ago , a historic cr@@ ick@@ et mat@@ ch took place in beng@@ al@@ ur@@ u .\n",
            "but there@@ in li@@ es a pleas@@ ant and inter@@ esting experience too .\n",
            "the stand up scheme and jan dhan account scheme are aimed at fre@@ e@@ ing these people from the cl@@ ut@@ ches of no@@ t@@ ori@@ ous mone@@ y@@ l@@ end@@ ers .\n",
            "let us also follow the light of knowledge be@@ sto@@ wed upon us by guru nan@@ ak de@@ v which insp@@ i@@ res us to end social di@@ spar@@ ities , exhor@@ ts us to do our b@@ it to fight against the e@@ vil of di@@ spar@@ ity .\n",
            "and the best part is , what she has written in this le@@ tter is very inter@@ esting .\n",
            "and , the an@@ sw@@ er is no .\n",
            "my dear countrymen , this great land , india , has given birth to inn@@ um@@ er@@ able great men , men whose de@@ eds for the s@@ ake for humanity have been ex@@ tra@@ ordinary , tru@@ ly un@@ for@@ get@@ table .\n",
            "last time i had completed a year in power . this year we are ent@@ ering a new year .\n",
            "there are some hom@@ es , ba@@ th@@ ed in sh@@ im@@ m@@ ering li@@ ghts on the other hand , just close by , there are some hom@@ es m@@ ired in dar@@ k@@ ness .\n",
            "once again , i pay tribu@@ te to this great man .\n",
            "ple@@ ase speak about this on mann k@@ i@@ baat .\n",
            "i am very exc@@ ited about my br@@ it@@ ain tri@@ p this time .\n",
            "the media also made this festival of li@@ ghts into an opportunity and an occasion to express grati@@ tude towards our armed forces . and why should it not be so\n",
            "but there is the second asp@@ ect which gi@@ ves me immense pleasure , happiness and a sense of satisfaction that the general public has started fe@@ el@@ ing that le@@ ave what happ@@ ened in the past , now they will not di@@ r@@ ty their sur@@ r@@ ound@@ ings .\n",
            "some t@@ v chan@@ n@@ els also took this idea forward .\n",
            "there are times when one has to face an ill@@ ness that le@@ a@@ ves a sc@@ ar for a li@@ fe@@ time .\n",
            "the way med@@ ve@@ de@@ v he@@ ap@@ ed pra@@ ise on his op@@ p@@ on@@ ent after a de@@ fe@@ at in a fi@@ er@@ ce du@@ el mak@@ es him a living example of the true ess@@ ence of the spirit of spor@@ t@@ s@@ man@@ ship .\n",
            "c@@ ant my doc@@ tor brothers and sisters sp@@ are just 12 days in a year for this service to the under privile@@ ged\n",
            "today , no@@ body , nei@@ ther me , n@@ or the government , n@@ or you , n@@ or even the previous government k@@ new how much money is st@@ ash@@ ed ab@@ road .\n",
            "i am g@@ la@@ d that now the villages of el@@ ep@@ han@@ ta and the ca@@ ves of el@@ ep@@ han@@ ta will be li@@ ghted due to electri@@ fication .\n",
            "i consid@@ er this to be an opportunity to let the world know of indi@@ as youth power not in the sense of w@@ inning or lo@@ sing a mat@@ ch .\n",
            "but the goal remains the same , and that is to sa@@ ve water and adop@@ t water conservation .\n",
            "community service and community mobil@@ ization are vir@@ tu@@ es which we have to im@@ bi@@ be into our real lives .\n",
            "they should supply this to the farmers who are w@@ ill@@ ing to adop@@ t organ@@ ic farm@@ ing .\n",
            "in other words , the gu@@ aran@@ tee for the r@@ ace of development li@@ es in the r@@ ace of unity .\n",
            "about 15 years back natural cal@@ am@@ ity was a part of the agricultural ministr@@ ys real@@ m because most of the cal@@ am@@ ities were limited to fa@@ min@@ e .\n",
            "i am not able to men@@ tion every state but all des@@ er@@ ve to be appreciated .\n",
            "i used to work as a vol@@ un@@ te@@ er there during those days , i got a ch@@ ance to go to a village and had a ch@@ ance to meet a l@@ ad@@ y of more than a hundred years of age . and she was looking at me and mo@@ c@@ k@@ ingly , s@@ aying , look at my house .\n",
            "this step will prove to be the one to move forward our march towards achieving the goal of social justice .\n",
            "for farmers , the se@@ ason just before on@@ set of farm@@ ing is of ut@@ most importance .\n",
            "and he ac@@ comp@@ lish@@ ed all this in his ra@@ ther short life .\n",
            "i always feel that if we look around us , some@@ where or something good always happ@@ ens .\n",
            "a lot of what you say is of great help to me in the func@@ tioning of the government .\n",
            "the pre@@ lim@@ in@@ ary information that i have , noti@@ fi@@ es me that from launch till date around 11 crore families have jo@@ ined this scheme .\n",
            "the people here , once again , are ready to play their active role in water conservation .\n",
            "but , i hope you are aware that i am just as common a being as you are .\n",
            "the in@@ human m@@ ass@@ ac@@ re at j@@ alli@@ an@@ w@@ all@@ a b@@ ag@@ h , provided a new inspiration and mission in life to that young te@@ en@@ ag@@ er , who un@@ ti@@ l then had sp@@ ent his days play@@ ing mer@@ r@@ ily in his fields .\n",
            "be it 26 , ali@@ pur road , associated with babasaheb ambedkar , or the sardar patel museum or the k@@ ran@@ ti man@@ di@@ r\n",
            "particularly for my farmer brothers and sisters , our new satellite car@@ to@@ s@@ at 2@@ d will be immen@@ sely hel@@ p@@ ful on a whole lot of subjec@@ ts such as know@@ ing how much water is there in our existing water sour@@ ces , how this should be best put to use , what things to keep in mind in this regard .\n",
            "ra@@ ther , it will give you the strength and confidence to try har@@ der and better the next time .\n",
            "i dedicated that ca@@ p to the museum itself , for vis@@ it@@ ors to see and be inspired with the spirit of pat@@ ri@@ o@@ tis@@ m .\n",
            "my journey from i to we .\n",
            "therefore keeping the importance of water in mind , a new j@@ al@@ as@@ ha@@ kti ministry has been created in the country .\n",
            "an e - ci@@ g@@ are@@ t@@ te is a ty@@ pe of electron@@ ic de@@ vice un@@ like a ty@@ p@@ ical ci@@ g@@ are@@ t@@ te .\n",
            "these awards also attr@@ act attention in new@@ sp@@ ap@@ ers and tel@@ e@@ vision also .\n",
            "modi ji : pr@@ an@@ a@@ am , di@@ di .\n",
            "along with this , they are under@@ going a training course in skill development .\n",
            "what does a w@@ inning team do while recei@@ ving the tr@@ op@@ hy\n",
            "he ke@@ eps im@@ par@@ ting knowledge about digital us@@ age to his pass@@ eng@@ ers all the time .\n",
            "these brave hear@@ ts made the su@@ pre@@ me sacrific@@ e in sec@@ uring the lives of a hundred twenty five crore indians .\n",
            "wher@@ e@@ as the fact is , through the a - s@@ at , we have acqu@@ ired the cap@@ ability of dest@@ ro@@ ying a satellite three hundred k@@ il@@ om@@ et@@ res away in a m@@ ere three min@@ utes .\n",
            "witn@@ ess@@ ing the mo@@ on means that the festival of ei@@ d can be celebrated .\n",
            "when i was a child we would be sc@@ ared by the very men@@ tion of the wor@@ d t@@ b .\n",
            "during the con@@ vers@@ ation with one of them , a vis@@ ually challeng@@ ed young man , and he mentioned that he happ@@ ened to be a stage arti@@ st , and speci@@ alized in m@@ im@@ ic@@ ry during enter@@ t@@ ain@@ ment program@@ s , so just like that i asked him as to who all he could im@@ it@@ ate\n",
            "t@@ ab@@ le@@ us dep@@ ict the issues of social concern in a very arti@@ sti@@ c manner .\n",
            "i would ur@@ ge my fel@@ low countrymen , who have hear@@ d g@@ ay@@ at@@ r@@ is message in my mann ki baat today , that it should be a message to all of us .\n",
            "in such an environment , when the common man comes to you tal@@ king about emerging hope , new z@@ e@@ al and events that have taken place in his life , it is not to the government ' s credit .\n",
            "he had wan@@ ted india to develop he@@ av@@ y industries and also pay full attention to ms@@ me , hand@@ lo@@ om , tex@@ til@@ es and co@@ tt@@ age industry .\n",
            "bhag@@ w@@ an bir@@ s@@ a m@@ und@@ a took on the mi@@ ght of the g@@ un@@ s c@@ ann@@ ons of the bri@@ tis@@ h , using traditional bo@@ ws and ar@@ ro@@ ws , to sha@@ ke them ap@@ art .\n",
            "you can im@@ ag@@ ine the inn@@ er bl@@ is@@ s of pay@@ ing homage to our re@@ ver@@ ed b@@ ap@@ u , with fif@@ t@@ een days of this cleanliness campaign , swach@@ ch@@ ata hi sev@@ a , when we celebrate gandhi jayanti on the 2nd of october .\n",
            "i would like to conve@@ y to par@@ ents that we do ex@@ ac@@ tly the same with our children .\n",
            "i will remember that meeting fore@@ ver .\n",
            "in a way it will a natural part of the overall lear@@ ning cur@@ ve . india is a land of ge@@ o@@ grap@@ hi@@ c and clim@@ atic di@@ versities .\n",
            "now i feel as if im con@@ vers@@ ing with my family while sit@@ ting at home .\n",
            "the festival of r@@ ak@@ sha band@@ han is a festival of special signific@@ ance .\n",
            "i will meet you all next at 11 am on s@@ und@@ ays , but i trust our journey shall never end and will continue recei@@ ving lo@@ ve and suggestions from you .\n",
            "it gi@@ ves the message of lo@@ ve and harmony - aw@@ ak@@ ens new hop@@ es and aspirations , and gi@@ ves new confidence to the people .\n",
            "i appreci@@ ate the election commission for rel@@ ent@@ l@@ ess@@ ly stri@@ ving to ensure the strengthening of our democracy .\n",
            "your st@@ aying healthy is very important to make india healthy .\n",
            "the ash@@ a work@@ er jam@@ un@@ a man@@ is@@ in@@ gh decided that she will not let any@@ one di@@ e of mal@@ ari@@ a .\n",
            "my countrymen , i would like to say one more thing to you .\n",
            "she f@@ ou@@ ght with mal@@ ari@@ a and pre@@ pa@@ red the entire village to fight against it .\n",
            "my dear countrymen , our san@@ sk@@ r@@ it sub@@ ha@@ sh@@ it , ep@@ i@@ gram@@ m@@ atic vers@@ es are , in a way , g@@ ems of w@@ is@@ dom .\n",
            "one such sain@@ t was - s@@ ant ra@@ vid@@ as .\n",
            "i do not wish that wh@@ at@@ ever i speak from the ram@@ parts of red fort should just be the op@@ in@@ i@@ on of the prime minister\n",
            "recently i came across a wond@@ erful in@@ cident , which i would like to share with you .\n",
            "anim@@ als , bir@@ ds , fl@@ or@@ a nature in its enti@@ re@@ ty , is fill@@ ed with the jo@@ ys of the se@@ ason .\n",
            "i keep getting your suggestions through tel@@ ep@@ h@@ one and my@@ go@@ v . in .\n",
            "i am from gujarat . i known of a g@@ ame played in gujarat called ch@@ om@@ al ist@@ o .\n",
            "i had never thought that there would be an opportunity in my life to tou@@ ch the hear@@ ts of young people around the world .\n",
            "i hope that you will now be even more acti@@ vely and enthusias@@ tic@@ ally involved in ensuring that our democracy should function with peop@@ les participation .\n",
            "i have read it several times that there are offic@@ es where on a regular basis , when they first meet in the mor@@ ning , they beg@@ in with yo@@ g and pr@@ an@@ ay@@ am and the efficiency of the entire office increas@@ es . the whole culture of the office gets trans@@ formed and the environment also under@@ go@@ es a positive change .\n",
            "the 3@@ 0th of january is the de@@ ath anniversary of our re@@ ver@@ ed b@@ ap@@ u , who sho@@ wed us a new path .\n",
            "h@@ ence , i would like to salute all these val@@ i@@ ant war@@ ri@@ ors on this occasion of kar@@ gi@@ l vi@@ j@@ ay di@@ was .\n",
            "i wish and hope that this festival fore@@ ver remains a festival of col@@ our@@ ful ch@@ e@@ er in the lives of all the countrymen - this is my wish .\n",
            "friends , as@@ ha@@ d@@ hi e@@ k@@ ad@@ ash@@ i , which fel@@ l on 2@@ 3rd july , is celebrated as a day of gr@@ and transformation of p@@ and@@ har@@ pur war@@ i .\n",
            "the bri@@ tis@@ h were so af@@ r@@ aid of lok man@@ ya ti@@ la@@ k that they tri@@ ed to charge him of se@@ dition th@@ ric@@ e in two decades this is no small thing .\n",
            "climate change , al@@ tered we@@ a@@ ther cy@@ cl@@ es , and trans@@ form@@ ations in the environment , are also having a big ne@@ g@@ ative impact .\n",
            "accep@@ ting a certain state provides us with the opportunity of op@@ ening up new@@ er v@@ ist@@ as .\n",
            "it should spread wide , it should find a place in every persons life . each person should take 20 - 25 - 30 min@@ utes out from his d@@ ail@@ y rou@@ t@@ ine and sp@@ end it on practi@@ c@@ ing yo@@ g .\n",
            "the country is indeed grat@@ eful for their contribution .\n",
            "across the country , central government , state governments , all units of the local self government institutions , one lakh thir@@ ty thousand bank br@@ an@@ ches , lakhs of bank employees , over one and a half lakh post offic@@ es , over a lakh bank - m@@ it@@ r@@ as are rel@@ ent@@ l@@ ess@@ ly at work , day and ni@@ ght , with complete dedication .\n",
            "just as the ra@@ ys of the su@@ n give us life , similarly ra@@ inf@@ all provide us life and sust@@ en@@ ance .\n",
            "the thought process comes to a stand@@ still and that in itself becom@@ es a bur@@ den .\n",
            "the bri@@ tis@@ h t@@ ree tre@@ ach@@ er@@ ously ar@@ re@@ sted him when he was as@@ le@@ ep , in a cover@@ t manner .\n",
            "i ur@@ ge e@@ min@@ ent people of the country to come forward and jointly contribute in campai@@ gn@@ ing for sp@@ read@@ ing awareness on vo@@ ter regist@@ ration and c@@ ast@@ ing ones vo@@ te on the day of pol@@ ling .\n",
            "today is the hol@@ y day of ram nav@@ ami .\n",
            "i remember , when i was the chief minister of gujarat the temple of am@@ ba@@ ji there is visited in the month of bha@@ dr@@ ap@@ ad by lakhs of devo@@ te@@ es travel@@ ling by foo@@ t . a certain n@@ go decided to distribu@@ te s@@ ap@@ l@@ ings as pr@@ as@@ ad to the devo@@ te@@ es , and the devo@@ te@@ es were told that mat@@ a am@@ ba@@ ji will keep on show@@ ering her gr@@ ace , if they took care of the s@@ ap@@ l@@ ings , till their mat@@ urity into tre@@ es in their hom@@ es and villages .\n",
            "vi@@ j@@ ay j@@ ind@@ al writ@@ es that par@@ ents should not bur@@ den their children with their own expec@@ tations .\n",
            "my dear countrymen , we will celebrate the au@@ sp@@ ici@@ ous day of ram nav@@ ami on the 5@@ th of april , maha@@ vi@@ r jayanti is on the 9@@ th of april , and on the 14@@ th of april is the birth anniversary of bab@@ a sa@@ he@@ b ambedkar .\n",
            "and the second one is that the lim@@ it under the anti de@@ fec@@ tion law was enhanced from one - thir@@ ds to two - thir@@ ds .\n",
            "on the 19@@ th of june , our mar@@ s mission completed one thousand days .\n",
            "in haryana , cr@@ ops that requ@@ ir@@ e me@@ ag@@ re water are being encouraged . farmers thus are sav@@ ed from suf@@ fer@@ ing lo@@ s@@ ses .\n",
            "all of us have hear@@ d about the an@@ n pra@@ as@@ han san@@ sk@@ ar , the first sol@@ id mor@@ se@@ l r@@ it@@ ual for t@@ od@@ d@@ l@@ ers in families all across india .\n",
            "he writ@@ es that in our country , safety standards at fact@@ ories and construction sit@@ es are not upto the mark .\n",
            "any@@ way , being the first mat@@ ch , it is nat@@ ur@@ ally memor@@ able , but i will ch@@ er@@ ish it for a special re@@ ason .\n",
            "in this ' par@@ i@@ k@@ sha pe char@@ ch@@ a ' program , i will tou@@ ch upon a lot of things related to all the fac@@ ets of examin@@ ations with my young friends , especially on the t@@ op@@ ic of st@@ ress free ex@@ ams .\n",
            "lo@@ ts of things keep happ@@ ening in the world .\n",
            "this effort would ri@@ d of the proble@@ m of water lo@@ gg@@ ing .\n",
            "the 21st of june has been mand@@ ated and is celebrated as the international yoga day in the entire world and people start prepar@@ ing for it months in advance .\n",
            "i think that s@@ onal , who her@@ self is an agriculture grad@@ u@@ ate , came upon this idea . and to present man@@ go s@@ ap@@ l@@ ings in the mar@@ ri@@ age , just think , refl@@ ects the lo@@ ve for nature in a br@@ il@@ li@@ ant manner .\n",
            "it cont@@ ains nu@@ tr@@ iti@@ ous el@@ ements n@@ it@@ ro@@ g@@ en , p@@ hosp@@ hor@@ ous and pot@@ assi@@ um in ab@@ und@@ ance and is consid@@ ered a fer@@ til@@ is@@ er of very good quality in the agriculture sector .\n",
            "besides food and clo@@ th@@ es it is a good se@@ ason for health .\n",
            "more than 25@@ ,000 african y@@ ou@@ ths have stu@@ di@@ ed human resource development and capacity building in india .\n",
            "i invited my countrymen to join the s@@ and@@ esh to soldiers campaign .\n",
            "this is what re@@ in@@ forces our bel@@ ief that our nation will be clean one day , for sure .\n",
            "my attention was dra@@ wn towards his work , so , i tal@@ ked to him on the ph@@ one and tri@@ ed to understand the new experi@@ ment being att@@ emp@@ ted by him .\n",
            "s@@ ak@@ al ji , i have conveyed your s@@ enti@@ ments to our l@@ ist@@ en@@ ers .\n",
            "the judi@@ cial system too could not es@@ cap@@ e the sin@@ ist@@ er sha@@ do@@ ws of the emer@@ g@@ ency .\n",
            "it was a very important decision but implementation of that decision is equ@@ ally important .\n",
            "sports k@@ its , m@@ n@@ reg@@ a job cards and sc@@ st certi@@ fic@@ ates were distribu@@ ted .\n",
            "government of india has made a successful effort to clean the ganga and invol@@ ve the people for this project with the cooperation of five state governments .\n",
            "many of our si@@ kh brothers and sisters set@@ t@@ led in other countries commit@@ t@@ ed@@ ly follow guru nan@@ ak de@@ v j@@ is ide@@ als .\n",
            "you must have hear@@ d of the mu@@ sical instru@@ ment called san@@ to@@ or .\n",
            "i had the ch@@ ance to visit si@@ ach@@ in a few days back .\n",
            "it is our responsibility that without any arti@@ fici@@ ality we share our leg@@ acy and intr@@ oduce our@@ selves to the world .\n",
            "and it is by my own personal experience that im t@@ ell@@ ing you that if y@@ our@@ e under pres@@ sure then you for@@ get even your own things but if you are re@@ la@@ x@@ ed , then you c@@ ant even im@@ ag@@ ine the kind of things you are able to remember , and these become extremely us@@ eful .\n",
            "can you s@@ end me your suggestions on how best to use that opportunity\n",
            "a few days ago , a gent@@ le@@ man had given me a very s@@ ound propo@@ si@@ tion .\n",
            "so i requ@@ ested a demonstr@@ ation and a pleas@@ ant sur@@ pr@@ ise was in st@@ ore for me . he m@@ im@@ ic@@ ked the very ex@@ act way i conver@@ se with you during mann ki baat .\n",
            "today is the time to say that good governance is our birth right and we will have it .\n",
            "have a look at it . do get these follow@@ ed in your village , in your mo@@ h@@ all@@ as , in your city , in your school , in your institution , even in offic@@ es .\n",
            "this is in a way a hum@@ or@@ ous sugg@@ estion .\n",
            "who had sp@@ ent 20 to 22 years of their lives in pr@@ is@@ on with n@@ el@@ son man@@ del@@ a , effec@@ tively sacrific@@ ing the entire period of their youth for the society .\n",
            "but , today , i want to have a disc@@ our@@ se with you on a new t@@ op@@ ic .\n",
            "u@@ ite a few active citizens have s@@ ent in suggestions on the subject of water conservation .\n",
            "and i keep in@@ sist@@ ing that we must bu@@ y some hand@@ lo@@ om products made by our we@@ av@@ ers , our khadi ar@@ tis@@ ans .\n",
            "it is hear@@ tw@@ arm@@ ing to see them discus@@ s boo@@ ks publi@@ shed on subjec@@ ts such as science , technology , innovation , history , culture , business , life - sk@@ et@@ ches and so on .\n",
            "today our nar@@ i sha@@ kti is as@@ sum@@ ing leadership rol@@ es .\n",
            "sri guru nan@@ ak de@@ v ji radi@@ ated his message to all cor@@ n@@ ers of the world .\n",
            "be it nav@@ r@@ at@@ ri or d@@ ur@@ ga pu@@ j@@ a how can this sha@@ kti - up@@ as@@ ana become the festival of celebr@@ ating social unity\n",
            "i congratulate everyone connected with this cleanliness programme providing strength to it .\n",
            "if we have b@@ ad hand@@ wr@@ iting , and we want to improve it , we have to con@@ sci@@ ously practi@@ ce for a long time .\n",
            "in the coming days this card is going to be us@@ eful as both - a credit and a deb@@ it card .\n",
            "if any of you gets an opportunity to go to por@@ band@@ ar , the place of birth of re@@ ver@@ ed b@@ ap@@ u , then there is a house be@@ hind the house of re@@ ver@@ ed b@@ ap@@ u , where a 200 - year old water tan@@ k still exist@@ s . it is still cap@@ able of st@@ oring water and has a mechanism to har@@ v@@ est ra@@ in water\n",
            "i think it has become im@@ per@@ ative to present isl@@ am in its true form to the world .\n",
            "but these will contribute to the real@@ isation of the dream of this country , this new india , that is being nur@@ tu@@ red by 125 crore countrymen , and this real@@ isation will be achieved before their e@@ y@@ es .\n",
            "have you ever thought about how much food we waste\n",
            "today , once again , they have accep@@ ted the challenge and i do believe that the ste@@ ely resolve of one hundred and twenty five crore countrymen , their collective demonstr@@ ation of pur@@ ush@@ ar@@ th will go a long way in invest@@ ing our nation with a new power and strength on its path of progress .\n",
            "today as we he@@ ad towards a new india , in which we want to honour those who are doing their work at the gr@@ ass - roo@@ t level without any care for a re@@ ward .\n",
            "climate change , global warm@@ ing is being discussed at every no@@ ok and cor@@ ner and now it has become an accep@@ ted stand@@ ard for em@@ bar@@ king on any new task .\n",
            "gst is prob@@ ab@@ ly be the biggest tax reform in the world .\n",
            "most people have der@@ i@@ ved political concl@@ u@@ sions out of that .\n",
            "i shall talk about it in de@@ t@@ ail later , but right now we have time on our h@@ ands some people can beg@@ in their practi@@ ce , wher@@ e@@ as some can ch@@ al@@ k out a plan .\n",
            "and so , not just our body , but our mind and value system get integrated with ush@@ ering unity in india to take india to lo@@ f@@ ti@@ er heights\n",
            "what is cer@@ t@@ ain@@ ly required is foc@@ using on your goal with a cal@@ m ste@@ ad@@ y de@@ me@@ an@@ our and the will to stri@@ ve .\n",
            "they had to open z@@ er@@ o bal@@ ance accounts . but these poor people sav@@ ed money and de@@ pos@@ ited a su@@ m of 2@@ 2@@ ,000 crores .\n",
            "in the next few wee@@ ks various education bo@@ ards across the country will initi@@ ate the process for the board examin@@ ations of the ten@@ th and tw@@ el@@ f@@ th standards .\n",
            "if you will have faith in these things , then marks will aut@@ om@@ ati@@ c@@ ally follow you and you will never have to cha@@ se marks\n",
            "lakhs of school children en@@ jo@@ yed this exhibition .\n",
            "in a way , i under@@ took the journey to meet my inn@@ er self .\n",
            "this award would be be@@ sto@@ wed on those organizations around the world , whose significant and unique contribu@@ tions in the promotion of yoga you cannot even im@@ ag@@ ine\n",
            "we can do this in our day to day lives .\n",
            "in almost all countries of the world , deep@@ aw@@ al@@ i is celebrated in one way or the other .\n",
            "we are part of a tradition where men were identified due to women - y@@ ash@@ od@@ a - n@@ and@@ an , k@@ au@@ sh@@ al@@ ya - n@@ and@@ an , gand@@ hari - pu@@ tra , these were identi@@ ties of a son .\n",
            "now it is going to be nearly two years on 2nd october and i can confid@@ ently say that one hundred and twenty five crore people of the country have now become more aware about cleanliness .\n",
            "the vol@@ un@@ te@@ ers of jam@@ i@@ at - u@@ le@@ ma - e - h@@ ind@@ set a fin@@ e , inspir@@ ing example of unity for cleanliness .\n",
            "the fact is , st@@ ories of the val@@ our and pat@@ ri@@ o@@ tis@@ m of our her@@ o@@ es should be told and re - told to our new@@ er generations in ways more than one .\n",
            "the subject about which most of the people from across the country have written is our re@@ ver@@ ed atal be@@ hari vajpay@@ ee .\n",
            "friends , i want to thank this bo@@ y ab@@ hi ch@@ at@@ ur@@ ve@@ di .\n",
            "last time in mann ki baat i tal@@ ked about or@@ g@@ an d@@ on@@ ation .\n",
            "a few wee@@ ks ago , an immense event enti@@ t@@ led ' par@@ i@@ k@@ sha pe char@@ ch@@ a ' was organ@@ ised in delhi in the form@@ at of a to@@ wn h@@ all programme .\n",
            "one is those huge gar@@ b@@ age pil@@ es which keep l@@ ying in the city well the people in the government will work to re@@ move those .\n",
            "all of us know that just the wor@@ d canc@@ er is more than en@@ ou@@ gh to sc@@ are the world .\n",
            "125 crore people of this country are going to take my ex@@ am .\n",
            "an ess@@ ay competition was organized among all the c@@ b@@ se aff@@ ili@@ ated schools . a po@@ e@@ try competition was organized to increase their participation .\n",
            "the mut@@ th@@ i bha@@ r d@@ ha@@ an@@ ya initiative has tur@@ ned into a big movement in na@@ shi@@ k .\n",
            "different ty@@ p@@ es of arti@@ fac@@ ts were made utiliz@@ ing gar@@ b@@ age .\n",
            "the world has accep@@ ted this .\n",
            "e@@ k@@ ta in her e - m@@ ail writ@@ es - ' the most important moment in the life of any ath@@ le@@ te is that when he or she holds the tr@@ ic@@ ol@@ or and i am proud that i could do that .\n",
            "the proble@@ m is that we are un@@ able to fully compreh@@ end it even am@@ ong@@ st those around us , and per@@ ha@@ ps we also h@@ es@@ it@@ ate to talk about it op@@ en@@ ly , with our family and friends .\n",
            "i do believe that you will indeed do so .\n",
            "at times a question ar@@ ises in the min@@ ds of peop@@ les as to what i achieved through mann ki baat . i would like to say today that of the fe@@ ed@@ back of this programme , one of the poin@@ ts that tou@@ ches my heart most - is when people tel@@ l me that when we si@@ t with all our family members to l@@ ist@@ en to mann ki baat , we feel that the he@@ ad of our family is sit@@ ting with us and sharing his ideas with us . when i hear@@ d this com@@ ment in a larg@@ er cir@@ c@@ le , i felt satis@@ fied to know that i am y@@ ours , i am one am@@ ong@@ st you , i am with you , you el@@ ev@@ ated me and in a way , and in this way i will continue to remain connected with you as a family member through man ki baat .\n",
            "with this thought i decided to do away with the tradition of inter@@ views for small posi@@ tions .\n",
            "my dear brothers and sisters , we just celebrated the festival of shi@@ v@@ ar@@ at@@ ri .\n",
            "i hope and believe that everyone will celebrate ei@@ d with g@@ ai@@ ety and fer@@ v@@ or and on this occasion children will speci@@ ally get a gr@@ and e@@ id@@ i .\n",
            "and not just that , it is a celebr@@ ation of inspiration for brave hear@@ ts .\n",
            "i would like to talk about an issue which is a matter of great concern .\n",
            "science technology , agriculture health , ma@@ them@@ ati@@ cs management , economy environment , the entire spectrum has been tou@@ ched upon . it is said that our ve@@ d@@ as have deta@@ iled re@@ ference on mantr@@ as , on ways means to counter the challenges of global warm@@ ing .\n",
            "i believe in your strength , h@@ ence , i believe in our nations future .\n",
            "mahatma gandhi also tal@@ ked about the last man standing in the qu@@ e@@ ue .\n",
            "wh@@ o@@ ever , ne@@ eded him , and wher@@ ever , gand@@ hi@@ ji was present to serve .\n",
            "the birth anniversary of dr . bab@@ a sa@@ he@@ b ambedkar is on the 14@@ th of april .\n",
            "how can we progress in this direction\n",
            "and i will take them seri@@ ously as active citizens are the biggest as@@ set for development .\n",
            "wal@@ let simp@@ ly means an e - pur@@ se .\n",
            "i am grat@@ eful to ak@@ b@@ ar sa@@ ha@@ b .\n",
            "from the se@@ ash@@ o@@ res to the m@@ oun@@ t@@ ains , people welcomed the first ra@@ ys of the su@@ n , with yoga .\n",
            "we will meet once again for mann ki baat ' next time .\n",
            "the role of a doc@@ tor is not limited to m@@ ere treatment of ail@@ ments .\n",
            "another encour@@ aging fact is that , today , there are a record 7@@ 8 women members of parliament .\n",
            "fir@@ st@@ ly , i am most grat@@ eful to such con@@ scienti@@ ous citizens .\n",
            "and for this re@@ ason , now that the ra@@ in@@ y se@@ ason is appro@@ ach@@ ing , sav@@ ing water should be our priority . and when we celebrate di@@ wal@@ i this time , then we should also re@@ vel in how much water did we sa@@ ve how much water we st@@ op@@ ped from f@@ low@@ ing out .\n",
            "have you ever wond@@ ered why he does so\n",
            "the district administration there under@@ took a huge task with peop@@ les participation . from 6 a .\n",
            "the attention of students ste@@ ers from home to ho@@ ste@@ l .\n",
            "so , ak@@ ash@@ v@@ an@@ i ma@@ it@@ ree on this side and bangladesh be@@ ta@@ ar on that side will mutually share the cont@@ ent and beng@@ al@@ i speaking people on both sides will en@@ joy the programmes of ak@@ ash@@ v@@ an@@ i , that is , all india radio .\n",
            "even now , some people think that they can re - intr@@ oduce into the system , money from corruption , black money , un@@ accoun@@ ted wealth and ben@@ ami money ( money in un@@ known persons name ) , using some route or the other .\n",
            "there is no d@@ en@@ ying the fact that we could not per@@ form up to the expec@@ tations .\n",
            "it is cor@@ rec@@ t that i am reach@@ ing m@@ ag@@ har on the 2@@ 8@@ th .\n",
            "this wom@@ an power b@@ in@@ ds closely together society as a whole , the family as a whole , on the a@@ x@@ is of unity on@@ eness .\n",
            "economic activities too should be back on tr@@ ack .\n",
            "shri v@@ ar@@ un vi@@ sh@@ wan@@ a@@ than has also written on nar@@ end@@ ram@@ odi@@ app that our ath@@ le@@ tes did a commend@@ able job and that i should talk about this in mann ki baat .\n",
            "then i re@@ alize that if i re@@ la@@ x a b@@ it , i will feel better .\n",
            "you are my inspiration and you are my energy\n",
            "it is going to be of use for the whole of human@@ kind . but being indians , we should all feel happy that in the entire process of this disc@@ overy , the s@@ ons of our country , our wor@@ th@@ y indian scientists , were also a part of it .\n",
            "this also helped in improving efficiency .\n",
            "affordable medic@@ ines are now available at am@@ r@@ it sto@@ res at pradhan mantri bhar@@ ati@@ ya jan a@@ us@@ ha@@ d@@ hi centres at hospit@@ als .\n",
            "they have emphasized on reforms in the educational set ups .\n",
            "my dear fel@@ low citizens , i would like you to note down one number . from tomorrow , you can give a mis@@ sed call on this number and l@@ ist@@ en to mann ki baat . you can even l@@ ist@@ en to it in your mother t@@ on@@ gue .\n",
            "my best wishes to all the students wh@@ ore going to appe@@ ar for their examin@@ ations , their par@@ ents and all the teachers as well .\n",
            "my dear countrymen , the 12@@ th of november , 2019 is the day when the 5@@ 5@@ 0th pr@@ ak@@ ash@@ o@@ t@@ sa@@ v of guru nan@@ ak de@@ v ji will be celebrated across the world .\n",
            "in india , citizens are contribu@@ ting in this campaign in their own way .\n",
            "in the year 2019 , the announ@@ cement of prime minister ' s awards for excell@@ ence in the promotion and development of yoga was a matter of great satisfaction for me .\n",
            "but you c@@ ant im@@ ag@@ ine the force that is l@@ ent to the programme when a village or a society adop@@ ts it .\n",
            "my dear students , you must have thought of travel@@ ling to places during your hol@@ id@@ ays .\n",
            "the time required to cover dist@@ ances has come down dr@@ as@@ tic@@ ally .\n",
            "ru@@ ch@@ i@@ ka d@@ ab@@ as has s@@ ent a message on my app and shared her experience .\n",
            "a@@ sist@@ er from chhattisgarh collec@@ ts cust@@ ard app@@ le and does business by making its ice cre@@ am .\n",
            "comp@@ ared to the world , we still have a large number of t@@ b pati@@ ents .\n",
            "the st@@ af@@ f of the un and dipl@@ om@@ ats from across the world particip@@ ated .\n",
            "he fe@@ els that our youth are phy@@ si@@ c@@ ally we@@ ak , comp@@ ared to those of other asian countries .\n",
            "any m@@ ach@@ ine will work the way we want it to .\n",
            "for mahatma gandhi , the individu@@ al and society , human be@@ ings humanity was every@@ thing .\n",
            "in rajasthan ap@@ na b@@ ach@@ ch@@ a , ap@@ na vid@@ y@@ al@@ aya campaign is being run to read@@ m@@ it those gir@@ ls into schools , who had dr@@ op@@ ped out , in order to encourage and mo@@ tiv@@ ate them to start stud@@ ying again .\n",
            "ple@@ ase bl@@ ess us like a mother and may your bl@@ ess@@ ings continue on to be show@@ ered upon us .\n",
            "i have only one int@@ ention in speaking with you all , come , let us serve our mother india .\n",
            "l@@ al ba@@ ha@@ du@@ r sha@@ sh@@ tr@@ i ji sho@@ wed that even am@@ id@@ st the de@@ af@@ ening s@@ oun@@ ds of g@@ un@@ fir@@ e and b@@ om@@ bar@@ d@@ ment , there exist@@ ed an al@@ tern@@ ative way for every citizen for expres@@ sing pat@@ ri@@ o@@ tis@@ m .\n",
            "in a way , we have to move ahead on the path of conver@@ ting gar@@ b@@ age to gold .\n",
            "you must have been a wit@@ ness y@@ our@@ self to the cur@@ i@@ os@@ ity in the air when the p@@ ad@@ ma awards are announced , and people are in@@ qu@@ i@@ si@@ tive about the awar@@ de@@ es .\n",
            "this self confidence , this very posi@@ tivity will by a c@@ at@@ aly@@ st in real@@ ising our resolve of new india , of making our dream come true .\n",
            "but the bul@@ let dam@@ aged his k@@ id@@ ney and a part of the int@@ est@@ ine was lost .\n",
            "i la@@ u@@ d the role played by inn@@ um@@ er@@ able individu@@ als , citizens who immedi@@ ately em@@ bar@@ k upon res@@ c@@ ue and relief operations wher@@ ever a disaster stri@@ k@@ es .\n",
            "so there will be many people , who still have their sh@@ op@@ ping left .\n",
            "you are aware of the fact that when some@@ one s@@ mo@@ k@@ es around us , we come to know about it through its od@@ our .\n",
            "i respec@@ t@@ fully bo@@ w to all the brave s@@ ons of the country , who laid down their lives , protec@@ ting the honour of their mo@@ ther@@ land , india .\n",
            "reiter@@ ating our resolve to ful@@ f@@ ill his dream of a prosperous and developed india , i along with all of you pay tributes to at@@ al@@ ji .\n",
            "we have even conver@@ ted the knowledge about food into a science .\n",
            "and those who are health con@@ sci@@ ous always say that , there should always be some space left both in the st@@ om@@ ach and on the pl@@ ate .\n",
            "just a month ago , on the 3@@ 0th of december , i had g@@ one to and@@ am@@ an n@@ ic@@ ob@@ ar isl@@ ands .\n",
            "and some@@ times it so happ@@ ens that a small com@@ ment from an individu@@ al in some remote village in india conve@@ ys something that just tou@@ ches our hear@@ ts .\n",
            "i can say that india is full of di@@ versities and i was able to wit@@ ness so many m@@ ag@@ n@@ ific@@ ent sc@@ en@@ es in those p@@ ic@@ tures\n",
            "besides u@@ su@@ al arran@@ ge@@ ments to welcome the prime minister , cleanliness will also be high on the agenda .\n",
            "i congratulate these d@@ are@@ de@@ v@@ ils , especially the daugh@@ ters from the c@@ ore of my heart .\n",
            "i believe you must have had a great time during your di@@ wal@@ i v@@ ac@@ ations am@@ id@@ st all fes@@ ti@@ vities .\n",
            "and then the ord@@ ers can be plac@@ ed .\n",
            "and mahatma gandhi always said , that for@@ giv@@ eness is the quality of great men .\n",
            "we should not for@@ get that the only voice that could be hear@@ d one year ago was that no@@ thing happ@@ ens , no@@ thing happ@@ ens , no@@ thing happ@@ ens .\n",
            "d@@ on@@ t you feel that wh@@ en@@ ever a v@@ end@@ or comes to your do@@ or to sel@@ l something , on his r@@ oun@@ ds , when we come into cont@@ act with small sh@@ op@@ keep@@ ers , ve@@ get@@ able sel@@ l@@ ers , au@@ to ric@@ k@@ sha@@ w dri@@ vers - in fact any person who ear@@ n@@ s through sh@@ e@@ er hard work we start bar@@ g@@ aining with him , ha@@ gg@@ ling with him : no not so much , make it two rupees less , five rupees less\n",
            "i have announced setting up of a committee .\n",
            "i also express my grati@@ tude .\n",
            "i c@@ are@@ fully hear@@ d what each state had to say .\n",
            "if the family celebr@@ ates a bir@@ th@@ day , certain au@@ sp@@ ici@@ ous day or ob@@ serve an in - memor@@ i@@ am day , then the family members with self - pre@@ pa@@ red nu@@ tr@@ iti@@ ous and del@@ ici@@ ous food , go to the an@@ gan@@ w@@ adi@@ s and also to the schools and these family members themselves serve the children and fe@@ ed them .\n",
            "they are also comple@@ t@@ ely un@@ aware of its d@@ an@@ ger and for this re@@ ason some@@ times e - ci@@ g@@ are@@ tt@@ ess@@ ne@@ ak into the house out of sh@@ e@@ er cur@@ i@@ os@@ ity .\n",
            "and these ho@@ sts are being profession@@ ally tr@@ ained in the techn@@ i@@ que of r@@ un@@ ning a home st@@ ay .\n",
            "just be under the open sk@@ y , go to the ro@@ of top , do deep bre@@ a@@ thing for five min@@ utes and retur@@ n to your stu@@ dies . you will experience re@@ la@@ x@@ ation in your body .\n",
            "almost in every state , a big plan@@ tation campaign gets under@@ way with the on@@ set of mon@@ soon .\n",
            "so will this campaign of po@@ sting p@@ ic@@ tures of three generations together per@@ forming yoga will ev@@ o@@ ke cur@@ i@@ os@@ ity across the nation as well as across the world .\n",
            "i feel such programmes should be organized every qu@@ ar@@ ter .\n",
            "my dear countrymen , 21st june has become a well known day world over .\n",
            "my dear countrymen , many of you have urged me on my go@@ v and narendra modi app to men@@ tion doctors day , the 1st of july . you are ab@@ sol@@ ut@@ ely right .\n",
            "in a way we have worked towards providing the bank at your do@@ or step .\n",
            "we have to face t@@ em@@ por@@ ary set@@ b@@ ac@@ ks in li@@ fe@@ .@@ . .@@ .@@ . but always remember - the capacity to over@@ come them re@@ sides within us .\n",
            "similarly , this time , let us do a campaign bharat ki l@@ ami .\n",
            "when i started read@@ ing about the freedom struggle , it was then i re@@ alized that how much struggle was involved in getting india fre@@ ed , how much sacrific@@ es were given and how so many freedom figh@@ ters sp@@ ent years in pr@@ is@@ on .\n",
            "pre@@ vention is better than c@@ ure . a person practi@@ c@@ ing yo@@ g , can eas@@ ily have the achievements of st@@ aying healthy , maint@@ aining bal@@ ance , being ric@@ h@@ ly end@@ o@@ wed with a strong will power , nur@@ t@@ uring su@@ pre@@ me self confidence and to have con@@ centr@@ ation in every task one does .\n",
            "it was inter@@ esting to know about the ongoing mir@@ ac@@ ul@@ ous ac@@ comp@@ lish@@ ments in the field of science .\n",
            "a few days ago , al@@ e@@ en@@ at@@ a@@ ay@@ an@@ g , a stud@@ ent from far off arunachal has written an inter@@ esting le@@ tter to me .\n",
            "we remember and pay our homage to ti@@ la@@ k ji on his birth anniversary on 2@@ 3rd july and his de@@ ath anniversary on 1st august .\n",
            "how do we make z@@ er@@ o waste our priority\n",
            "it is true that in the last dec@@ ade there has been a dec@@ line in mat@@ ernal mor@@ t@@ ality r@@ ates but even now , we are not able to sa@@ ve the lives of a large number of pre@@ gn@@ ant women .\n",
            "some of you have written to me to in@@ corporate skills development cour@@ ses in the school cur@@ ricul@@ um from 5@@ th stand@@ ard .\n",
            "but this is no ordinary radio chan@@ nel , it is a very big and very important step .\n",
            "i thank bharat ji for sharing this experience with me .\n",
            "i beg@@ in mann ki baat today with a he@@ av@@ y heart .\n",
            "a few days ago , i was in ahmedabad , where i got the privile@@ ge of unve@@ il@@ ing the statue of dr . vik@@ ram sar@@ ab@@ ha@@ i .\n",
            "whether it is er@@ a@@ dic@@ ating poverty , prev@@ ent@@ ing dise@@ ases , connec@@ ting with the world , or dis@@ semin@@ ating knowledge and information , technology and science have made a mark for themselves .\n",
            "i have received thousands of suggestions on the nar@@ end@@ ram@@ od@@ i mobile app .\n",
            "ac@@ tu@@ ally speaking , i have not retur@@ ned you brought me back , you posi@@ tioned me here and gave me the opportunity to speak once again .\n",
            "a 2@@ 4@@ x@@ 7 hel@@ pl@@ ine facility is also available . its number is 18@@ 00 1@@ 14 7@@ 70 .\n",
            "it was dr . bab@@ a sa@@ he@@ b who envis@@ aged water power as nation power .\n",
            "i hope that the amount of emphasis that is given to the rights at every level during most of the time , is also given to discus@@ sing du@@ ties of citizens in an emp@@ h@@ atic manner .\n",
            "if you ob@@ serve in your neighb@@ our@@ hood , then you will wit@@ ness for y@@ our@@ selves how people over@@ come the difficul@@ ties in their lives\n",
            "he sho@@ wed us that to succ@@ e@@ ed it is not necessary for the person to be bor@@ n in an ill@@ u@@ stri@@ ous or rich family , but even those who are bor@@ n to poor families in india can also d@@ are to dream their dreams and re@@ alize those dreams by achieving success .\n",
            "start it from now , one month in advance and you will be a particip@@ ating partner on 21st june .\n",
            "si@@ r , i want to know how we can ch@@ ec@@ k such un@@ fortun@@ ate events and in@@ cid@@ ents from taking place .\n",
            "he would say - that is , if your heart is p@@ ure , then the ab@@ sol@@ ute ma@@ ster re@@ sides within your heart .\n",
            "you should have a dream to real@@ ise and a goal that you are committed to .\n",
            "there cannot be any bigg@@ er pleasure for me than to know that my visit has been connected with the cleanliness campaign .\n",
            "one of his disc@@ over@@ ies is fam@@ ous as the ram@@ an effect .\n",
            "the ti@@ ger population in india is 2@@ 9@@ 6@@ 7 , two thousand n@@ ine hundred si@@ x@@ ty seven .\n",
            "if you are ten@@ se , then all the do@@ ors see@@ m to be clo@@ sed , no@@ thing can ent@@ er from out@@ side and no@@ thing can come out from in@@ side .\n",
            "many vi@@ du@@ sh@@ is of india have contribu@@ ted in com@@ pos@@ ing the vers@@ es of the ve@@ d@@ as .\n",
            "she always tr@@ ies to help others , but one hab@@ it of h@@ ers am@@ az@@ es me .\n",
            "i congratulate the people in the government , government officials and the citizens of vi@@ z@@ i@@ an@@ ag@@ ar@@ am district on this great ac@@ comp@@ lish@@ ment of achieving this fe@@ at through immense hard work and setting a very inspir@@ ing example in the process .\n",
            "in the run - up to and prepar@@ ation for the 2017 fif@@ a event , we can har@@ ness and disp@@ lay our many strengths while doing so , we can do im@@ age - br@@ and@@ ing for india as well .\n",
            "my dear countrymen , tomorrow , the 3@@ 1st october is the birth anniversary of sardar v@@ all@@ ab@@ h@@ bha@@ i patel , the great son of india who vo@@ wed to un@@ ite india and made this the sol@@ e objec@@ t of his life .\n",
            "your cooperation is ne@@ eded to create public awareness .\n",
            "take the case of ab@@ du@@ l g@@ ha@@ fo@@ or k@@ hat@@ ri of gujarat , who has done an am@@ az@@ ing job of re@@ vi@@ ving the traditional ro@@ g@@ an pa@@ int@@ ing form of k@@ ut@@ ch .\n",
            "what are the things we need to learn in the changing times\n",
            "we all have to pay a memor@@ able tribu@@ te to b@@ ap@@ u and have to take the country to new@@ er heights by dra@@ wing inspiration from b@@ ap@@ u .\n",
            "at the age of 23 she lost her h@@ us@@ b@@ and due to l@@ ack of prop@@ er treatment , and this in@@ cident inspired her to build a hospital for the poor .\n",
            "and this time i have spo@@ k@@ en about it well in time .\n",
            "there are many employment opportunities created by tourism .\n",
            "but this s@@ und@@ ay has made one w@@ ai@@ t end@@ l@@ ess@@ ly\n",
            "and , ath@@ ar@@ v@@ a veda , written thousands of years ago , is the most auth@@ enti@@ c guid@@ ing sc@@ ri@@ p@@ ture about nature and environment .\n",
            "today our mon@@ et@@ ary policy , start up india , stand up india initiative have become se@@ ed@@ bed for our young innov@@ ators and young entrepreneurs .\n",
            "our body is made up of five basic el@@ ements and wh@@ en@@ ever we come in cont@@ act with these el@@ ements we get f@@ res@@ h energy .\n",
            "the president launched this radio chan@@ nel .\n",
            "they will l@@ ist@@ en to their problems and direct the government in taking appropriate measures in sol@@ ving those problems .\n",
            "we have an eas@@ y medium at our di@@ spo@@ sal and so we will all do it .\n",
            "mar@@ i@@ y@@ app@@ an than@@ ga@@ vel@@ u won a gold med@@ al in high j@@ um@@ p .\n",
            "they also disp@@ l@@ ay@@ ed as to how conven@@ i@@ ent these new techn@@ i@@ que to@@ ile@@ ts are and there is ab@@ sol@@ ut@@ ely no in@@ conven@@ i@@ ence or h@@ es@@ itation in emp@@ t@@ ying or clean@@ ing these to@@ ile@@ ts and even the p@@ sy@@ ch@@ ological bar@@ ri@@ er does not come in the way at all .\n",
            "in andhra pradesh , ne@@ er@@ u pr@@ ag@@ ati mission has been using technology for ground water re@@ char@@ ging .\n",
            "i have received a le@@ tter .\n",
            "our young generation must know what had happ@@ ened on the 9@@ th of august 194@@ 2 .\n",
            "this le@@ tter is special in many ways .\n",
            "i hold in high est@@ e@@ e@@ m , the election commis@@ sions of all states , security personnel and other st@@ af@@ f members who contribute in ensuring str@@ ict ad@@ her@@ ence to free and f@@ air pol@@ ling .\n",
            "i have plann@@ ed to keep my spe@@ ech short this time not more than 40 - 45 - 50 min@@ utes .\n",
            "i had made a hum@@ ble requ@@ est that on the occasion of r@@ ak@@ sha band@@ han we give our sisters these insurance schemes as a gi@@ ft .\n",
            "in our country there is a network of ash@@ a workers .\n",
            "i have only one message for you all . your par@@ ents , your teachers , your family members and your friends have a lot of expec@@ tations from you .\n",
            "you recei@@ ve it in your hand for a moment and then ab@@ and@@ on it .\n",
            "and our real wealth - the youth become condem@@ ned to tre@@ ad on the path of waste , qu@@ ite un@@ aware\n",
            "we have introduced a gold mone@@ tiz@@ ation scheme .\n",
            "children , the el@@ der@@ ly , the youth full of energy and enthusias@@ m , women , gir@@ ls tur@@ ned out to participate in this b@@ att@@ le .\n",
            "we know that wh@@ en@@ ever an indian achiev@@ es something no@@ table , the whole country fe@@ els a new energy and the self - confidence gets a boost .\n",
            "various government departments regul@@ arly ob@@ serve cleanliness for@@ t@@ ni@@ ght .\n",
            "my dear countrymen , agriculture mak@@ es a very major contribution to the fund@@ a@@ ment@@ als of our country@@ s economy .\n",
            "one r@@ un@@ s short of words in compli@@ menting and congratul@@ ating our scientists , their entire team .\n",
            "for dr . sh@@ y@@ ama pr@@ as@@ ad mu@@ k@@ her@@ j@@ ee , the most important thing was the integr@@ ity and unity of india - and for this , at the young age of 5@@ 2 , he also sacrific@@ ed his life .\n",
            "and that is why today , i feel like men@@ tioning about a few children . they are n@@ id@@ hi b@@ ai@@ pot@@ u , mon@@ ish jo@@ sh@@ i , de@@ v@@ an@@ sh@@ i ra@@ w@@ at , tan@@ ush j@@ ain , har@@ sh de@@ v@@ d@@ har@@ kar , an@@ ant ti@@ war@@ i , pre@@ et@@ i na@@ g , ath@@ ar@@ v@@ a de@@ sh@@ mu@@ kh , ar@@ on@@ y@@ at@@ esh gan@@ g@@ ul@@ i h@@ ri@@ th@@ i@@ k al@@ a@@ am@@ and@@ a@@ a .\n",
            "i know for sure that your adv@@ ent is trans@@ forming the land@@ sc@@ ap@@ e of india .\n",
            "you only look at his experience , his knowledge , and the grap@@ h of his success .\n",
            "india is grap@@ pl@@ ing with di@@ verse challenges .\n",
            "the most hear@@ t@@ ening asp@@ ect of back to village programme is the fact , that it was organized in such remote villages , where even officials had to tra@@ verse difficul@@ t ter@@ ra@@ in and clim@@ b m@@ oun@@ t@@ ains while wal@@ king on foo@@ t over a period of day or day and a half .\n",
            "my best wishes to all the young people about to beg@@ in a new inn@@ ings .\n",
            "if you are better than your competi@@ tor , then you will become com@@ plac@@ ent , as you will be fill@@ ed with over - confidence .\n",
            "my second requ@@ est to the countrymen is to share many traditional me@@ th@@ ods that have been in use over the centur@@ ies in our country for the conservation of water .\n",
            "we shall celebrate national sports day on 2@@ 9@@ th august and i extend my best wishes to all spor@@ t l@@ ov@@ ers and also pay my tributes to the leg@@ end@@ ary h@@ oc@@ key w@@ iz@@ ard shri d@@ hy@@ an@@ chand@@ ji .\n",
            "our w@@ o@@ men@@ s h@@ oc@@ key team qu@@ ali@@ fied for the ol@@ y@@ mp@@ ic games after a space of 3@@ 6 long years .\n",
            "i hope that you too become inspired with the spirit of pat@@ ri@@ o@@ tis@@ m and do something good in that ve@@ in .\n",
            "many sports competi@@ tions for k@@ id@@ s were organized under the a@@ e@@ g@@ is of k@@ hel@@ o india .\n",
            "the mantr@@ a of national unity is the very foundation of a strong and great india .\n",
            "as a sym@@ bo@@ l of these , i would like to read a po@@ e@@ m which has been s@@ ent by sh@@ r@@ im@@ an ash@@ wan@@ i kum@@ ar .\n",
            "there is a wom@@ an no@@ or ja@@ han in kan@@ pur . wat@@ ching her on tel@@ e@@ vision it se@@ em@@ ed that she did not have much of a ch@@ ance to get education .\n",
            "people from maharashtra , karnataka , go@@ a , andhra pradesh , tel@@ eng@@ ana have deep devo@@ tion and respect for v@@ it@@ th@@ al .\n",
            "we are tr@@ ying to provide a s@@ mo@@ ke free life to 5 crore such families and we are successfully advanc@@ ing in that direction .\n",
            "our armed forces have con@@ sist@@ ently disp@@ l@@ ay@@ ed un@@ par@@ all@@ el@@ ed courage and val@@ our .\n",
            "we recently lost 18 brave s@@ ons of our country in a terrorist attack in ur@@ i sector in jammu and kashmir .\n",
            "the government of india has launched various schemes of social security for the common man .\n",
            "i believe that many such att@@ emp@@ ts are being made and we can turn im@@ possible in to possible when we are together and stri@@ ve with collective resolve .\n",
            "mad@@ am i am grat@@ eful to you for c@@ alling me with this tou@@ ching message .\n",
            "this is a great new system .\n",
            "very few people would also be know@@ ing that in 19@@ 3@@ 7 , on the invitation of dr . sh@@ y@@ ama pr@@ as@@ ad mu@@ k@@ her@@ j@@ ee , gur@@ u@@ de@@ v r@@ abin@@ d@@ ran@@ ath t@@ ag@@ ore addressed the conv@@ oc@@ ation in k@@ ol@@ k@@ ata university in bangl@@ a .\n",
            "today , the vill@@ ag@@ ers have to go to other vill@@ ag@@ ers for char@@ ging their mobile p@@ hon@@ es .\n",
            "she has vo@@ wed to s@@ end her only son to join the cr@@ p@@ f .\n",
            "i do not wish to tr@@ ou@@ ble you further , since i can see how bus@@ y you are with my@@ ri@@ ad pres@@ su@@ res on the work front .\n",
            "many people are taking an initiative to form their own te@@ ams .\n",
            "i would like to congratulate our hon@@ our@@ able parlia@@ ment@@ ari@@ an sri k@@ ari@@ a m@@ und@@ a ji for the same .\n",
            "that is all for today , many many than@@ ks .\n",
            "less than a hundred days are now left for the international yoga day on 21st june .\n",
            "my dear young friends , the country celebr@@ ates national sports day on the 2@@ 9@@ th of august .\n",
            "and its equ@@ ally true that our hear@@ t@@ felt thoughts do bring about a change of se@@ ason in some@@ ones life , fi@@ gur@@ ati@@ vely speaking .\n",
            "almost immedi@@ ately after its launch , our satellite has s@@ ent back some p@@ ic@@ tures .\n",
            "a fes@@ tive atmo@@ sphere regarding cleanliness got ge@@ ared up .\n",
            "if we rec@@ all lord buddha , a fact comes to light that his birth , his en@@ li@@ gh@@ ten@@ ment and his maha@@ par@@ in@@ ir@@ v@@ ana , all three happ@@ ened under a t@@ ree .\n",
            "of course , on the other hand cham@@ pi@@ on nad@@ al too show@@ ered glo@@ wing pra@@ ise on d@@ an@@ i@@ ils g@@ ame .\n",
            "they have brought me up .\n",
            "there are many who rus@@ h to d@@ on@@ ate b@@ lo@@ od , when required .\n",
            "ol@@ y@@ mp@@ ic games were held at the same venue only a few days ago .\n",
            "students sub@@ m@@ it themselves to the t@@ ut@@ el@@ age of prof@@ ess@@ ors v@@ is a v@@ is the sha@@ do@@ w of their par@@ ents .\n",
            "all these things were brought to light and i offered my hear@@ t@@ felt appreciation to india today group for their commend@@ able efforts .\n",
            "my dear fel@@ low citizens , let me beg@@ in by wish@@ ing nam@@ as@@ kar to all of you .\n",
            "yo@@ ga@@ has bro@@ k@@ en bar@@ ri@@ ers of c@@ aste , cre@@ ed and ge@@ o@@ grap@@ hy to un@@ ite the people of the entire world , which is the very ess@@ ence of the real s@@ enti@@ ment inn@@ ate to v@@ as@@ ud@@ ha@@ i@@ v@@ a k@@ ut@@ um@@ b@@ ak@@ am that we have follow@@ ed in le@@ tter and spirit over centur@@ ies .\n",
            "some@@ times ut@@ en@@ si@@ l ha@@ w@@ k@@ ers and clo@@ th sel@@ l@@ ers too p@@ ass by our hom@@ es .\n",
            "he had the courage to show a mir@@ r@@ or to the bri@@ tis@@ h about their wr@@ ong do@@ ings .\n",
            "there is a mo@@ od of fes@@ tivity and with this the prepar@@ ations for di@@ wal@@ i also beg@@ in .\n",
            "india bec@@ ame the four@@ th country in the world , pos@@ s@@ ess@@ ing this capacity . and now , on the 2@@ 2nd of july , the nation wat@@ ched with pride chand@@ ra@@ y@@ a@@ an ii taking str@@ id@@ es into space from s@@ ri@@ har@@ i@@ ko@@ ta .\n",
            "we can feel the ess@@ ence of these ide@@ als in his com@@ posi@@ tions . they are relevant inspir@@ ing even in modern times .\n",
            "the very men@@ tion of e@@ aster re@@ min@@ ds us of the inspir@@ ational pre@@ ach@@ ing of lord j@@ es@@ us ch@@ r@@ ist which has always im@@ pres@@ sed on man@@ kind the message of peace , harmony , justice , mer@@ c@@ y and comp@@ as@@ sion .\n",
            "but i have been con@@ fron@@ ted with a new ty@@ pe of question , about which earlier i had never given a thought .\n",
            "come , let us p@@ ledge to come together to wi@@ pe out these e@@ vil custom@@ s from our social f@@ ab@@ ric@@ .@@ . .@@ .@@ .\n",
            "this is a good beginning .\n",
            "come let us join water conservation , and invol@@ ve our@@ selves in making a l@@ ist of more and more innovative me@@ th@@ ods to mo@@ tiv@@ ate people to con@@ serve water .\n",
            "this prov@@ es that when humanity stand@@ s together , it cre@@ ates wond@@ ers .\n",
            "guru nan@@ ak ji under@@ took many significant spirit@@ ual jour@@ ne@@ ys called ud@@ a@@ asi .\n",
            "this occasion is celebrated as national sports day throu@@ gh@@ out the country .\n",
            "right now , a team of the central government officials are in tamil nadu .\n",
            "i express my grati@@ tude to him for this sugg@@ estion .\n",
            "i once again bo@@ w in re@@ ver@@ ence to s@@ ant ra@@ vid@@ as ji .\n",
            "at@@ al@@ ji was a true pat@@ ri@@ o@@ t .\n",
            "those very cham@@ bers have now been tur@@ ned into ex@@ qu@@ is@@ ite mus@@ e@@ um@@ s .\n",
            "ac@@ tu@@ ally from kashmir to kan@@ y@@ ak@@ u@@ mar@@ i and from k@@ ut@@ ch to k@@ am@@ r@@ up and from am@@ rel@@ i to arunachal pradesh , these 3 - 4 months have examin@@ ations g@@ al@@ ore .\n",
            "i was see@@ ing a message on the social media . some@@ one had po@@ sted a p@@ ic@@ ture with the ti@@ t@@ le my her@@ o of the day .\n",
            "my youn@@ ger friends , the days of examin@@ ations are appro@@ ach@@ ing .\n",
            "you will be with me but that al@@ one is not en@@ ou@@ gh .\n",
            "this will be a big help to them .\n",
            "so i thought why not use the term div@@ yan@@ g in@@ ste@@ ad of vik@@ lan@@ g in our country . they are those people who have one or more such organ@@ s which have div@@ in@@ ity , where div@@ ine power f@@ low@@ s which we nor@@ m@@ al bo@@ di@@ ed people do not have . i like this term .\n",
            "see , the ex@@ am you are going to appe@@ ar at is the ex@@ am of what you have stu@@ di@@ ed during this whole year .\n",
            "on 2@@ 6th january we celebrate republic day . can we citizens and school and college students take up the initiative of clean@@ ing the statue of any great men inst@@ alled in our city , of clean@@ ing the pre@@ mis@@ es , we can do best of cleanliness and best of dec@@ oration on the occasion of 2@@ 6th january . and i am not s@@ aying this on government lines .\n",
            "when i shared my inn@@ er thoughts with you during last months mann ki baat , i had requ@@ ested every@@ body to s@@ end their experiences and suggestions to me on narendra modi app .\n",
            "it is a unique sports tour@@ na@@ ment me@@ ant for young canc@@ er sur@@ vi@@ v@@ ors only they who have emer@@ ged figh@@ ting canc@@ er , can take part .\n",
            "ple@@ ase do be concerned about this .\n",
            "the up@@ coming generations of our nation will always remember his pr@@ ic@@ el@@ ess contribution in maint@@ aining the sanc@@ ti@@ ty of indian democracy .\n",
            "what can be my contribution for this as an individu@@ al\n",
            "in this nov@@ el scheme , during the har@@ v@@ est period , an@@ gan@@ w@@ ad@@ i workers collec@@ t a hand@@ ful of ric@@ e gra@@ in from the people .\n",
            "for this purpose , we have launched de@@ end@@ ay@@ al up@@ ad@@ hy@@ aya gram j@@ yo@@ ti programme .\n",
            "she launched the sat@@ y@@ ag@@ ra@@ h and st@@ op@@ ped e@@ ating and she did that not to ask something for her@@ self , not to demand good clo@@ th@@ es or have sw@@ e@@ ets . this daugh@@ ter m@@ all@@ am@@ ma was ad@@ am@@ ant to have a to@@ il@@ et built in her house .\n",
            "so , when people regist@@ er in e - ge@@ m , all the government departments become aware of them .\n",
            "when we talk about science , the first name that stri@@ k@@ es us is that of bharat r@@ at@@ na si@@ r c . v .\n",
            "during the festival se@@ ason , our festivals , our culture , our traditions are focused upon .\n",
            "she is el@@ der to most of us and has been a wit@@ ness to my@@ ri@@ ad pha@@ ses , different er@@ as the country has pas@@ sed through .\n",
            "marks have become all important .\n",
            "and im happy to see that cleanliness is no long@@ er con@@ fin@@ ed to being a government programme .\n",
            "because by nature , we are not con@@ sci@@ ous . and that is why our ear@@ s he@@ ar when we si@@ t in the air@@ c@@ ra@@ ft , but no one re@@ aliz@@ es that these in@@ struc@@ tions are for me .\n",
            "to beg@@ in with , hear@@ ti@@ est congratulations on your vic@@ tory in the elections , with a huge marg@@ in .\n",
            "i li@@ ked qu@@ ite a few aspects of this book , but i noti@@ ced that it cont@@ ains may mantr@@ as for students , but for par@@ ents and teachers this book does not have much .\n",
            "i congratulate no@@ or je@@ han and ee t@@ v that they presented this effort which was going on in a remote cor@@ ner of kan@@ pur to the country and the world .\n",
            "some@@ one resear@@ ching on climate change or working on ancient indian tex@@ ts , some@@ one dri@@ ving a tr@@ uc@@ k for li@@ vel@@ i@@ hood , who w@@ ent on to construc@@ t a gur@@ ud@@ war@@ a or has built a mo@@ s@@ que wher@@ ever our people are , they have in their own way em@@ bel@@ lish@@ ed or ad@@ or@@ ned the land of their adop@@ tion .\n",
            "i congratulate the election commission and every person connected with the elec@@ tion@@ e@@ ering process and salute the aware vo@@ ters of india .\n",
            "i wish to congratulate those who took out time for an in dep@@ th an@@ aly@@ sis of our work , there were some pra@@ ises some support and some@@ times shor@@ t@@ com@@ ings were also poin@@ ted out , i understand the importance of all these things .\n",
            "he accord@@ ed the highest signific@@ ance to the spirit of service .\n",
            "this gra@@ in is used to make pi@@ ping ho@@ t food for children and women .\n",
            "to construc@@ t 10@@ ,000 house@@ hold to@@ ile@@ ts in 7@@ 1 gram panchay@@ ats in those hundred hours .\n",
            "i can say that 21st june , the international yoga day affected me in the same manner . at that time , when i proposed the international yoga day , it was just an idea .\n",
            "what should be done by the civil society\n",
            "your friends , your family , your sur@@ r@@ ound@@ ings , and environment , all these can prev@@ ent you from going into de@@ pres@@ sion and if you un@@ fortun@@ ately has g@@ one into it , they can also pul@@ l you out of it .\n",
            "it felt as if we were celebr@@ ating di@@ wal@@ i together with a hundred and twenty five crore indians .\n",
            "each one bec@@ ame committed to do something .\n",
            "they have s@@ ent in some re@@ ally good suggestions .\n",
            "five years from now , we will celebrate 75 years of indi@@ as independence .\n",
            "it is a practi@@ ce in the entire country and tra@@ di@@ tion@@ ally we follow this me@@ th@@ od of bur@@ ning off the remains of the cr@@ ops or the str@@ a@@ w .\n",
            "i can see some good results , cleanliness can now be witn@@ essed in two parts .\n",
            "with the help of technology , through the video bridge even a single moment sp@@ ent@@ with the beneficiaries was very en@@ jo@@ y@@ able , very mo@@ tiv@@ ational and provided satisfaction to work more .\n",
            "we always feel the contribution of our teachers in our lives .\n",
            "i congratulate and fel@@ ic@@ it@@ ate the young her@@ o@@ es and their families who were hon@@ ou@@ red with various g@@ all@@ an@@ try awards on the eve of this republic day .\n",
            "i have hear@@ d this year also , this r@@ ace is being plann@@ ed at many places . people are exc@@ ited to be prepar@@ ing for this r@@ ace .\n",
            "but i would like to tel@@ l you that gur@@ u@@ de@@ v was also a pa@@ inter .\n",
            "we are still left with about three wee@@ ks for the international yoga day .\n",
            "so my best wishes to you for the coming mon@@ soon se@@ ason . may all of you en@@ joy the ra@@ ins .\n",
            "with this i got an idea and i am requ@@ esting you all to s@@ end suggestions about what should be the t@@ op@@ ics for my 15@@ th august spe@@ ech .\n",
            "we will play a big role in the transformation of the nation , if we make a collective effort .\n",
            "everyone does not have this facility .\n",
            "there was a time when the children in the family w@@ ent out to play , the mother would first ask , when will you come back home\n",
            "another thing is that many people come to meet me who are from all the sections of the society .\n",
            "it is also an am@@ az@@ ing co@@ in@@ cid@@ ence that the month which witn@@ essed the first struggle for independence was the month in which ve@@ er sav@@ ar@@ kar ji was bor@@ n .\n",
            "every mor@@ ning , first of all , people look for new@@ sp@@ ap@@ ers , tel@@ e@@ vision , news and social media to ch@@ ec@@ k indian play@@ ers w@@ inning med@@ als .\n",
            "i have a lot of faith in yoga .\n",
            "therefore promotion of yoga is a great example of social service .\n",
            "this happ@@ ened in the vi@@ z@@ i@@ an@@ ag@@ ar@@ am district of andhra pradesh .\n",
            "it included schools within and out@@ side india .\n",
            "when a poor man go@@ es for an ordinary job , he fac@@ es so many problems to get a dec@@ ent re@@ ference and has to go through through a team of ag@@ ents who take money from him in exchange for the position he is ap@@ pl@@ ying for .\n",
            "i requ@@ est you to re@@ pe@@ at this task in mann ki baat .\n",
            "the works done during these years were t@@ ested on every tou@@ ch stone .\n",
            "but i was am@@ az@@ ed when every@@ body in the house , friends and my teachers said the same thing , o@@ h dear , if your had sec@@ u@@ red just 4 more marks , you would have made it to 90 .\n",
            "people are making their to@@ ile@@ ts clean and col@@ our@@ ful , by having them p@@ ain@@ ted and other r@@ en@@ nov@@ ations , you will find lo@@ ts of p@@ ho@@ to@@ s of such to@@ ile@@ ts particip@@ ating in the \" clean be@@ au@@ ti@@ ful to@@ il@@ et \" cont@@ est spread from kan@@ ya k@@ u@@ mar@@ i to k@@ ut@@ ch@@ h till k@@ am@@ r@@ up on social media .\n",
            "i salute the div@@ ine fl@@ ame respec@@ t@@ fully .\n",
            "so you can understand the dep@@ th of our relationship .\n",
            "the pres@@ s was comple@@ t@@ ely mu@@ f@@ f@@ led .\n",
            "i offer my tributes to d@@ hy@@ an ch@@ and ji and wish to re@@ mind you all about his in@@ valu@@ able contribution .\n",
            "and just as technology pl@@ ays an increas@@ ingly bigg@@ er role in our economic system , it also inv@@ ites those who mis@@ use it .\n",
            "a@@ fro@@ z sha@@ h gather@@ ed the people of the area into a peop@@ les collective and gave it the sha@@ pe of a peop@@ les movement in itself an inspir@@ ing example .\n",
            "there were improv@@ ements in the infrastructure to increase tourism .\n",
            "make yoga a part of your life .\n",
            "they are organ@@ ic fertiliz@@ er themselves .\n",
            "earlier they had to sp@@ end money on cle@@ aring those st@@ um@@ ps , now those very st@@ um@@ ps are a source of revenue .\n",
            "it is true that sports games that were once a part par@@ c@@ el of every chil@@ ds life , in every lan@@ e in the neighb@@ our@@ hood , are f@@ ad@@ ing into o@@ bli@@ vi@@ on .\n",
            "if you have any ideas for the third international yoga day , ple@@ ase do s@@ end me your suggestions and guid@@ ance through my mobile application .\n",
            "you too would feel that , you w@@ oul@@ d@@ ve committed the same m@@ ist@@ ake .\n",
            "y@@ est@@ er@@ day , krish@@ na - jan@@ ma ma@@ ho@@ t@@ sa@@ v , the festival of the birth of lord krish@@ na was celebrated throu@@ gh@@ out india .\n",
            "i was re@@ ally im@@ pres@@ sed with the pro@@ w@@ ess of that div@@ yan@@ g young man .\n",
            "we are famil@@ i@@ ar with the lines of the sh@@ lok - sh@@ u@@ bha@@ m kar@@ o@@ ti kal@@ yan@@ am , a@@ ar@@ o@@ gy@@ am d@@ han@@ s@@ amp@@ ad@@ a@@ a .\n",
            "they beg@@ in work early in the mor@@ ning , not know@@ ing when they will w@@ ind up at ni@@ ght .\n",
            "accord@@ ing to one estim@@ ate , more than 35 crore people in the world suf@@ fer from de@@ pres@@ sion .\n",
            "i am happy that the prom@@ ise that we made of introduc@@ ing important schemes , we will do so around di@@ wal@@ i and d@@ han@@ ter@@ as when people bu@@ y speci@@ ally bu@@ y gold .\n",
            "it was afghan@@ ist@@ ans first international mat@@ ch and its a matter of honour for us that this historic mat@@ ch for afghanistan was played with india .\n",
            "people are devo@@ ting hard work and contribu@@ ting finan@@ ci@@ ally as well to the mass movement that has been started in maharashtra , .\n",
            "in a few days from now , maha@@ shi@@ v@@ r@@ ar@@ tr@@ i will be celebrated , and that too on a mon@@ day , and when a shi@@ v@@ r@@ at@@ ri f@@ alls on a mon@@ day , it becom@@ es all that special in our heart of our hear@@ ts\n",
            "he played a far more significant role , when it came to a small region such as lak@@ sha@@ d@@ we@@ ep too .\n",
            "today , thousands of poor people are tre@@ ated free of cost in this hospital that has come up through hard - work .\n",
            "we are all wit@@ ness to the fact that how within a year a place developed as a world fam@@ ous tourism destination .\n",
            "10 days ago , mother india had to be@@ ar the loss of many of her val@@ i@@ ant s@@ ons .\n",
            "this has been a very sub@@ l@@ ime experience for me .\n",
            "many years ago , what lord vi@@ sh@@ we@@ sh@@ war@@ a had done for democracy and empowerment of women can make for a very good subject to study in the world .\n",
            "this war was also f@@ ou@@ ght by the daugh@@ ters of our country , who were new@@ ly mar@@ ri@@ ed and their h@@ en@@ na were still f@@ res@@ h on their h@@ ands .\n",
            "t . us@@ has fe@@ at , l@@ al@@ ita bab@@ ar qu@@ ali@@ fied for the fin@@ als in tr@@ ack and field .\n",
            "these fin@@ est examples coming up to me show how our society is bringing change in human values and human mind and how the citizens of this country are taking the nation forward .\n",
            "they collec@@ t clo@@ th@@ es , sw@@ e@@ ets , food from hom@@ es , look for the ne@@ edy and hand them over dis@@ cre@@ et@@ ly .\n",
            "but , after recei@@ ving this ph@@ one call , i also started thin@@ king that the point made there was per@@ t@@ in@@ ent .\n",
            "banks are the ma@@ inst@@ re@@ am of the economy and to reach this facility to the hom@@ es of the poor , the programme of bank m@@ it@@ ra is being strength@@ ened .\n",
            "each and every indian would be proud of these daugh@@ ters .\n",
            "this campaign is resul@@ ting in ra@@ ising awareness levels in people as far as sanitation and cleanliness , along with f@@ it@@ ness levels .\n",
            "and those gi@@ f@@ ted with ex@@ tra - ordinary intelligence do not let in@@ qu@@ i@@ si@@ tiv@@ eness to remain just that they beg@@ in further qu@@ es@@ tioning it , look around for new@@ er qu@@ er@@ ies , and try to create new real@@ ms in the spirit of en@@ qu@@ ir@@ y .\n",
            "at present , wh@@ en@@ ever im tour@@ ing , it is my sin@@ c@@ ere effort to meet people beneficiaries of pm@@ j@@ ay scheme under ayushman bhar@@ ats um@@ b@@ rel@@ la .\n",
            "rou@@ t@@ ine t@@ as@@ ks of a modern day la@@ dies cl@@ u@@ b shall be taken up , but besides that , let all members of the la@@ dies cl@@ u@@ b come together per@@ form an activity of service\n",
            "so friends lo@@ ts of good wishes to you all .\n",
            "witn@@ ess@@ ing the mi@@ ght of the collective is in itself hear@@ tw@@ arm@@ ing satis@@ f@@ ying .\n",
            "on the one hand , brave hear@@ ts like bhag@@ at singh , su@@ k@@ h@@ de@@ v and raj@@ guru , were inspir@@ ing the youth towards an armed revolution .\n",
            "sports means , physical f@@ it@@ ness , mental al@@ er@@ t@@ ness and person@@ ality enhanc@@ ement . what el@@ se does one need\n",
            "and not only this he is providing employment to ten to fif@@ t@@ een people .\n",
            "when the nation was celebr@@ ating di@@ wal@@ i i was at si@@ ach@@ in .\n",
            "these officials would visit the districts , villages and meet the local government officials and talk to peop@@ les represent@@ atives and the citizens of those regions .\n",
            "on the 1@@ 6th of may , a team com@@ pr@@ ising five tribal students of an ash@@ ram school in chand@@ rap@@ ur , maharashtra - man@@ e@@ e@@ sha d@@ h@@ ur@@ ve , pr@@ am@@ esh ale , um@@ ak@@ ant mad@@ ha@@ vi , k@@ a@@ vid@@ as k@@ at@@ mod@@ e and vik@@ as so@@ y@@ am - sc@@ al@@ ed the wor@@ l@@ ds highest pe@@ ak .\n",
            "two days back we completed the 50 years of the 19@@ 65 war . and wh@@ en@@ ever we talk of the 65 war it is natural that we remember l@@ al ba@@ ha@@ du@@ r sha@@ str@@ i .\n",
            "the ministry of dr@@ in@@ king water and sanitation has organized , the swach@@ ch san@@ kal@@ p se swach@@ ch@@ a sid@@ d@@ hi pr@@ ati@@ yo@@ g@@ ita , from the resolve of cleanliness to att@@ aining cleanliness competition com@@ pr@@ ising an ess@@ ay competition , a short fil@@ m making competition and a pa@@ int@@ ing competition .\n",
            "g@@ ay@@ at@@ ri , a young gir@@ l from de@@ h@@ r@@ ad@@ un , who is a stud@@ ent of class 11 , has p@@ hon@@ ed in with a message : - est@@ e@@ em@@ ed princip@@ al , prime minister si@@ r , my respec@@ t@@ ful greetings to you .\n",
            "there have been very few steps which have been taken from the per@@ spec@@ tive of social security but by way of these three schemes we are taking a big le@@ a@@ p .\n",
            "this time when i was thin@@ king about ' mann ki baat ' , i was sure that a lot of qu@@ estions would crop up about this subject and th@@ ats what ex@@ ac@@ tly happ@@ ened . in the last few wee@@ ks wher@@ ever i w@@ ent and met people , ' man v@@ s .\n",
            "expres@@ sing support for this , many people have written to me , s@@ ent me le@@ t@@ ters , shared f@@ it@@ ness mantr@@ a - f@@ it india st@@ ories on social media . a gent@@ le@@ man sh@@ r@@ im@@ an sha@@ shi@@ k@@ ant b@@ hon@@ sal@@ e , sharing his p@@ ho@@ to by the sw@@ im@@ m@@ ing po@@ ol , has written , my we@@ ap@@ on is my body , my el@@ ement is water , my world is sw@@ im@@ m@@ ing .\n",
            "in a way , s@@ on@@ als mar@@ ri@@ age in an ever@@ la@@ sting st@@ ory of lo@@ ve for nature , i congratulate s@@ onal and sh@@ r@@ im@@ an m@@ hat@@ re ji for this innovative effort .\n",
            "once while i was going through your com@@ ments , i came across an inter@@ esting point by atis@@ h mu@@ k@@ hop@@ ad@@ hy@@ ay ji .\n",
            "the whole world is looking at india as a hub for investment innovation and development .\n",
            "how to increase public participation\n",
            "let me keep on l@@ ist@@ ening to your thoughts , let me keep on understanding and ch@@ er@@ ish them .\n",
            "you can fil@@ m a two - three - min@@ ute mov@@ i@@ e that insp@@ i@@ res cleanliness .\n",
            "we should live with hope , we should move ahead with confidence .\n",
            "today india has emer@@ ged as a bri@@ ght spo@@ t in the global economy and today the highest foreign direct investment or fdi in the world , is f@@ low@@ ing to india .\n",
            "after the nepal ear@@ th@@ qu@@ ake i tal@@ ked to pak@@ ist@@ ans prime minister n@@ aw@@ a@@ z sh@@ ari@@ f .\n",
            "we can only do this when we our@@ selves are proud of our traditions .\n",
            "our sc@@ ri@@ p@@ tures have provided great guid@@ ance with respect to this subject .\n",
            "there are many t@@ op@@ ics on this app , such as what is the state of the market , what is the position of the wh@@ ol@@ es@@ ale market , which cr@@ ops are doing well these days , which are the appropriate p@@ esti@@ cid@@ es .\n",
            "he s@@ ays that during ex@@ ams , par@@ ents have a vital role to play .\n",
            "i do hope that you will pay a visit to the national soldiers memorial and the national police memorial .\n",
            "the s@@ mil@@ es thus ac@@ cru@@ ed on the fac@@ es of num@@ er@@ ous under@@ privile@@ ged families will more than double your joy during festiv@@ al@@ s.@@ . .@@ .@@ . your fac@@ es will be li@@ t up with a glo@@ w that will bri@@ gh@@ ten your di@@ wal@@ i immen@@ sely .\n",
            "come , le@@ ts make an endeav@@ our in this direction .\n",
            "after l@@ ist@@ ening to my thoughts , ple@@ ase do not h@@ es@@ it@@ ate in sharing your thoughts or adv@@ ice to me , i will appreci@@ ate that your suggestions keep f@@ low@@ ing coming .\n",
            "she not only tr@@ ained the women of the village in pursu@@ ing other g@@ ain@@ ful means of employment , but most import@@ antly she integrated technology with agriculture and my dear countrymen , it has per@@ ha@@ ps happ@@ ened for the first time , that in the rol@@ l of p@@ ad@@ ma awards be@@ sto@@ wed this year , 12 farmers have been the re@@ cip@@ i@@ ents .\n",
            "there is a very inter@@ esting fact about gur@@ u@@ de@@ v that in 19@@ 13 he was not only the first asian to recei@@ ve the no@@ be@@ l pr@@ ize , but k@@ ni@@ gh@@ th@@ o@@ od was also con@@ ferred upon him by the bri@@ tis@@ h .\n",
            "she has formed a sam@@ iti ( committee ) of women who manu@@ fac@@ ture lan@@ tern@@ s that run on solar energy . they r@@ ent the lan@@ ter@@ n for rs .\n",
            "what i tel@@ l you about them will f@@ ill your hear@@ ts with pride ver@@ ve as well .\n",
            "we know that mahatma gandhi was bor@@ n in 18@@ 6@@ 9 .\n",
            "sh@@ ob@@ ha ji , thank you very much for pay@@ ing attention to this sh@@ ining example of indi@@ as pride .\n",
            "ac@@ tu@@ ally , p@@ and@@ har@@ pur war@@ i is an am@@ az@@ ing journey in itself .\n",
            "mr . par@@ im@@ al sha@@ h from than@@ e talks about educational reforms on my port@@ al my@@ go@@ v . in .\n",
            "why d@@ on@@ t we compe@@ te with our@@ selves\n",
            "poor people requ@@ ir@@ e pul@@ ses and some amount of oil to prep@@ are ve@@ get@@ ab@@ les for their food .\n",
            "the life of many a med@@ al w@@ inn@@ er is also immen@@ sely inspir@@ ing .\n",
            "i am tal@@ king about n@@ one other than chand@@ r@@ as@@ e@@ k@@ har az@@ ad .\n",
            "their pro@@ di@@ gi@@ ous vir@@ tu@@ os@@ ity cr@@ ad@@ les our social f@@ ab@@ ri@@ c , ensuring its bri@@ ght future .\n",
            "people are sharing vide@@ os of f@@ it@@ ness challenge on social media they are t@@ ag@@ ging each other to spread the challenge .\n",
            "but earlier she would b@@ orrow money at high r@@ ates and b@@ are@@ ly man@@ age her business .\n",
            "dear countrymen , we recently celebrated the festival of nav@@ r@@ at@@ ri and vi@@ jay@@ ad@@ ash@@ m@@ i .\n",
            "through mann ki baat , i once again have been bl@@ essed with the opportunity to be face - to - face with you .\n",
            "this is a significant , c@@ utting ed@@ ge compe@@ t@@ ency in the ar@@ en@@ a of security .\n",
            "modi ji : lo@@ ts of good wishes to y@@ oul@@ ata di@@ di : greetings to you .\n",
            "chand@@ rash@@ e@@ k@@ har az@@ ad put his life on the st@@ ake , but he never bo@@ wed in front of the foreign rule .\n",
            "my dear countrymen , this year mahatma gand@@ his 15@@ 0th birth anniversary celebrations will beg@@ in .\n",
            "we have been hear@@ ing in our sha@@ a@@ str@@ as , our hol@@ y tex@@ ts , k@@ sha@@ ma@@ ve@@ er@@ as@@ ya b@@ hu@@ shan@@ am , that is , for@@ giv@@ eness is the ad@@ or@@ n@@ ment of the brave .\n",
            "be it the government , institutions , society , citizens we all have to come together to make this happ@@ en .\n",
            "d@@ are@@ de@@ vil st@@ un@@ ts per@@ formed by them was a@@ we inspir@@ ing for our foreign gu@@ ests .\n",
            "once again i have the opportunity of connec@@ ting with you through mann ki baat .\n",
            "festivals l@@ end a sw@@ e@@ et@@ ness to our relation@@ ships , bring a warm@@ th of toge@@ th@@ ern@@ ess in the family and fo@@ ster bro@@ ther@@ hood in society .\n",
            "students of age between seven to sev@@ ent@@ een took up this task .\n",
            "in such a difficul@@ t situation , our brave heart soldiers are not only protec@@ ting the b@@ ord@@ ers of the country , but are also r@@ un@@ ning a sw@@ ac@@ ch si@@ ach@@ en campaign in that ar@@ en@@ a .\n",
            "i was see@@ ing their z@@ e@@ al and enthusias@@ m . the facilities in our country are qu@@ ite less as comp@@ ared to other nations , but in@@ ste@@ ad of com@@ pl@@ aining they were just sharing their joy and exc@@ it@@ ement .\n",
            "the human cha@@ in that commen@@ ced formation from gandhi m@@ ai@@ d@@ an in pat@@ na g@@ ained momentum , tou@@ ching the state b@@ ord@@ ers .\n",
            "if you have g@@ one on hol@@ id@@ ays with your par@@ ents , then try and remember what you sa@@ w .\n",
            "there are hom@@ es where sw@@ e@@ et@@ me@@ ats are left to ro@@ t there are hom@@ es where children year@@ n for a sw@@ e@@ et\n",
            "my dear countrymen , we are contin@@ ually see@@ ing that our daugh@@ ters are bringing la@@ u@@ rel@@ s to the country in all the fields be it education , economic activities , social sp@@ her@@ es or in sports they are sc@@ al@@ ing new heights .\n",
            "we need not wor@@ ry about keeping an account the account will be maint@@ ained aut@@ om@@ ati@@ c@@ ally .\n",
            "this li@@ ver was brought from l@@ uc@@ know by making special arran@@ ge@@ ments .\n",
            "i inv@@ ite you to come and become a part of nation building .\n",
            "in order to ensure that their fel@@ low countrymen could s@@ le@@ ep peac@@ ef@@ ul@@ ly , these brave s@@ ons to@@ iled rel@@ ent@@ l@@ ess@@ ly , day or ni@@ ght .\n",
            "it is an in@@ justice to the poor .\n",
            "you must have experi@@ enc@@ ed that when one part of your body fe@@ els a l@@ it@@ t@@ le b@@ it of p@@ ain , the entire body do@@ es@@ n@@ t feel well .\n",
            "recently our daugh@@ ters per@@ formed br@@ il@@ li@@ antly in the w@@ o@@ men@@ s cr@@ ick@@ et world c@@ up .\n",
            "pr@@ an@@ am pradhan mantri ji , i am dr .\n",
            "so this year we should again , during the gan@@ esh festival , organ@@ ise ess@@ ay competi@@ tions , have open discussions and remember the contribu@@ tions of lo@@ k@@ man@@ ya ti@@ la@@ k .\n",
            "earlier , we used to keep our gold in the loc@@ k@@ ers and paid for the loc@@ k@@ ers our@@ selves .\n",
            "i know that these hab@@ its do not change ov@@ ern@@ i@@ ght , and when we talk about it , we inv@@ ite cri@@ tic@@ ism .\n",
            "we need to change this min@@ d@@ set .\n",
            "what i take half an hour to say , he s@@ ays in a few min@@ utes .\n",
            "any@@ way , young friends , you are eng@@ ro@@ s@@ sed in prepar@@ ing for your ex@@ ams and here i am , eng@@ aging you in matters close to my heart .\n",
            "but some@@ times when a cal@@ am@@ ity stri@@ k@@ es during fes@@ ti@@ vities then it is very p@@ ain@@ ful and it fe@@ els even more .\n",
            "such individu@@ als l@@ end power to our nation .\n",
            "today i would like to say something especially to mo@@ thers and sisters also , since health and well@@ being have been a major part of our con@@ vers@@ ation today .\n",
            "we hear@@ d through the experiences of ri@@ p@@ ud@@ am@@ an ji , how this programme should be organ@@ ised\n",
            "in many parts of the country , our youth and farmers demonstr@@ ated their bra@@ very wh@@ il@@ st standing up against the in@@ justice .\n",
            "the v@@ a@@ an@@ ch@@ e gujarat campaign carried out in gujarat was a successful experi@@ ment .\n",
            "i would like that in future , through the niti aayog , the exc@@ ep@@ tion@@ ally successful efforts of these states should be app@@ li@@ ed to other states also .\n",
            "i do it every time , but this di@@ wal@@ i was an enti@@ re@@ ly different experience .\n",
            "may we continue to be inspired by such in@@ cid@@ ents of our history .\n",
            "after a year of hard work , one has the opportunity to disp@@ lay ones capabilities , so this should be a festival of joy and enthusias@@ m .\n",
            "it was resol@@ ved to double the number of ti@@ g@@ ers wor@@ l@@ d@@ wide by 2022 .\n",
            "today , you can see gan@@ esh p@@ and@@ als in almost every stre@@ et of various cities .\n",
            "but on retur@@ ning , i pur@@ cha@@ sed the book and read it more than tw@@ ice .\n",
            "farmers produc@@ ing gra@@ ins have also become associated with this trust .\n",
            "and wher@@ ever he w@@ ent he won hear@@ ts through his str@@ ai@@ ght@@ for@@ war@@ d@@ ness , hum@@ il@@ ity and sim@@ plic@@ ity .\n",
            "i extend my hear@@ ti@@ est greetings to you all , un@@ ti@@ l the next ep@@ is@@ od@@ e of mann ki baat , when i shall share my thoughts with you once again .\n",
            "recently indi@@ as b@@ ad@@ min@@ t@@ on play@@ er , k@@ id@@ amb@@ i sh@@ ri@@ k@@ ant has brought gl@@ ory to the nation by w@@ inning indonesia open .\n",
            "im@@ bi@@ b@@ ing cleanliness as a nature is not en@@ ou@@ gh .\n",
            "this se@@ ems to be the re@@ ason as to how he could develop this app which is most su@@ ited to needs of the village .\n",
            "i will try my best , if possible , to take out time tw@@ ice a month or even once to speak with you .\n",
            "do give it a thought . it is not like you need lo@@ ts of cl@@ ass@@ es , a great tr@@ ain@@ er , h@@ ef@@ ty fe@@ es or a big budget for this .\n",
            "earlier , the cleanliness stat@@ us was presented before the countrymen after conduc@@ ting a surve@@ y of 7@@ 3 cities .\n",
            "i ur@@ ge my countrymen , especially the youth of our country and those who have won pr@@ iz@@ es under l@@ uc@@ k@@ y gra@@ ha@@ k yojana or dig@@ i - dhan v@@ y@@ ap@@ ar yojana to become amb@@ ass@@ ad@@ ors of these schemes on their own .\n",
            "one of the foc@@ al poin@@ ts of ' par@@ i@@ k@@ sha pe char@@ ch@@ a ' was that there were li@@ vely interac@@ tions on various t@@ op@@ ics related with the entire process of examin@@ ations .\n",
            "or@@ g@@ an d@@ on@@ ation is the biggest d@@ on@@ ation .\n",
            "and therefore this bur@@ den of han@@ k@@ ering for marks h@@ ind@@ ers us some@@ times from going in the right direction .\n",
            "my experience regarding youth is different .\n",
            "we will promote and also continue doing such work and the entire world will wat@@ ch it , entire nation will wat@@ ch it and our new generation will also wat@@ ch it .\n",
            "shri k@@ hand@@ u mar@@ ut@@ i m@@ hat@@ re , a farmer of nar@@ ay@@ an@@ pur village of of j@@ un@@ ner tal@@ u@@ ka of p@@ une got his gr@@ and@@ daugh@@ ter s@@ onal mar@@ ri@@ ed in a very inspir@@ ing manner .\n",
            "his brothers self - est@@ e@@ e@@ m got aw@@ ak@@ ened he att@@ ained self - re@@ alization through this and even a lion brought up among sh@@ e@@ ep started ro@@ aring like a lion .\n",
            "le@@ ts see whether i succ@@ e@@ ed or not .\n",
            "for me , even read@@ ing these mess@@ ages prov@@ ed to be a source of inspiration .\n",
            "this de@@ ed f@@ ound a men@@ tion in gu@@ inn@@ ess book of world rec@@ or@@ ds .\n",
            "the c@@ ar festival of lord j@@ ag@@ ann@@ ath , the r@@ ath y@@ at@@ ra , is being celebrated in several parts of the country with great pi@@ ety and fer@@ v@@ our .\n",
            "our jaw@@ ans work in the field of security .\n",
            "i was just gathering data of the so@@ wing of this crop se@@ ason .\n",
            "the s@@ mo@@ k@@ er also kno@@ ws this fact and so do people around .\n",
            "with his team , he beg@@ an with the gar@@ b@@ age b@@ ins of l@@ od@@ i gar@@ den .\n",
            "i requ@@ est the l@@ ist@@ en@@ ers of mann ki baat to visit p@@ and@@ har@@ pur war@@ i at least once , wh@@ en@@ ever they get a ch@@ ance .\n",
            "i have provided further im@@ pe@@ t@@ us to simp@@ lif@@ ying this system even more .\n",
            "friends , if i ask you to start doing yoga from tomorrow mor@@ ning then that would be un@@ f@@ air .\n",
            "what the wor@@ d , sat@@ y@@ ag@@ ra@@ ha means , what dis@@ agreement can me@@ an , what non - cooperation in the face of such a vast emp@@ ir@@ e could be gandhi ji established a comple@@ t@@ ely new vision of re@@ sist@@ ance , not through m@@ ere words , but through a successful experi@@ ment .\n",
            "she ha@@ ils from a small village t@@ enda ga@@ on in bal@@ as@@ ore district where most of the population is scheduled tri@@ be .\n",
            "how the common man of this country ke@@ eps on in@@ s@@ pr@@ ing me all the time\n",
            "this is what is called the se@@ eds of success in@@ her@@ ent in f@@ ail@@ ures .\n",
            "my dear countrymen , on the one hand , we take pride in yoga , on the other we can also take pride in our achievements in space science .\n",
            "and i would like to men@@ tion to my countrymen , that festivals like nav@@ ar@@ at@@ ri in gujarat , or d@@ ur@@ ga ut@@ sa@@ v in bengal are tre@@ mend@@ ous tourist attr@@ actions .\n",
            "let us show respect to such individu@@ als .\n",
            "digital transactions in india are witn@@ ess@@ ing a very rapid sur@@ ge .\n",
            "it is the result of our conc@@ er@@ ted efforts and commitment that yoga has now become a mass movement and reached every house .\n",
            "a small gi@@ ft by mahatma gandhi , has become a part of her life and a part of history .\n",
            "i get to know as to what all is happ@@ ening on the ground , in our villages and in the hear@@ ts and min@@ ds of the poor .\n",
            "for the first time constitu@@ tional saf@@ e@@ gu@@ ards gu@@ aran@@ te@@ ed that the president could only announ@@ ce the emer@@ g@@ ency upon the written recommend@@ ation of the cabinet , and that the period of emer@@ g@@ ency could not be extended more than six months at any st@@ ret@@ ch of time .\n",
            "accord@@ ing to him the industry was an effective medium by which jobs could be made available to the po@@ o@@ rest of the poor and the po@@ o@@ re@@ r .\n",
            "but now that h@@ es come to know of the jan a@@ us@@ ha@@ d@@ hi k@@ endra , he has be@@ gu@@ n pur@@ cha@@ sing medic@@ ines from there and his exp@@ ens@@ es have been reduc@@ ed by about 75 .\n",
            "in a way , this is a sav@@ ing for the poor , this is his empowerment for the future .\n",
            "but , if we become aware at an early stage and put a st@@ op to it , then a lot can be sav@@ ed .\n",
            "but when the jan@@ ata party won the general elections in 197@@ 7 , he bec@@ ame the prime minister of the country .\n",
            "it should be the collective voice of 125 crores countrymen .\n",
            "ambedkar stressed that lord buddha has been a great inspiration in his social ph@@ il@@ os@@ op@@ hy . bab@@ a sa@@ he@@ b had said my social ph@@ il@@ os@@ op@@ hy may be said to be en@@ sh@@ r@@ ined in three words\n",
            "and those who were not able to succ@@ e@@ ed , i would like to tel@@ l them once again that there is a lot to do in life .\n",
            "i would like to tel@@ l all of you that you must have a positive thin@@ king .\n",
            "even if a dro@@ p of water is w@@ ast@@ ed , then we should feel rem@@ or@@ se and p@@ ain .\n",
            "my dear countrymen , today is 25@@ th september , the birth anniversary of pan@@ dit de@@ en d@@ ay@@ al up@@ ad@@ hy@@ ay ji and his birth cent@@ en@@ ary year commen@@ ces from today .\n",
            "the museum has 4 histor@@ ical exhi@@ b@@ iti@@ ons which disp@@ lay more than 4@@ 50 pa@@ int@@ ings and ar@@ t@@ works which are of three centur@@ ies v@@ int@@ age .\n",
            "there are times when some of your words act as a c@@ at@@ aly@@ st in sh@@ ar@@ p@@ ening my thought process .\n",
            "can gold be conver@@ ted from de@@ ad - money to a live strength or not\n",
            "i once again wish you all the very best on this au@@ sp@@ ici@@ ous festival of di@@ wal@@ i .\n",
            "as far as dis@@ ast@@ ers are concerned , this country has bor@@ n@@ e the br@@ un@@ t of many a natural as well as man made disaster , such as chem@@ ical industrial mis@@ ha@@ ps .\n",
            "y@@ es , for many families , the first part of the year is ex@@ am se@@ ason .\n",
            "and especially , since this place is sit@@ u@@ ated in the mid@@ st of h@@ ill@@ s and fore@@ sts . but , sab@@ r@@ im@@ ala temple in itself is an example to show how this challenge could be conver@@ ted into a san@@ sk@@ ar , a hab@@ it and what a tre@@ mend@@ ous strength public participation has one police offic@@ er p .\n",
            "dear countrymen , there is news of d@@ eng@@ ue every@@ where .\n",
            "programmes to fel@@ ic@@ it@@ ate them should be held throu@@ gh@@ out the country .\n",
            "this is the same n@@ and@@ l@@ al bo@@ se whose ar@@ t@@ work ad@@ or@@ n@@ s our constitution\n",
            "we take pride in the fact that the satell@@ ites developed by our students and s@@ oun@@ ding ro@@ c@@ kets have reached space .\n",
            "to the world which is fill@@ ed with st@@ ress , yo@@ g gi@@ ves the power to lead a bal@@ anc@@ ed life .\n",
            "flo@@ od affected areas are being closely monit@@ o@@ red .\n",
            "pradhan mantri ji , i am ap@@ ar@@ na from p@@ une .\n",
            "many such institutions work hard to promote gan@@ esh id@@ ols made from cl@@ ay and they also prop@@ ag@@ ate it .\n",
            "it is the se@@ ason of festivals .\n",
            "the entire country had come together as one to fight for the cause .\n",
            "yo@@ g@@ esh sain@@ i is an engine@@ er who left his job in americ@@ a and retur@@ ned to serve mother india .\n",
            "on march 2@@ 4@@ th , the world observ@@ ed tu@@ ber@@ cu@@ lo@@ sis day .\n",
            "our country is full of diversity and this wide range of diversity will also in@@ cul@@ c@@ ate vari@@ ations within you as a teach@@ er .\n",
            "if you view the journey of human ev@@ ol@@ ution , you will noti@@ ce that bre@@ ak@@ throu@@ gh@@ s in progress have taken birth in the wom@@ b of some adv@@ ent@@ ure or the other .\n",
            "they are not even min@@ d@@ ful of their sur@@ r@@ ound@@ ings , which mi@@ ght have inc@@ en@@ di@@ ary it@@ ems leading to fir@@ e ac@@ cid@@ ents .\n",
            "yoga is a gu@@ aran@@ tee of well@@ ness and f@@ it@@ ness both .\n",
            "par@@ ents in most parts of the country must be bus@@ y with their chil@@ dr@@ ens examin@@ ations .\n",
            "we can also sm@@ el@@ l the fr@@ ag@@ r@@ ance of ri@@ gh@@ te@@ ous@@ ness .\n",
            "where will you reach then\n",
            "this was done cl@@ an@@ dest@@ in@@ ely , which is not the u@@ su@@ al practi@@ ce .\n",
            "and they are stri@@ ving to bring every year 2 to 3 lakh h@@ ec@@ t@@ are@@ s additional land under mic@@ ro - irrigation .\n",
            "i have received a similar call from mr . y@@ ell@@ app@@ a vel@@ an@@ kar ji from karnataka : - modi ji , nam@@ aste . i am c@@ alling from a village in k@@ op@@ pal district of karnataka .\n",
            "but we still have to work to sa@@ ve our mo@@ thers and our children .\n",
            "so we are full of soldiers on both sides of the family . our soldiers and officers who are gu@@ arding our border have been recei@@ ving these affec@@ tion@@ ate mess@@ ages and all in the army cir@@ c@@ le are getting tre@@ mend@@ ous inspiration and encour@@ agement .\n",
            "what should be the lo@@ go\n",
            "this becom@@ es a source of strength in itself , when the spor@@ t@@ sper@@ son fe@@ els that his 125 crore countrymen are with him , his mor@@ ale gets boo@@ sted .\n",
            "let us all re@@ alize the mantr@@ a of tain ty@@ ak@@ ten bh@@ un@@ j@@ h@@ ita .\n",
            "when they retur@@ n with this p@@ ledge , they can eas@@ ily understand the problems of these states even when they reach delhi .\n",
            "under this , on the 9@@ th of every month , all pre@@ gn@@ ant women will get a ch@@ ec@@ k - up at government health cent@@ ers free of cost .\n",
            "i felt that the han@@ dic@@ ra@@ f@@ ts did not recei@@ ve due attention .\n",
            "whether something is good , l@@ it@@ t@@ le less effective or b@@ ad , wh@@ at@@ ever it is , one has to learn from it and move ahead in life p@@ utting the lear@@ ning from it into practi@@ ce .\n",
            "during the first for@@ t@@ ni@@ ght of march , women and child development ministry along with the ministry of tribal affairs will be laying st@@ ress on the cleanliness campaign .\n",
            "his effort is that no family member should take to s@@ mo@@ king .\n",
            "but i want to share another matter of great pride with you .\n",
            "al@@ ong@@ with being it profes@@ sional , your lo@@ ve for san@@ sk@@ r@@ it has gl@@ add@@ ened me .\n",
            "after the broad@@ c@@ ast of this show , a large number of people have been discus@@ sing about j@@ im cor@@ be@@ t@@ t national par@@ k .\n",
            "rajasthan , des@@ er@@ t area , has celebrated v@@ an ma@@ ho@@ t@@ sa@@ v in a very big way and ple@@ d@@ ged to plant 25 lakhs tre@@ es .\n",
            "this great man has contribu@@ ted a lot in building the unity of india .\n",
            "i had requ@@ ested last time too and now , while rememb@@ ering lo@@ k@@ man@@ ya ti@@ la@@ k , i will once again ur@@ ge all of you to celebrate gan@@ esh ut@@ sa@@ v with great enthusias@@ m and fer@@ v@@ our whole hear@@ t@@ ed@@ ly but in@@ si@@ st on keeping these celebrations ec@@ o - friendly .\n",
            "one such ill@@ u@@ stri@@ ous indian is being hon@@ ou@@ red in v@@ ati@@ can city on 13@@ th october .\n",
            "but , we are now aw@@ a@@ iting the ra@@ ins .\n",
            "how to increase their income\n",
            "my dear countrymen , i am happy that the people of our country are thin@@ king about issues , which are pos@@ ing a challenge not only at the present but also the future .\n",
            "in our culture , we of@@ ten talk of safety of values we now need to re@@ alize the values of safety .\n",
            "in the coming october month , from 1st october to 15@@ th october , dr@@ in@@ king water and sanitation department , panchay@@ ati ra@@ j department and rural development department these three are going to work under a design@@ ated road@@ map in their respective areas .\n",
            "my countrymen , 1 . 25 crore indians have in@@ fin@@ ite strength and capabilities .\n",
            "india has had a rich histor@@ ical can@@ v@@ as . every single month , every single day in fact , is a mark@@ er of one histor@@ ical event or the other .\n",
            "i must say that developing healthy traditions for a s@@ ound democracy , making con@@ st@@ ant efforts to strengthen democracy , encour@@ aging open - min@@ ded deb@@ ates would also be a appropriate tribu@@ te to at@@ al@@ ji .\n",
            "since the 4@@ th of march is national safety day , the prime minister should include safety in the mann ki baat programme in order to ra@@ ise awareness on safety .\n",
            "the celebr@@ ation of gan@@ esh ut@@ sa@@ v pu@@ bl@@ ic@@ ly had become an effective medium in promoting a spirit of social aw@@ ak@@ ening , integration , am@@ ity and equality among the m@@ ass@@ es besides projec@@ ting a sense of dedication and fes@@ tivity in them .\n",
            "this place stand@@ s t@@ esti@@ mon@@ y to a series of sacrific@@ es of men who laid down their lives for the s@@ ake of their country , so that we could live , so that the country could be safe secure , p@@ av@@ ing the way for development .\n",
            "i salute all my jaw@@ ans .\n",
            "we should also ch@@ ec@@ k if such a thing has happ@@ ened in our case too whether cir@@ c@@ um@@ st@@ ances , sit@@ u@@ ations , dist@@ ances have made us ali@@ en , whether d@@ ust has gather@@ ed over our determin@@ ations .\n",
            "and once you are hab@@ it@@ u@@ ated to using un@@ f@@ air means , you will never feel the desire to ac@@ tu@@ ally learn any@@ thing in life .\n",
            "it gi@@ ves me joy , fe@@ els n@@ ice .\n",
            "empowerment is another form of self reli@@ ance .\n",
            "it should be our p@@ ledge that water from the villages st@@ ays in the villages and water for the cities remains available for them .\n",
            "mass participation led to good results .\n",
            "the festival of di@@ wal@@ i is celebrated in various ways all over the country .\n",
            "why d@@ on@@ t we associ@@ ate each festival with clean@@ iness\n",
            "identi@@ f@@ ying her an@@ on@@ y@@ mous person@@ a , she has been hon@@ ou@@ red with the p@@ ad@@ ma shri for her contribution to society .\n",
            "he has qu@@ o@@ ted one line from a po@@ e@@ m written by har@@ i@@ v@@ an@@ sh ra@@ i b@@ ach@@ cha@@ n ji wher@@ e@@ in he said : m@@ it@@ ti ka tan@@ n , ma@@ st@@ i ka mann , k@@ sha@@ n bha@@ r je@@ e@@ w@@ an , mer@@ a par@@ ic@@ ha@@ y .\n",
            "but one thing is for sure , especially during the days when examin@@ ations are held .\n",
            "y@@ es , if you have some@@ one ne@@ ar@@ by who kno@@ ws yoga , and if you ask them during your ex@@ ams , and even if you have never done yoga before , they will su@@ re@@ ly be able to tel@@ l you a few things to do in yoga that can very eas@@ ily be done in a few min@@ utes .\n",
            "i call upon you all to come forward and participate .\n",
            "in@@ qu@@ i@@ si@@ tiv@@ eness has played a significant role in the journey of prog@@ res@@ sion of human life and development .\n",
            "i extend fel@@ ic@@ it@@ ations to all on the occasion of maha@@ vi@@ r jayanti .\n",
            "in a way , this is end@@ o@@ wed with the mi@@ ght to transform the entire se@@ ason - cy@@ c@@ le of the country .\n",
            "to understand the vir@@ tu@@ es of pat@@ ri@@ o@@ tis@@ m , sacrific@@ e and pers@@ ever@@ ance , one do@@ es@@ n@@ t need to re@@ ver@@ t to histor@@ ical events .\n",
            "whether working in the field of cyber security , or dedicated to ayur@@ veda , enter@@ t@@ aining the society through mu@@ sic , or through po@@ e@@ try\n",
            "once again , i congratulate the med@@ al w@@ inn@@ ers at the asian games and also wish the rema@@ ining play@@ ers per@@ form well .\n",
            "a while ago , when distingu@@ ished dign@@ it@@ aries of all asean countries were here on the 2@@ 6th of january , they were ac@@ compan@@ i@@ ed by cultural tr@@ ou@@ p@@ es from their respective countries . and its a matter of immense pride that a maj@@ ority of these countries presented the ram@@ ay@@ an in front of us .\n",
            "it could be that this att@@ emp@@ t of his may work for other people of our country .\n",
            "dear countrymen , i have the good for@@ t@@ une to talk to the nation from ram@@ parts of red fort on 15@@ th@@ august , it is a tradition .\n",
            "the beli@@ ev@@ ers of democracy f@@ ou@@ ght a pr@@ ol@@ on@@ ged war , and the great nation that india is , where the spirit of democracy per@@ v@@ ades the very being of all its people , the strength of that spirit was demonstr@@ ated when the opportunity of elections came .\n",
            "it gl@@ add@@ ens me to see many places associated with ill@@ u@@ stri@@ ous s@@ ons of india being developed in delhi .\n",
            "i would like to speci@@ ally men@@ tion the ar@@ du@@ ous endeav@@ ors of the n@@ dr@@ f d@@ are@@ de@@ v@@ ils .\n",
            "we just sa@@ w how the ter@@ ri@@ ble flo@@ ods in kerala have affected human lives .\n",
            "dear countrymen , you may be know@@ ing that i have tri@@ ed to s@@ end a le@@ tter , to be deli@@ ver@@ ed person@@ ally through my represent@@ atives , to all those who sur@@ r@@ end@@ ered their co@@ o@@ king gas subsidy on my appe@@ al .\n",
            "i am an ordinary citizen of village ti@@ ra@@ al@@ i of te@@ h@@ si@@ l ti@@ ra@@ al@@ i in district har@@ da of madhya pradesh .\n",
            "this in itself gi@@ ves us an opportunity to see something new .\n",
            "my dear countrymen , we had launched swach@@ ha bharat mission two years ago on 2nd october , the birth anniversary of our re@@ ver@@ ed b@@ ap@@ u .\n",
            "will you let your hol@@ id@@ ays during the most important years of your life s@@ li@@ p away just like that\n",
            "mor@@ ar@@ ji des@@ ai ste@@ ered the course of the country through some difficul@@ t times , when the very democratic f@@ ab@@ ri@@ c of the country was under a thre@@ at .\n",
            "providing security to people in kashmir is the responsibility of the administration .\n",
            "it mak@@ es me immen@@ sely happy that teachers and students who have had successful c@@ are@@ ers , as also par@@ ents and social thin@@ k@@ ers , have shared their experiences and suggestions with me .\n",
            "people are figh@@ ting a b@@ att@@ le against mal@@ nutrition in innovative and inter@@ esting ways .\n",
            "so bur@@ ning the st@@ um@@ p of cr@@ ops not only bur@@ n@@ s them , it bur@@ n@@ s the sk@@ in of our mo@@ ther@@ land .\n",
            "there is another re@@ ason why i consid@@ er 2@@ 6th july qu@@ ite important because my@@ go@@ v . in was launched a few months after our government was formed .\n",
            "and most of the beneficiaries are from the s@@ c , st , ob@@ c categ@@ ory who want to work hard and run their families independ@@ ently .\n",
            "fertiliz@@ er companies have been asked to bu@@ y the com@@ post made out of waste .\n",
            "there was a certain me@@ th@@ od@@ olo@@ gy of awar@@ ding p@@ ad@@ ma awards every year , but this entire process has been changed for the past three years .\n",
            "they have brought about a re@@ fre@@ sh@@ ing fe@@ el@@ ing .\n",
            "they are making efforts to know about nature and are lear@@ ning the art of living and tr@@ ying to understand the world .\n",
            "but this does not me@@ an that you keep s@@ le@@ ep@@ ing all the time . some mi@@ ght re@@ mark that since the prime minister has said so , just keep s@@ le@@ ep@@ ing , there is no need to w@@ ake up and study .\n",
            "in our life , a teach@@ er holds the same place which a mother does .\n",
            "these b@@ ad hab@@ its have become a part of our nature .\n",
            "you must also try to become a part of this program . and through social media and nam@@ o app , you can also see its live telec@@ ast .\n",
            "many fam@@ ous person@@ al@@ ities including ac@@ t@@ ress ra@@ ve@@ en@@ a t@@ and@@ on bec@@ ame a part of it .\n",
            "all state governments and all m@@ un@@ ici@@ pal@@ ities will now have to take con@@ cre@@ te actions due to the rising public pres@@ sure .\n",
            "millions of tre@@ es are plan@@ ted .\n",
            "the very commitment re@@ ver@@ ence of n@@ and@@ l@@ al bo@@ se have made him , along with the constitution , im@@ mor@@ tal .\n",
            "about 175 child mar@@ ri@@ ages have been pre@@ ven@@ ted so far .\n",
            "c@@ ant a l@@ it@@ t@@ le emphasis be given on this asp@@ ect in skill development\n",
            "a few days ago i tri@@ ed to do something different .\n",
            "think about it , we should gi@@ ft such a present to our sisters , which re@@ ally provides her security in future .\n",
            "i want to tel@@ l my countrymen , that de@@ pres@@ sion is not in@@ cur@@ able .\n",
            "i look at the positive side of it , which is that even the parliament of the country is discus@@ sing about the issue of cleanliness in india . look at the other side , on one hand the parliament and on the other hand this child of the country , both are tal@@ king about cleanliness , a country cannot be more fortun@@ ate than this . this ongoing movement of thoughts , the environment th@@ ats building against fil@@ th , there has been an awareness towards cleanliness .\n",
            "why not over@@ come some of our shor@@ t@@ com@@ ings\n",
            "transform this campaign into a peop@@ les movement .\n",
            "many families men@@ tion that now wh@@ en@@ ever k@@ id@@ s e@@ at a ch@@ oc@@ ol@@ ate they themselves p@@ ic@@ k the wr@@ ap@@ per and di@@ spo@@ ses it .\n",
            "but it was not like that from the start .\n",
            "they too , can inter@@ view such people , and inspire the young generation .\n",
            "hear@@ ti@@ est greetings to all fel@@ low citizens on this au@@ sp@@ ici@@ ous occasion .\n",
            "all of us know that in a few days time , the wor@@ l@@ ds largest sports event , the biggest sports car@@ ni@@ val will take place .\n",
            "they are moving ahead shoulder to shoulder and are bringing gl@@ ory to the nation with their commend@@ able achievements .\n",
            "kar@@ gi@@ l war was not limited to just our b@@ ord@@ ers , but each of our cities , villages contribu@@ ted in this war .\n",
            "i am also grat@@ eful to our re@@ ver@@ ed m@@ ur@@ ar@@ i b@@ ap@@ u for such a n@@ ice message .\n",
            "mat@@ un@@ ga station in mumbai is the first station in india which is run by an all wom@@ an st@@ af@@ f .\n",
            "the other st@@ ory that tou@@ ched the c@@ ore of my heart was e@@ id@@ ga@@ h@@ .@@ . .@@ .@@ . the sensi@@ tivity of a young la@@ d , his un@@ sul@@ li@@ ed lo@@ ve for his gr@@ and@@ mother , such mat@@ urity at that early age\n",
            "she is sp@@ read@@ ing light through her work .\n",
            "the second cir@@ c@@ le , ve@@ er@@ ta cha@@ k@@ ra , dep@@ ic@@ ts the courage and bra@@ very of our soldiers .\n",
            "i wish all of you best wishes for good health , for this month of august im@@ bu@@ ed with the spirit of pat@@ ri@@ o@@ tis@@ m and for many festivals that have continu@@ ed over centur@@ ies .\n",
            "the e - ci@@ g@@ are@@ t@@ te has been ban@@ ned so that this new form of int@@ o@@ x@@ ic@@ ation does not dest@@ ro@@ y our demo@@ grap@@ hic@@ ally young country .\n",
            "from 194@@ 7 to 19@@ 50 , dr . sh@@ y@@ ama pr@@ as@@ ad mu@@ k@@ her@@ j@@ ee was the first industries minister of india and , in a sense , hel@@ aid a strong foundation for indi@@ as industrial development , he had pre@@ pa@@ red a sol@@ id b@@ ase , it was he who had pre@@ pa@@ red a st@@ out platform .\n",
            "when we think of south africa , it is very natural to remember mahatma gandhi and n@@ el@@ son man@@ del@@ a .\n",
            "ab@@ p news has started a program called y@@ e bharat d@@ esh ha@@ i mer@@ a . in this program they have shown how people have become aware towards cleanliness and through this they are tr@@ ying to train people about keeping their sur@@ r@@ ound@@ ings clean .\n",
            "if you go to ali@@ garh , do visit the railway station .\n",
            "there will be a sense of inn@@ er satisfaction .\n",
            "pres@@ ently mann ki baat is broad@@ c@@ ast at 11 am but we are going to start its broad@@ c@@ ast in regional langu@@ ages immedi@@ ately after my hin@@ di broad@@ c@@ ast .\n",
            "this is why i am s@@ aying that i am very exc@@ ited about my br@@ it@@ ain tri@@ p this time . this issue has been l@@ ying ent@@ an@@ g@@ led since many years , now it has become the property of 125 crore countrymen and if the name of babasaheb ambedkar is att@@ ach@@ ed , you can im@@ ag@@ ine how happy it mak@@ es me .\n",
            "his contribution to h@@ oc@@ key was un@@ par@@ all@@ el@@ ed .\n",
            "i want to thank you from the c@@ ore of my heart , not because you have been connected with mann ki baat , for such a long time i want to express my grati@@ tude and inde@@ b@@ t@@ ed@@ ness as millions of people from across the country come together with mann k@@ i@@ baat .\n",
            "and i have always felt that we should come out of this situation and , therefore , today i want to talk in some de@@ t@@ ail with my young friends .\n",
            "this is no long@@ er just a government programme .\n",
            "the festival of li@@ ghts deep@@ aw@@ al@@ i conve@@ ys the message of tam@@ so ma j@@ yo@@ ti@@ r@@ gam@@ aya , to move from dar@@ k@@ ness to light .\n",
            "and you must know that the power of mem@@ ory to rec@@ all is gre@@ atest when we are re@@ la@@ x@@ ed .\n",
            "if one has to achieve hol@@ istic development , the fact is that there exist@@ s a huge world , a life beyond boo@@ ks\n",
            "it is the ess@@ ence of the em@@ o@@ tions of the 125 crore indians wan@@ ting to come together and create a m@@ ag@@ n@@ ific@@ ent india .\n",
            "the festival of sam@@ v@@ at@@ sar@@ i is sym@@ b@@ ol@@ ic of for@@ giv@@ eness , non - violence and bro@@ ther@@ hood .\n",
            "come , le@@ ts welcome the young vis@@ it@@ ors from all across the world with the festival of sports , le@@ ts en@@ joy the spor@@ t , and create a con@@ du@@ ci@@ ve spor@@ ting atmo@@ sphere in the country .\n",
            "i would like to once again express my deep grati@@ tude to the millions of my countrymen who have contribu@@ ted to the successful achiev@@ ement of this great fe@@ at with such fel@@ icity .\n",
            "state governments too should deli@@ ber@@ ate on this matter and provide their suggestions .\n",
            "they have shared their v@@ ac@@ ation plans .\n",
            "which show the power of unity , and how big results can be achieved by working together\n",
            "try to encourage him to express him@@ self , to re@@ ve@@ al and bring out his in@@ sec@@ ur@@ ities and comple@@ x@@ es while eng@@ aging him in light - hear@@ ted happy con@@ vers@@ ation .\n",
            "this waste de@@ pos@@ ited in the p@@ it gets comple@@ t@@ ely dec@@ om@@ po@@ sed in six to tw@@ el@@ ve months time .\n",
            "but , i did not know cha@@ i p@@ ar char@@ ch@@ a could be linked to a mar@@ ri@@ age also .\n",
            "what are the hop@@ es and aspirations of the common man in the country\n",
            "this is a wond@@ erful experience .\n",
            "it is the t@@ all@@ est statue in the world .\n",
            "in our country there is an inn@@ ate respect for the military and the security forces .\n",
            "the first mantr@@ a is the expres@@ sion of de@@ pres@@ sion in@@ ste@@ ad of its sup@@ pres@@ sion .\n",
            "one way , a le@@ tter from you can act as a source of inspiration for me on the other it may turn out to be a source of energy for me .\n",
            "economic pro@@ w@@ ess of villages im@@ parts momentum to the nations economic progress .\n",
            "do we ever work for our k@@ id@@ s spirit@@ ual progress and discus@@ s it with them , ra@@ ther we discus@@ s only mater@@ ial progress .\n",
            "at the time i was rele@@ as@@ ing the data on ti@@ g@@ ers , i also remembered the asi@@ atic lion of the gi@@ r in gujarat .\n",
            "in our freedom struggle , the sal@@ t sat@@ y@@ ag@@ ra@@ h at d@@ and@@ i was an important tur@@ ning point .\n",
            "once you learn to accep@@ t , maximum number of problems will be sol@@ ved there and then .\n",
            "other arran@@ ge@@ ments are in progress , but he did something re@@ ally innovative .\n",
            "modi ji i wat@@ ch whether on t@@ v or social media , wher@@ ever you go , one can see people in that city pay special attention to cleanliness .\n",
            "three bra@@ ve@@ heart women bha@@ v@@ na kan@@ th , mo@@ han@@ a singh and av@@ an@@ i ch@@ at@@ ur@@ ve@@ di have become figh@@ ter pil@@ o@@ ts and are under@@ going training on the su@@ k@@ ho@@ i - 30 .\n",
            "but this mantr@@ a of unity should be a continu@@ ously present in our thoughts in our min@@ ds , be@@ ha@@ vi@@ our and our expres@@ sions .\n",
            "i am told this radio station also used to broad@@ c@@ ast wee@@ k@@ ly news bul@@ le@@ t@@ ins in eng@@ lish , hin@@ di , tamil , bangl@@ a , mar@@ ath@@ i , pun@@ j@@ ab@@ i , p@@ ash@@ to ur@@ d@@ u langu@@ ages .\n",
            "5@@ th september happ@@ ens to be the bir@@ th@@ day of indi@@ as former president dr . sar@@ v@@ ap@@ all@@ i r@@ adha@@ krish@@ n@@ an ji and the country celebr@@ ates this as teachers day .\n",
            "after read@@ ing these st@@ ories , i felt my@@ self to be emp@@ a@@ the@@ tic .\n",
            "it does not tr@@ ample the dreams of a family and waste the lives of our children .\n",
            "let us build an empow@@ ered , cap@@ able new india .\n",
            "he was the son of a farmer .\n",
            "but it is true that if 125 crore countrymen resolve , and dec@@ ide to wal@@ k step by step on a path to real@@ ise that resolve , the dream of new india can be fulfill@@ ed in our li@@ fe@@ time .\n",
            "the prime minister mudra fund is about funding the un@@ fund@@ ed .\n",
            "wh@@ at@@ ever post he held in his life , he always tri@@ ed to live like a teach@@ er , an educ@@ at@@ or .\n",
            "they give me renew@@ ed energy .\n",
            "and , this is an example which sho@@ ws that great t@@ as@@ ks can be ac@@ comp@@ lish@@ ed by working in un@@ is@@ on to move ahead .\n",
            "if she had not for@@ b@@ id@@ den me from re@@ ve@@ al@@ ing her name , i from the c@@ ore of my heart wan@@ ted to announ@@ ce her name while tal@@ king to you . she w@@ ro@@ te in her le@@ tter that after my appe@@ al to the people to fo@@ reg@@ o their co@@ o@@ king gas subsidy , she had given up her subsidy and then she even for@@ got about it .\n",
            "even at the age of 100 , she is training yoga to people from all over the world and till now has tr@@ ained 15@@ 00 as yoga teachers .\n",
            "i look forward to your suggestions on the 2017 fif@@ a under - 17 world c@@ up which you can s@@ end me through nar@@ end@@ ram@@ odi@@ app .\n",
            "in a similar manner , when the tr@@ ic@@ ol@@ our was ho@@ ist@@ ed at the red fort in october 2018 , it sur@@ prised everyone , since conv@@ enti@@ on@@ ally , this is done on the 15@@ th of august .\n",
            "it is of@@ ten seen that gir@@ ls are not encouraged much to participate in sports such as k@@ ab@@ add@@ i .\n",
            "the mo@@ tive be@@ hind this scheme is making healthcare affordable and encour@@ aging ease of living .\n",
            "i extend my than@@ ks to bangladesh also for collabor@@ ating with us in this initiative .\n",
            "and the entire bri@@ tis@@ h emp@@ ir@@ e fe@@ ared these three young men .\n",
            "i visited gujarat recently and the officers there in@@ formed me that por@@ band@@ ar , the birth place of mahatma gandhi , will achieve the target of total od@@ f on 2nd october , 2016 .\n",
            "i have been fortun@@ ate that i had a lot of opportunities to travel .\n",
            "to me , each and every individu@@ al involved in this mission consti@@ t@@ ute a new anti - corruption c@@ ad@@ re in the country .\n",
            "all of us pay tributes to the marty@@ rs who sacrific@@ ed their lives for the mo@@ ther@@ land , by observ@@ ing sil@@ ence for 2 min@@ utes at 11 am on 3@@ 0th january .\n",
            "last month a work@@ sho@@ p was organized in bh@@ op@@ al .\n",
            "why not add to our strengths\n",
            "some friend of min@@ e had s@@ ent me a link and i had the opportunity to see it . i re@@ ally wan@@ ted to share it with you .\n",
            "if a person is healthy , then he will work hard , ear@@ n for the family and help in r@@ un@@ ning the family s@@ mo@@ o@@ th@@ ly .\n",
            "some called it l@@ ag@@ or@@ i , at other places it was sat@@ ori@@ ya , sa@@ at pa@@ th@@ ar , di@@ k@@ or@@ i , sat@@ odi@@ y@@ a@@ .@@ . .@@ .@@ . one g@@ ame with many nam@@ es\n",
            "he dedicated his life in ensuring that food , sh@@ el@@ ter , education and spirit@@ ual knowledge reached out to people .\n",
            "this is great help for the common man , especially for senior citizens who requ@@ ir@@ e medic@@ ines on a d@@ ail@@ y basis and results in a lot of sav@@ ings .\n",
            "i read these my@@ self and shall try to express them in wh@@ at@@ ever time i have with me on 15@@ th august .\n",
            "in india , it has been said earth is the mother and i am her son .\n",
            "we all want a good future for our children .\n",
            "true in le@@ tter and spirit to the mo@@ t@@ to , sec@@ uring the country@@ s maritime b@@ ord@@ ers and maritime environment , the jaw@@ ans of co@@ ast gu@@ ard are rel@@ ent@@ l@@ ess@@ ly at work ni@@ ght and day , bra@@ ving the most ad@@ verse conditions .\n",
            "this is an int@@ en@@ sely rural technology .\n",
            "i want to say that people will have to be made aware about the importance of cleanliness .\n",
            "let there be stri@@ v@@ ings for the real@@ isation of these millions and millions of resol@@ ves .\n",
            "on the 19@@ th of may , san@@ ge@@ et@@ a ba@@ h@@ al , aged more than 50 , sc@@ al@@ ed the ever@@ est .\n",
            "i extend my hear@@ ti@@ est fel@@ ic@@ it@@ ations to each and every ministry , every department and all employees of the central and state governments involved with this process .\n",
            "but dear friends , you are my true soldiers , my true partners .\n",
            "we have recently celebrated 12@@ 5@@ th birth anniversary of bab@@ a sa@@ he@@ b ambedkar .\n",
            "i am grat@@ eful to shri ra@@ o@@ for inspir@@ ing the children of the country .\n",
            "the 4@@ 4@@ th amendment , made it mand@@ atory that the proc@@ e@@ ed@@ ings of parliament and legisl@@ ative ass@@ em@@ bli@@ es were made public through the new@@ sp@@ ap@@ ers .\n",
            "in the past few days we sa@@ w on tel@@ e@@ vision that the wi@@ f@@ e of a poor ha@@ w@@ k@@ er in delhi received a tran@@ sp@@ l@@ ant in del@@ his g . b .\n",
            "one of the cre@@ ations dre@@ w my attention and i want to share it with you .\n",
            "al@@ thou@@ gh in the next ep@@ is@@ od@@ e of ' mann ki baat ' i shall d@@ well upon it in de@@ t@@ ail , still today i am t@@ ell@@ ing this in@@ a advance so that you get time to prep@@ are .\n",
            "they have em@@ bar@@ ked upon the journey of being partners in nation building .\n",
            "if people from the entire world ar@@ d@@ ently particip@@ ated in programmes on yoga day , why should india not feel el@@ ated many times over\n",
            "the s@@ add@@ est part is that , for this too , they have cho@@ s@@ en to mis@@ use the poor , the under@@ privile@@ ged people .\n",
            "she co@@ ol@@ ly sp@@ ent two thousand rupees on a sar@@ i , and four hundred and fif@@ ty rupees on a p@@ iz@@ z@@ a . wher@@ e@@ as , she ha@@ gg@@ led for a long time , over mer@@ ely five rupees , with the au@@ to dri@@ ver , who took us to the m@@ all .\n",
            "i have been extremely fortun@@ ate to have been bl@@ essed with the opportunity to visit a number of significant places associated with re@@ ver@@ ed mahatma gandhi and pay my homage .\n",
            "well , ill tel@@ l you the re@@ ason , one of the security personnel of it@@ b@@ p called vik@@ as th@@ ak@@ ur is from a small village in si@@ r@@ m@@ our district of him@@ achal .\n",
            "it was a historic war against in@@ justice , which was f@@ ou@@ ght under the leadership of ban@@ ga - band@@ h@@ u and led to the un@@ pre@@ c@@ ed@@ ented vic@@ tory of the people of bangladesh .\n",
            "desp@@ ite these measures , some difficul@@ ties per@@ si@@ st but i am confident that our farmer who bra@@ vely fac@@ es every difficul@@ ty , including natural cal@@ am@@ ities , is standing firm in the face of present od@@ ds also .\n",
            "however , this is not the case with an e - ci@@ g@@ are@@ t@@ te and many te@@ en@@ ag@@ ers and young people , un@@ know@@ ingly and some@@ times prou@@ d@@ ly disp@@ laying an e - ci@@ g@@ are@@ t@@ te@@ as a fa@@ shi@@ on stat@@ ement , keep it@@ betw@@ i@@ x@@ t their boo@@ ks , in their offic@@ es , in their po@@ c@@ kets , some@@ times hold it in their h@@ ands and thus f@@ all pre@@ y to it .\n",
            "once you te@@ ach the poor people about the us@@ age of rup@@ ay card , they will show@@ er their bl@@ ess@@ ings upon you .\n",
            "friends , for our nation and the constitu@@ ent states , as well as for the tourism industry , this ' statue of unity ' can be a subject of research .\n",
            "in this d@@ an@@ ger@@ ous atmo@@ sphere , adi@@ v@@ asi women are becoming self reli@@ ant by dri@@ ving e - ric@@ k@@ sha@@ ws .\n",
            "be they farmers , trad@@ ers , small entrepreneurs , professionals , house@@ wi@@ ves , students all are enthusias@@ tic@@ ally particip@@ ating and also getting benefits from it .\n",
            "it is natural for our countrymen to be cur@@ i@@ ous about it .\n",
            "we must ab@@ ide by the same ess@@ ence , the same spirit as we celebrate teachers day .\n",
            "it came to the attention of engineering students from the ne@@ ar@@ by urban areas that this village was in di@@ re need of the to@@ ile@@ ts .\n",
            "not only has she advanced her@@ self but has carried forward the country and society to new@@ er heights .\n",
            "we have to prep@@ are a far reach@@ ing plan keeping in view the ol@@ y@@ mp@@ ics in 2020 , 20@@ 24 , 20@@ 28 .\n",
            "my friends , there are many l@@ it@@ t@@ le things that can be employ@@ ed in our country@@ s effective fight against mal@@ nutrition .\n",
            "from my last con@@ vers@@ ation in mann ki baat to this one , i would ur@@ ge you all to change this min@@ d@@ set . nei@@ ther is our country is like this n@@ or our people indi@@ f@@ fer@@ ent .\n",
            "there has been an increase in the dis@@ g@@ ust towards s@@ qu@@ al@@ or .\n",
            "this year also i requ@@ est that each family should bu@@ y one or the other khadi it@@ e@@ m so that the poor may also be able to light an ear@@ then l@@ am@@ p and celebrate di@@ wal@@ i .\n",
            "there are so many things that you have to think of .\n",
            "many good wishes to you all on the national sports day .\n",
            "my mother is go@@ d to me .\n",
            "i have been observ@@ ing how gst has imp@@ ac@@ ted the transport and log@@ is@@ tics sector\n",
            "we get to know certain things through media , through the people and through government sour@@ ces which add to our enthusias@@ m for work .\n",
            "women achiev@@ ers of our country@@ .@@ . .@@ .@@ . the first f@@ em@@ ale mer@@ ch@@ ant navy cap@@ tain , the first f@@ em@@ ale pass@@ eng@@ er train dri@@ ver , the first f@@ em@@ ale fir@@ e figh@@ ter , the first f@@ em@@ ale bu@@ s dri@@ ver , the first wom@@ an to set foo@@ t on an@@ tar@@ c@@ tic@@ a , the first wom@@ an to reach m@@ oun@@ t ever@@ est@@ .@@ . .@@ .@@ . first la@@ dies in every field .\n",
            "my dear countrymen , some@@ times i noti@@ ce that the money that our poor families have to sp@@ end on their healthcare , th@@ ro@@ ws their life off the tr@@ ack .\n",
            "my dear young friends , you are in a great h@@ ur@@ ry to ent@@ er the examin@@ ation h@@ all , you are in a rus@@ h to qu@@ ick@@ ly be se@@ ated in your place .\n",
            "he has expressed the wish of s@@ ending his second son too , to take on the en@@ em@@ y if need be , he him@@ self would go and fight .\n",
            "sec@@ ond@@ ly because of bur@@ ning this the top soil gets bur@@ n@@ t .\n",
            "the my@@ th that has been spread is that e - ci@@ g@@ are@@ t@@ tes pose no d@@ an@@ ger .\n",
            "one sardar , as history go@@ es , sardar v@@ all@@ ab@@ h@@ bha@@ i patel was the second great man after chan@@ ak@@ ya who carried on the st@@ up@@ end@@ ous job of un@@ iting our country .\n",
            "we need to understand our@@ selves .\n",
            "my dear countrymen , sa@@ ad@@ ar nam@@ as@@ kar .\n",
            "in 2017 , the de@@ ath t@@ ol@@ l on account of he@@ at w@@ ave re@@ mark@@ ab@@ ly came down to around 2@@ 20 or so .\n",
            "i too am making a separ@@ ate arran@@ gement for their experiences on the nar@@ end@@ ram@@ odi@@ app to ensure that you can read it . for this is a tal@@ e of her@@ o@@ ism , a tal@@ e of personal experiences , and i would be happy to bring you the st@@ ories of these daugh@@ ters .\n",
            "this de@@ ed is commend@@ able indeed it also disp@@ l@@ ays their commitment towards cleanliness and the environment .\n",
            "tomorrow is 3@@ 0th january , the de@@ ath anniversary of our re@@ ver@@ ed b@@ ap@@ u .\n",
            "our mother india is a land full of g@@ ems and tre@@ as@@ ures and the tradition of these great sain@@ ts that ad@@ or@@ ned our land are at the same level as the great men who dedicated and sacrific@@ ed their lives for our mother india .\n",
            "and , d@@ hy@@ an ch@@ and ji s@@ co@@ red three goals in just those ten min@@ utes and said i av@@ eng@@ ed the h@@ it by sc@@ oring goals .\n",
            "i had never thought that the youth of our country and the world pay attention to di@@ verse things .\n",
            "what cour@@ ses should be de@@ signed to that end\n",
            "what a wond@@ erful journey of life he has had , only because he follow@@ ed the route of compe@@ ting with him@@ self ra@@ ther than compe@@ ting with others\n",
            "at least 15 places and if possible , make a program to st@@ ay over there for an over ni@@ ght , two - ni@@ ght st@@ ay .\n",
            "for me , it was an opportunity to meet my@@ self .\n",
            "this 2 min@@ utes sil@@ ence is an expres@@ sion of our collective resolve and re@@ ver@@ ence for the marty@@ rs .\n",
            "] this observ@@ ation expressed centur@@ ies ago , str@@ ai@@ gh@@ ta@@ way imp@@ li@@ es that practi@@ c@@ ing yo@@ gic exerc@@ ises on a regular basis lead@@ s to im@@ bi@@ b@@ ing ben@@ e@@ fic at@@ tributes which stand by our side like rel@@ atives and friends .\n",
            "we all know that ad@@ dic@@ tion to , to@@ b@@ ac@@ co@@ is very har@@ m@@ ful for health and it becom@@ es very difficul@@ t to qu@@ it this ad@@ dic@@ tion .\n",
            "come , let us all march together to make the india which was dre@@ am@@ t of by our freedom figh@@ ters and re@@ alize gandhi ' s dreams - ' sw@@ an@@ ta@@ h : su@@ kha@@ y@@ ah ' .\n",
            "i ur@@ ge the state governments also to set up similar commit@@ te@@ es to find out as to what we can do in sports . what can each state do\n",
            "in districts pr@@ one to flo@@ ods and cy@@ cl@@ ones , an initiative nam@@ es a@@ ap@@ ada m@@ it@@ ra has been launched for training vol@@ un@@ te@@ ers .\n",
            "i have reiterated again and again that i am open to any sugg@@ estion in the interest of the farmers .\n",
            "and h@@ ence i will say , p for prep@@ are and p for play , one who pl@@ ays b@@ lo@@ ss@@ om@@ s , the person who pl@@ ays , sh@@ ines . this is a wond@@ erful re@@ med@@ y for maint@@ aining the vig@@ our of the mind , bra@@ in and body .\n",
            "each one should nur@@ ture dreams for the next ol@@ y@@ mp@@ ics .\n",
            "and almost 27 lakh indians have set@@ t@@ led in these nations from a long time .\n",
            "the top soil of our land is bur@@ n@@ t which pu@@ sh@@ es our fer@@ ti@@ le soil to de@@ ath .\n",
            "v@@ ar@@ un c . b@@ hat@@ i won a br@@ on@@ z@@ e med@@ al in high j@@ um@@ p .\n",
            "the army has resol@@ ved to wi@@ pe out terror@@ ists and their har@@ b@@ our@@ ers .\n",
            "and therefore the most effective medicine that exist@@ s for mem@@ ory rec@@ all is re@@ la@@ x@@ ation .\n",
            "in a few days from now , we shall celebrate the fes@@ tive occasion of jan@@ ma@@ sh@@ tam@@ i .\n",
            "and many leaders from africa have stu@@ di@@ ed in india .\n",
            "there is no al@@ tern@@ ative to boo@@ ks , one has to study , yet ones b@@ ent of mind should be towards disc@@ over@@ ing new things .\n",
            "and till the time the people who would clear these st@@ um@@ ps would not come with tr@@ ac@@ tors , the st@@ um@@ ps would remain standing for long .\n",
            "of course , those games were known by different nam@@ es , depend@@ ing on the place . p@@ it@@ th@@ o@@ o is one such g@@ ame .\n",
            "the nation will not t@@ ol@@ er@@ ate those commit@@ ting rap@@ es . with this point in view parliament has made a provision of str@@ ic@@ t@@ est p@@ uni@@ sh@@ ment by passing the cr@@ imin@@ al act amendment bill .\n",
            "i had not asked any@@ one to be kha@@ di@@ d@@ hari , but the fe@@ ed@@ back i got from khadi sto@@ res was that in a wee@@ ks time the sal@@ es had j@@ um@@ ped up by 125 .\n",
            "i also appe@@ al to the corporate sector to come out with ways means pro@@ acti@@ vely for appropriate di@@ spo@@ sal of all acc@@ um@@ ul@@ ated pl@@ as@@ tic .\n",
            "by connec@@ ting with others , serv@@ ing them and serv@@ ing them sel@@ fl@@ ess@@ ly , you will eas@@ ily be able to shed the w@@ eight op@@ pres@@ sing your own heart and mind .\n",
            "my dear countrymen , i am g@@ la@@ d that every time , i requ@@ est people for suggestions in mann ki baat , num@@ er@@ ous suggestions p@@ our in .\n",
            "my dear countrymen , sh@@ r@@ im@@ an d@@ att@@ at@@ ra@@ ya ram@@ chand@@ ra b@@ end@@ re , g@@ yan@@ p@@ ith la@@ ure@@ ate and po@@ et has described the signific@@ ance of the month of sa@@ w@@ an in this manner - in his po@@ e@@ m .\n",
            "but , this time after the decision taken on the 8@@ th inst@@ ant , people ru@@ shed to de@@ posi@@ t their old curr@@ ency no@@ tes .\n",
            "this is what be@@ gets new science , new technology .\n",
            "i must ad@@ m@@ it with pride that if a single individu@@ al dec@@ id@@ es it in his heart then a huge public campaign can be launched .\n",
            "many f@@ airs are held on these occa@@ sions .\n",
            "shi@@ k@@ har th@@ ak@@ ur has written on my@@ go@@ v that we could have per@@ formed better at the ol@@ y@@ mp@@ ics .\n",
            "it is the need of the times that we@@ ve to address the aspirations that have been ar@@ is@@ en in the poor of our nation .\n",
            "he should be given an opportunity to be am@@ ong@@ st people with wh@@ om he speak@@ s op@@ en@@ ly .\n",
            "a number of important bil@@ ls beneficial to the youth and the back@@ ward cl@@ ass@@ es were pas@@ sed during this session .\n",
            "i have always said that the government is open to all views and suggestions on the issue of land acqu@@ i@@ si@@ tion act , on which a deb@@ ate is going on .\n",
            "i see this as a good si@@ gn .\n",
            "similarly , you can see that pres@@ ently efforts are af@@ o@@ o@@ t and discussions are being held about sim@@ ul@@ tan@@ e@@ ously holding the elections for lok sabha and for state ass@@ em@@ bli@@ es .\n",
            "a yoga session with yoga ma@@ sters was organ@@ ised at the un head@@ qu@@ ar@@ ters .\n",
            "besides , clear guidelines were defin@@ ed to dis@@ qu@@ ali@@ fy the de@@ fec@@ tor .\n",
            "this time around it will commen@@ ce on the 1@@ 1@@ th of september .\n",
            "and now , i will be going there with these schol@@ ars , this gi@@ ves me immense happiness .\n",
            "one of them was sh@@ r@@ add@@ ha th@@ am@@ ban from kerala a gr@@ ade 12 stud@@ ent .\n",
            "from k@@ ut@@ ch to k@@ am@@ r@@ up , from kashmir to kan@@ y@@ ak@@ u@@ mar@@ i , everyone is endeav@@ oring to contribute in some way or the other so that wher@@ ever a disaster stri@@ k@@ es , be it kerala or any other part of india , human life retur@@ n@@ s to nor@@ mal@@ c@@ y .\n",
            "we are making a con@@ st@@ ant effort to develop these five pil@@ gr@@ im@@ age spo@@ ts .\n",
            "in the days to come , wher@@ ever we are , let us do our b@@ it to encourage our spor@@ t@@ sper@@ s@@ ons .\n",
            "friends , safety in the times of cr@@ ises , dis@@ ast@@ ers are t@@ op@@ ics on which many mess@@ ages keep coming in - people keep wr@@ iting to me .\n",
            "when we read and he@@ ar about sardar sa@@ he@@ b ' s st@@ y@@ le of working , we come to know of the sh@@ e@@ er m@@ ag@@ n@@ it@@ ude of the me@@ ti@@ cul@@ ous@@ ness in his planning .\n",
            "wh@@ en@@ ever you come to delhi , you must pay a visit to these places .\n",
            "i was present there and i sa@@ w that he did not ask a single question on the state of his health\n",
            "i have a question how to gu@@ ide people that bur@@ ning of far@@ m remains or str@@ a@@ w not only dest@@ ro@@ ys the mic@@ ro organ@@ isms of the soil but also contribu@@ tes to the poll@@ ution in delhi , pun@@ ja@@ b and haryana .\n",
            "go@@ d has gi@@ f@@ ted each one of us with a unique talent .\n",
            "he pre@@ ferred to live a teachers life .\n",
            "these young bo@@ ys gir@@ ls had been sel@@ ected under mission sha@@ ur@@ ya . true to its name , they brought gl@@ ory to the country with their brave de@@ ed of con@@ qu@@ ering the ever@@ est .\n",
            "in all the programmes of gandhi 150 , let there be a sense of collec@@ tiv@@ eness , let there be a spirit of service .\n",
            "the sid@@ d@@ h@@ ganga mut@@ t regul@@ ar@@ ily used to organ@@ ise c@@ att@@ le and agricultural f@@ airs .\n",
            "gold is consid@@ ered the medium of financial security .\n",
            "he w@@ ro@@ te , i like l@@ ist@@ ening to the programme mann ki baat .\n",
            "and we should never under@@ estim@@ ate its value .\n",
            "2nd october is the birth anniversary of mahatma gandhi and l@@ al ba@@ ha@@ du@@ r sha@@ str@@ i ji .\n",
            "india will remain ever grat@@ eful to at@@ al@@ ji for bringing the 9@@ 1st amendment act , 200@@ 3 .\n",
            "it is a g@@ all@@ ery whose w@@ alls are in@@ sc@@ ri@@ bed with soldiers tal@@ es of val@@ our .\n",
            "life tru@@ ly means what is the knowledge that you have acqu@@ ired\n",
            "my h@@ us@@ b@@ and is an army offic@@ er .\n",
            "every par@@ ent w@@ ants the schools and colle@@ ges to be func@@ tioning proper@@ ly at the earli@@ est .\n",
            "this is a great initiative that can be adop@@ ted any@@ where .\n",
            "there has been a long tradition of vi@@ du@@ sh@@ is@@ .@@ . .@@ .@@ .\n",
            "this is our int@@ ention be@@ hind the mudra scheme .\n",
            "i fel@@ ic@@ it@@ ate the resid@@ ents of mu@@ bar@@ ak@@ pur , for trans@@ forming the pi@@ ous occasion of ram@@ z@@ an into an opportunity for the welfare of society on .\n",
            "just a few days ago , amb@@ ass@@ ad@@ ors of appro@@ xim@@ ately ei@@ gh@@ ty - five countries w@@ ent to am@@ r@@ it@@ s@@ ar from delhi .\n",
            "the basic aim is to ensure prop@@ er care of the new@@ bor@@ n , the future citizen of india , from the time of birth . the new@@ bor@@ n should get the complete lo@@ ve and attention of the mother .\n",
            "and i ass@@ ure you that i will keep tr@@ ying till the end .\n",
            "i pay my deeply felt respec@@ t@@ ful homage to guru nan@@ ak de@@ v on the occasion of the for@@ th@@ coming pr@@ ak@@ ash@@ o@@ t@@ sa@@ v .\n",
            "our daugh@@ ters , who were for@@ ced to si@@ ft through gar@@ b@@ age and be@@ g from home to home in order to ear@@ n a living - today they are lear@@ ning se@@ wing and sti@@ t@@ ching clo@@ th@@ es to cover the imp@@ over@@ ished .\n",
            "festivals have their own signific@@ ance in social life .\n",
            "now , tel@@ l me countrymen , can gold become a property or not\n",
            "my dear countrymen , on the 5@@ th of june , our nation , india will offic@@ i@@ ally ho@@ st the world environment day celebrations .\n",
            "it is our responsibility that we gi@@ ft su@@ pre@@ me yoga teachers to the world .\n",
            "just d@@ on@@ t see it , show it to others too . read it and may be you may find something to connect with it .\n",
            "but what i want to tel@@ l you is the fact that this time i had an opportunity to meet those great men who had devo@@ ted their young lives for society figh@@ ting shoulder to shoulder with n@@ el@@ son man@@ del@@ a for the ide@@ als of equality and equ@@ al opportunity\n",
            "we can bu@@ y things and pay our bil@@ ls too .\n",
            "this r@@ it@@ ual is per@@ formed when the t@@ od@@ d@@ l@@ er star@@ ts fe@@ ed@@ ing on sol@@ id food for the first time\n",
            "i also wish today to speci@@ ally talk to those living in kashmir .\n",
            "it will include people from all wal@@ ks of life , from every se@@ g@@ ment of our society .\n",
            "youth organ@@ is@@ ations , stud@@ ent organ@@ is@@ ations , n@@ go@@ s , etc .\n",
            "and now we are hear@@ ing more about it because it is affec@@ ting the city life too .\n",
            "the sh@@ e@@ er vol@@ u@@ me of students , who appe@@ ared in the board examin@@ ations , is a clear indic@@ ation that our children from jammu kashmir are committed to at@@ tain new@@ er heights of progress , to build a bri@@ ght future through the medium of education .\n",
            "this should also be supp@@ li@@ ed to those farmers who care about the health of their soil and are w@@ ill@@ ing to improve its quality and also to those farmers whose soil quality has de@@ ter@@ i@@ or@@ ated because of over use of chem@@ ical fertiliz@@ ers .\n",
            "in this st@@ ory , the living dep@@ ic@@ tion of the par@@ ad@@ o@@ x@@ es in a poor farmer ' s life is seen .\n",
            "this initiative is yet to achieve the speed that i have aimed for .\n",
            "pre@@ m se@@ h@@ g@@ al , col . gur@@ b@@ ak@@ sh singh d@@ h@@ ill@@ on and major general sha@@ hn@@ aw@@ a@@ z k@@ han had to face a tri@@ al under bri@@ tis@@ h rule .\n",
            "i also congratulate all those ministries of the government of india and also those ministers who tur@@ ned this vision into re@@ ality .\n",
            "i was inspired to visit the cell@@ ul@@ ar j@@ ail only after read@@ ing sav@@ ar@@ kar@@ j@@ is book ma@@ az@@ i jan@@ m@@ the@@ p .\n",
            "he writ@@ es , the 1st of february is the de@@ ath anniversary of a@@ str@@ on@@ au@@ t kal@@ pan@@ a cha@@ w@@ la .\n",
            "he st@@ ro@@ ve to ensure en@@ sh@@ r@@ in@@ ement of fund@@ a@@ mental rights that ob@@ l@@ iter@@ ated any possi@@ bility of dis@@ cr@@ im@@ ination on the basis of c@@ aste and community .\n",
            "i am fortun@@ ate en@@ ou@@ gh to have lear@@ ned a few langu@@ ages , but still there are so many langu@@ ages that i could not learn .\n",
            "for our daugh@@ ters this is the opportunity to experience an india free from dis@@ cr@@ im@@ ination .\n",
            "from r@@ ak@@ sha band@@ han to di@@ wal@@ i there will be many festivals .\n",
            "a lot of gar@@ b@@ age was being du@@ m@@ ped into the river and along its banks .\n",
            "the new@@ sp@@ ap@@ ers of our country have comp@@ ared it with el@@ ep@@ han@@ t@@ ine w@@ ei@@ ghts . you can well im@@ ag@@ ine the m@@ ag@@ n@@ it@@ ude of the achiev@@ ement of our scientists in space .\n",
            "i am happy to let you know that i played a positive active role in organiz@@ ing the community mobil@@ isation programme - back to village in my state jammu kashmir .\n",
            "sh@@ il@@ p@@ i v@@ arm@@ a has brought this important thing to my noti@@ ce , which al@@ thou@@ gh even you must have en@@ coun@@ tered , but may be you did not take it that seri@@ ously , but i feel there is need to look at it seri@@ ously .\n",
            "sh@@ il@@ p@@ a , i am very grat@@ eful to you that you have arti@@ cul@@ ated something so sensi@@ tive in so simp@@ le a manner .\n",
            "this means i shall take up our thoughts and ideas acc@@ um@@ ul@@ ated over the three months of march , april entire may after the elections , with confidence an@@ e@@ w . with the power of your bl@@ ess@@ ings , once again i shall beg@@ in this series of con@@ vers@@ ations through mann ki ba@@ at@@ .@@ . .@@ .@@ .\n",
            "since last many years , run for unity program@@ s have been organized in various parts of the country on the 3@@ 1st of october .\n",
            "yoga is a gu@@ aran@@ tee of both f@@ it@@ ness and well@@ ness .\n",
            "and after some days we will celebrate di@@ wal@@ i too .\n",
            "the speed at which the s@@ mo@@ o@@ th transi@@ tion has taken place , along with rapid mi@@ gr@@ ation and new regist@@ r@@ ations , has insti@@ l@@ led a new sense of confidence in the entire country .\n",
            "some must be eng@@ aged in teach@@ ing under@@ privile@@ ged children , others could be oc@@ c@@ up@@ i@@ ed in sp@@ read@@ ing awareness in the field of sanitation and health , while many could be doing society a service by working as doc@@ tor and engine@@ ers .\n",
            "the bihar yoga vid@@ y@@ al@@ aya in m@@ un@@ ger dedicated to yoga from past many decades was also hon@@ ou@@ red .\n",
            "all the members ro@@ se above party interests to make the mon@@ soon session most productive and this is why lok sabha pas@@ sed 21 bil@@ ls and in raj@@ ya sabha four@@ t@@ een bil@@ ls were pas@@ sed .\n",
            "id@@ ol@@ at@@ ers and those lo@@ a@@ thing id@@ ol - wor@@ ship co - ex@@ ist . we have ad@@ ap@@ ted our@@ selves to my@@ ri@@ ad k@@ in@@ ds of ide@@ olog@@ ies , different ways of wor@@ shipping and all ty@@ p@@ es of traditions and im@@ bi@@ bed the art of co - exist@@ ential living .\n",
            "i would like to he@@ ar your views on this . thank you .\n",
            "if we learn and ad@@ ap@@ t our@@ selves to use these services , then we will not requ@@ ir@@ e the curr@@ ency , we will not need no@@ tes , we will not need co@@ ins . businesses will function aut@@ om@@ ati@@ c@@ ally , resul@@ ting in a certain transparency .\n",
            "this act will play an effective role in cur@@ b@@ ing cr@@ im@@ es against women and gir@@ ls .\n",
            "it is during this se@@ ason that the festival of r@@ ak@@ sha band@@ han sym@@ b@@ ol@@ izing the b@@ ond between a bro@@ ther and sist@@ er also f@@ alls .\n",
            "he has expressed a great p@@ ain , others have also .\n",
            "be it the government or n@@ go@@ s , something or the other is being carried out on a war foo@@ ting .\n",
            "in other words , such a system is being set up where in farm@@ s in any part of the country will have a market - connect .\n",
            "people remembered him as the best member of parliament , sensi@@ tive writ@@ er , best or@@ at@@ or and most popul@@ ar prime minister and will continue to remember him .\n",
            "i ass@@ ure you that i will do wh@@ at@@ ever and wh@@ en@@ ever something is required to be done for you .\n",
            "there are more than four thousand trad@@ ers who have got pr@@ iz@@ es of fif@@ ty thousand rupees each .\n",
            "under this plan my le@@ tter had reached this mother who w@@ ro@@ te back to me s@@ aying that i was doing a good work under the campaign of providing relief to those poor la@@ dies who are getting freedom from the s@@ mo@@ ke of a ch@@ ul@@ ha .\n",
            "i inv@@ ite you once again , come , let us remember and salute our great freedom figh@@ ters , who sacrific@@ ed their lives for india and let us move ahead with the p@@ ledge to do something for our nation .\n",
            "i ap@@ pl@@ au@@ d all such ash@@ a workers through the st@@ ory of jam@@ un@@ a man@@ i .\n",
            "they can use technology vide@@ os , posts , b@@ lo@@ gs , sc@@ ri@@ p@@ ts , nov@@ el ideas to put forward all these .\n",
            "i could have very well sec@@ u@@ red the same marks by devo@@ ting that time to stud@@ ying , which i w@@ ast@@ ed while p@@ ond@@ ering how to c@@ op@@ y .\n",
            "the world is fac@@ ing natural cal@@ am@@ ities .\n",
            "today , i received a ph@@ one call from lak@@ h@@ w@@ ind@@ er singh from j@@ al@@ and@@ har , pun@@ ja@@ b .\n",
            "so set your own targets for y@@ our@@ selves . keep an open mind , think fre@@ ely , and ass@@ ess your own abilities .\n",
            "this cir@@ c@@ le houses a ro@@ w of d@@ en@@ se tre@@ es .\n",
            "out of these , some play@@ ers even won more than one med@@ al .\n",
            "similarly , in this summit the leaders will be meeting .\n",
            "my dear countrymen , on august 15 , as the nations pradhan se@@ w@@ ak , i get an opportunity to communic@@ ate with the country from the ram@@ parts of the red fort .\n",
            "some@@ times i feel the nation is way ahead and the government is l@@ ac@@ king be@@ hind .\n",
            "tal@@ es of val@@ our of the her@@ o@@ es of our freedom struggle em@@ an@@ ating from every no@@ ok cor@@ ner of these mus@@ e@@ um@@ s inspire us to del@@ ve deep@@ er into our history .\n",
            "remember how , during the kum@@ b@@ h , pray@@ ag@@ ra@@ j was dec@@ or@@ ated with stre@@ et pa@@ int@@ ings\n",
            "following that , many amb@@ ass@@ ad@@ ors shared those p@@ ho@@ to@@ s on social media , wr@@ iting about their gl@@ ori@@ ous experiences .\n",
            "included people of every langu@@ age , every state and every generation had fac@@ ed tor@@ tures during this strugg@@ le@@ today is the birth anniversary of ve@@ er sav@@ ar@@ kar@@ j@@ ee .\n",
            "a l@@ it@@ t@@ le ca@@ rel@@ ess@@ ness about cleanliness and acc@@ um@@ ul@@ ated water can cause de@@ ath , which i believe is not right .\n",
            "for the first time a program of this scale is being organized on the indian soil .\n",
            "you can wr@@ ite ess@@ ays in various langu@@ ages and there is no age lim@@ it .\n",
            "i me@@ an to say that be@@ ti b@@ ac@@ ha@@ o - be@@ ti p@@ adha@@ o movement has also developed in many sha@@ p@@ es and forms .\n",
            "the awareness about hol@@ istic health care has enhanced the gl@@ ory of yoga and yoga day .\n",
            "at a time when separ@@ ati@@ st forces are ra@@ ising their u@@ g@@ ly head@@ s this has been indi@@ as great contribution to the world .\n",
            "you too keep a po@@ s m@@ ach@@ ine for transactions in credit cards .\n",
            "this is not something that needs to be t@@ au@@ ght .\n",
            "a m@@ ass@@ age was s@@ ent to the whole world , a m@@ ass@@ age also w@@ ent to the separ@@ ati@@ st forces and our sym@@ pa@@ th@@ y was expressed towards the people of kashmir .\n",
            "but , today one more time , i want to reiter@@ ate my commitment .\n",
            "thir@@ ty crore new families have been linked to this scheme , bank accounts have been op@@ ened .\n",
            "no marty@@ r , no family is an exc@@ ep@@ tion to that .\n",
            "this is my experience and it mi@@ ght be of help to you as well .\n",
            "i will defin@@ it@@ ely put this under governments noti@@ ce .\n",
            "and it is true that in our country about 5@@ 0,000 mo@@ thers and almost 13 lakh children di@@ e during delivery or immedi@@ ately after it every year .\n",
            "cash can be trans@@ ferred even with a ph@@ one with ordinary fe@@ at@@ ures .\n",
            "let us beg@@ in t@@ od@@ ays mann ki baat with a ph@@ one call .\n",
            "some days ago , a few states taking adv@@ ant@@ age of this we@@ a@@ ther have started many campai@@ gn@@ s in this direction and the government of india has also pas@@ sed camp@@ a law , under which about 40 thousand crores rupees will go to the states for plan@@ ting tre@@ es .\n",
            "today , through mann ki baat , i felt like expres@@ sing these my@@ ri@@ ad of thoughts .\n",
            "i ur@@ ge all of you to take ut@@ most care of y@@ our@@ self and also celebrate the festival with great enthusias@@ m .\n",
            "to get themselves ar@@ re@@ sted vol@@ un@@ tar@@ ily to f@@ ill j@@ ail@@ s , on the one hand , and on the other to im@@ mer@@ se themselves in cre@@ ative work .\n",
            "i am sure , you too mi@@ ght be en@@ jo@@ ying the ra@@ ins with bha@@ ji@@ y@@ as , pak@@ or@@ as , cor@@ n and a ho@@ t c@@ up of te@@ a .\n",
            "as it is , in our families , mo@@ thers always tel@@ l their children to take only as much food they can e@@ at .\n",
            "students should care for their health along with their stu@@ dies so that they are able to wr@@ ite their ex@@ ams well , in good health and spirit .\n",
            "today , during my mann ki baat , i would like to share my inn@@ er thoughts on ex@@ ams with par@@ ents , students and teachers .\n",
            "there are so many places in our country that have been k@@ ept so clean .\n",
            "d@@ on@@ t let it go waste .\n",
            "more than two lakh thir@@ ty thousand villages have dec@@ la@@ red themselves open de@@ fec@@ ation free .\n",
            "today , i want to talk about j@@ av@@ ed ah@@ m@@ ed .\n",
            "so , let me congratulate all my young friends for the same .\n",
            "then , l@@ al ba@@ ha@@ du@@ r sha@@ sh@@ tr@@ i ji had very ap@@ tly tri@@ ed to tou@@ ch the em@@ o@@ tional uni@@ verse of the country .\n",
            "i have brought them to you , now it is your job to keep them going .\n",
            "could you ple@@ ase el@@ abor@@ ate whether its results after one month are mat@@ ching the expec@@ tations of the government\n",
            "this will be a big service to humanity .\n",
            "it has been almost 70 years .\n",
            "the ide@@ als which b@@ ap@@ u practi@@ ced in his life , things that he im@@ par@@ ted are relevant even today .\n",
            "it has been decided that the ms@@ p of noti@@ fied cr@@ ops will be fi@@ x@@ ed at least one and a half times of their cost .\n",
            "for the vo@@ ting , there were around 10 lakh pol@@ ling stations , more than 40 lakh ev@@ m m@@ ach@@ ines , over 17 lakh v@@ v@@ p@@ at m@@ ach@@ in@@ es@@ .@@ . .@@ .@@ . you can im@@ ag@@ ine the gar@@ gan@@ tu@@ an task\n",
            "and , on teachers day , i salute pul@@ el@@ a g@@ op@@ i ch@@ and ji for his hard work , his dedication towards sports and his manner of fin@@ ding happiness in the success of his di@@ sci@@ ples .\n",
            "today , if an inspir@@ ing person@@ ality is required for ab@@ ly guid@@ ing our youth , it cer@@ t@@ ain@@ ly is that of bhag@@ w@@ an bir@@ s@@ a m@@ und@@ a .\n",
            "for all the sel@@ ected can@@ di@@ d@@ ates for the ri@@ o ol@@ y@@ mp@@ ics , and for our f@@ av@@ our@@ ite spor@@ t@@ sper@@ s@@ ons , we should create a ch@@ e@@ erful and positive atmo@@ sphere .\n",
            "you can im@@ ag@@ ine the kind of resources and man@@ power that was required for such a m@@ am@@ mo@@ th exerc@@ ise .\n",
            "his birth anniversary is celebrated as teach@@ er day across the country .\n",
            "my dear countrymen , just like every time earlier , i have received a ra@@ ther large number of le@@ t@@ ters , e ma@@ ils , ph@@ one c@@ alls and com@@ ments from you .\n",
            "the practi@@ ce of yoga lead@@ s to building up of courage , which always protec@@ ts us like a fa@@ ther .\n",
            "i will still ur@@ ge you in today ' s digital world and in the time of go@@ o@@ g@@ le guru , to take some time out from your d@@ ail@@ y rou@@ t@@ ine and devo@@ te it to the book .\n",
            "i conve@@ y my best wishes to all those young persons who are under@@ taking this mission .\n",
            "last year , i made announ@@ c@@ ements of some yoga competi@@ tions and some awards as well .\n",
            "her achiev@@ ement is all the more special because she has made the im@@ pos@@ ing challenges in her life the key to her success .\n",
            "there are times when we come across many s@@ lo@@ gan@@ s on safety at the work place but n@@ one following their le@@ tter and spirit .\n",
            "now im per@@ ple@@ x@@ ed and d@@ on@@ t know how to hand@@ le the situation .\n",
            "we all know that peace , unity and harmony are the only way to sol@@ ve our problems and also the way to our progress and development .\n",
            "this year marks the cent@@ en@@ ary of the cham@@ par@@ an sat@@ y@@ ag@@ ra@@ ha .\n",
            "it is also but natural that we remember his s@@ log@@ an j@@ ai jaw@@ an j@@ ai k@@ is@@ an .\n",
            "they played an important role in prev@@ ent@@ ing the situation form wor@@ s@@ ening further and once again gujarat started its peaceful march .\n",
            "but this sc@@ en@@ ari@@ o had continu@@ ed for five days , or seven days , or ten days , but , the delivery of the cour@@ ts decision gener@@ ated a pleas@@ ant and sur@@ pr@@ ising change of mo@@ od in the country .\n",
            "far south , the people are eng@@ ro@@ s@@ sed in on@@ am fes@@ ti@@ vities and y@@ est@@ er@@ day the entire nation celebrated the hol@@ y festival of r@@ ak@@ sha band@@ han .\n",
            "i had the opportunity to he@@ ar them speak .\n",
            "su@@ d@@ d@@ en@@ ly some@@ one th@@ ro@@ ws a stone into the water and there are ri@@ p@@ ples on the sur@@ face .\n",
            "this qu@@ i@@ z is an att@@ emp@@ t to famil@@ i@@ ar@@ ise the youth with indi@@ as gl@@ ori@@ ous history and the her@@ o@@ es of the freedom movement .\n",
            "guru nan@@ k de@@ v ji firm@@ ly beli@@ ev@@ ed that any service done sel@@ fl@@ ess@@ ly was beyond ev@@ al@@ uation .\n"
          ]
        }
      ],
      "source": [
        "cat valid.en"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBntDyrwfuQP"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
